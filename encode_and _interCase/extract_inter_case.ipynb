{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 1;\n                var nbb_unformatted_code = \"# !pip install pandas pyarrow\";\n                var nbb_formatted_code = \"# !pip install pandas pyarrow\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# !pip install pandas pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 2;\n                var nbb_unformatted_code = \"import pandas as pd\\nimport numpy as np\\nfrom hyperopt import fmin, tpe, hp, Trials\\nfrom tqdm import tqdm\\nimport threading\\nimport random\\nimport time\\nimport gc\\n\\n# show all columns in pandas\\npd.set_option('display.max_columns', None)\\n\\n# standardize column names: ['case_id', 'activity', 'resource', 'timestamp'] for all logs\\ncase_id_col = 'case_id'\\nactivity_col = 'activity'\\nresource_col = 'resource'\\ntimestamp_col = 'timestamp'\\nlabel_col = 'label'\\ntreatment_col = \\\"Treatment1\\\"\\n\\n# Define positive_activities_dict\\npositive_activities_dict = {\\n    'bpic2017': [\\\"A_Pending\\\"],\\n    'bpic2012': [\\\"A_APPROVED\\\", \\\"A_REGISTERED\\\", \\\"A_ACTIVATED\\\"],\\n    # 'trafficFines': [\\\"Send for Credit Collection\\\"]\\n}\\n\\nnegative_activities_dict = {\\n    \\n    'bpic2017': [\\\"A Denied\\\", \\\"A Cancelled\\\"],\\n    'bpic2012': [\\\"A_CANCELLED\\\", \\\"A_DECLINED\\\"],\\n}\";\n                var nbb_formatted_code = \"import pandas as pd\\nimport numpy as np\\nfrom hyperopt import fmin, tpe, hp, Trials\\nfrom tqdm import tqdm\\nimport threading\\nimport random\\nimport time\\nimport gc\\n\\n# show all columns in pandas\\npd.set_option(\\\"display.max_columns\\\", None)\\n\\n# standardize column names: ['case_id', 'activity', 'resource', 'timestamp'] for all logs\\ncase_id_col = \\\"case_id\\\"\\nactivity_col = \\\"activity\\\"\\nresource_col = \\\"resource\\\"\\ntimestamp_col = \\\"timestamp\\\"\\nlabel_col = \\\"label\\\"\\ntreatment_col = \\\"Treatment1\\\"\\n\\n# Define positive_activities_dict\\npositive_activities_dict = {\\n    \\\"bpic2017\\\": [\\\"A_Pending\\\"],\\n    \\\"bpic2012\\\": [\\\"A_APPROVED\\\", \\\"A_REGISTERED\\\", \\\"A_ACTIVATED\\\"],\\n    # 'trafficFines': [\\\"Send for Credit Collection\\\"]\\n}\\n\\nnegative_activities_dict = {\\n    \\\"bpic2017\\\": [\\\"A Denied\\\", \\\"A Cancelled\\\"],\\n    \\\"bpic2012\\\": [\\\"A_CANCELLED\\\", \\\"A_DECLINED\\\"],\\n}\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from hyperopt import fmin, tpe, hp, Trials\n",
    "from tqdm import tqdm\n",
    "import threading\n",
    "import random\n",
    "import time\n",
    "import gc\n",
    "\n",
    "# show all columns in pandas\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# standardize column names: ['case_id', 'activity', 'resource', 'timestamp'] for all logs\n",
    "case_id_col = 'case_id'\n",
    "activity_col = 'activity'\n",
    "resource_col = 'resource'\n",
    "timestamp_col = 'timestamp'\n",
    "label_col = 'label'\n",
    "treatment_col = \"Treatment1\"\n",
    "\n",
    "# Define positive_activities_dict\n",
    "positive_activities_dict = {\n",
    "    'bpic2017': [\"A_Pending\"],\n",
    "    'bpic2012': [\"A_APPROVED\", \"A_REGISTERED\", \"A_ACTIVATED\"],\n",
    "    # 'trafficFines': [\"Send for Credit Collection\"]\n",
    "}\n",
    "\n",
    "negative_activities_dict = {\n",
    "    \n",
    "    'bpic2017': [\"A Denied\", \"A Cancelled\"],\n",
    "    'bpic2012': [\"A_CANCELLED\", \"A_DECLINED\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 3;\n                var nbb_unformatted_code = \"# Inter-case 1\\ndef add_nr_ongoing_cases(df):\\n    # Function to get the number of cases that have been started and did not finish\\n\\n    # Sort the DataFrame by 'timestamp' column\\n    df = df.sort_values(['timestamp', 'event_nr'])#.reset_index(drop=True)\\n    \\n    # Extract relevant columns as NumPy arrays\\n    case_ids = df['case_id'].values\\n    event_nrs = df['event_nr'].values \\n    case_lengths = df['case_length'].values\\n    \\n    # Initialize an array to store the inter-case feature values\\n    inter_case_feature = np.zeros_like(case_ids)\\n    \\n    # Keep track of the cases that have been started but not finished\\n    started_cases = set()    \\n    # Keep track of the cases that have been finished\\n    finished_cases = set()   \\n    \\n    \\n    # Iterate over each row in the DataFrame\\n    for i in range(len(df)):\\n        case_id = case_ids[i]\\n        # Check if the case has been started but not finished\\n        if case_id in started_cases:\\n            inter_case_feature[i] = len(started_cases) \\n        else:\\n            started_cases.add(case_id)\\n            inter_case_feature[i] = len(started_cases)   # Exclude the current case\\n        \\n        # Check if the case has been finished\\n\\n        if event_nrs[i] == case_lengths[i]:\\n            finished_cases.add(case_id)\\n            started_cases.remove(case_id)\\n\\n            inter_case_feature[i] = len(started_cases) #- 1\\n    \\n    # Add the inter-case feature as a new column to the DataFrame\\n    df['nr_ongoing_cases'] = inter_case_feature\\n    return df\\n\\n# Inter-case 2\\ndef add_nr_past_events_in_60_minute_intervals(df):\\n    # Function to calculate the number of events per 60 minute intervals\\n\\n    # Convert timestamp to datetime if not already in datetime format\\n    df['timestamp'] = pd.to_datetime(df['timestamp'])\\n    \\n    # Sort the DataFrame by 'timestamp' column\\n    df = df.sort_values(['timestamp', 'event_nr'])\\n    \\n    # Get the start and end timestamps\\n    start_timestamp = df['timestamp'].iloc[0]\\n    end_timestamp = df['timestamp'].iloc[-1]\\n    \\n    # Create intervals of 60 minutes\\n    intervals = pd.date_range(start=start_timestamp.floor('60min'), end=end_timestamp.ceil('60min'), freq='60min')\\n    \\n    # Bin the timestamps into intervals\\n    df['interval'] = pd.cut(df['timestamp'], bins=intervals, right=False, labels=False)\\n    \\n    # Group by intervals and sum the events\\n    event_counts = df.groupby('interval').size()\\n    #print(event_counts)\\n    \\n    # Fill missing intervals with 0 events\\n    event_counts = event_counts.reindex(range(len(intervals)), fill_value=0)\\n    \\n    # Calculate cumulative sum of event counts\\n    cumulative_events = event_counts\\n    \\n    # Map cumulative events to the DataFrame\\n    df['nr_past_events'] = df['interval'].map(cumulative_events)\\n    \\n    return df\\n\\n\\n# Demand intesity features:\\n# arrival rate, case creation rate, and case completion rate\\ndef calculate_arrival_rate(df, time_window='1H'):\\n    # Convert 'timestamp' to datetime if not already in datetime format\\n    df['timestamp'] = pd.to_datetime(df['timestamp'])\\n    \\n    # Sort the DataFrame by 'timestamp'\\n    df = df.sort_values('timestamp')\\n    \\n    # Calculate the number of cases created within the specified time window\\n    df['arrival_rate'] = df.resample(time_window, on='timestamp').size().fillna(0).reset_index(drop=True)\\n    # Calculate the number of cases created and completed within the specified time window\\n    df['case_creation_rate'] = df.resample(time_window, on='timestamp')['case_id'].nunique().diff().fillna(0).reset_index(drop=True)\\n    df['case_completion_rate'] = df.resample(time_window, on='timestamp')['case_id'].nunique().shift(-1).diff().fillna(0).reset_index(drop=True)\\n    \\n    # Set negative values to 0\\n    df['case_creation_rate'] = df['case_creation_rate'].clip(lower=0)\\n    df['case_completion_rate'] = df['case_completion_rate'].clip(lower=0)\\n    df['arrival_rate'] = df['arrival_rate'].clip(lower=0)\\n    \\n    \\n    df.fillna(0, inplace=True)\\n    \\n    return df\";\n                var nbb_formatted_code = \"# Inter-case 1\\ndef add_nr_ongoing_cases(df):\\n    # Function to get the number of cases that have been started and did not finish\\n\\n    # Sort the DataFrame by 'timestamp' column\\n    df = df.sort_values([\\\"timestamp\\\", \\\"event_nr\\\"])  # .reset_index(drop=True)\\n\\n    # Extract relevant columns as NumPy arrays\\n    case_ids = df[\\\"case_id\\\"].values\\n    event_nrs = df[\\\"event_nr\\\"].values\\n    case_lengths = df[\\\"case_length\\\"].values\\n\\n    # Initialize an array to store the inter-case feature values\\n    inter_case_feature = np.zeros_like(case_ids)\\n\\n    # Keep track of the cases that have been started but not finished\\n    started_cases = set()\\n    # Keep track of the cases that have been finished\\n    finished_cases = set()\\n\\n    # Iterate over each row in the DataFrame\\n    for i in range(len(df)):\\n        case_id = case_ids[i]\\n        # Check if the case has been started but not finished\\n        if case_id in started_cases:\\n            inter_case_feature[i] = len(started_cases)\\n        else:\\n            started_cases.add(case_id)\\n            inter_case_feature[i] = len(started_cases)  # Exclude the current case\\n\\n        # Check if the case has been finished\\n\\n        if event_nrs[i] == case_lengths[i]:\\n            finished_cases.add(case_id)\\n            started_cases.remove(case_id)\\n\\n            inter_case_feature[i] = len(started_cases)  # - 1\\n\\n    # Add the inter-case feature as a new column to the DataFrame\\n    df[\\\"nr_ongoing_cases\\\"] = inter_case_feature\\n    return df\\n\\n\\n# Inter-case 2\\ndef add_nr_past_events_in_60_minute_intervals(df):\\n    # Function to calculate the number of events per 60 minute intervals\\n\\n    # Convert timestamp to datetime if not already in datetime format\\n    df[\\\"timestamp\\\"] = pd.to_datetime(df[\\\"timestamp\\\"])\\n\\n    # Sort the DataFrame by 'timestamp' column\\n    df = df.sort_values([\\\"timestamp\\\", \\\"event_nr\\\"])\\n\\n    # Get the start and end timestamps\\n    start_timestamp = df[\\\"timestamp\\\"].iloc[0]\\n    end_timestamp = df[\\\"timestamp\\\"].iloc[-1]\\n\\n    # Create intervals of 60 minutes\\n    intervals = pd.date_range(\\n        start=start_timestamp.floor(\\\"60min\\\"),\\n        end=end_timestamp.ceil(\\\"60min\\\"),\\n        freq=\\\"60min\\\",\\n    )\\n\\n    # Bin the timestamps into intervals\\n    df[\\\"interval\\\"] = pd.cut(df[\\\"timestamp\\\"], bins=intervals, right=False, labels=False)\\n\\n    # Group by intervals and sum the events\\n    event_counts = df.groupby(\\\"interval\\\").size()\\n    # print(event_counts)\\n\\n    # Fill missing intervals with 0 events\\n    event_counts = event_counts.reindex(range(len(intervals)), fill_value=0)\\n\\n    # Calculate cumulative sum of event counts\\n    cumulative_events = event_counts\\n\\n    # Map cumulative events to the DataFrame\\n    df[\\\"nr_past_events\\\"] = df[\\\"interval\\\"].map(cumulative_events)\\n\\n    return df\\n\\n\\n# Demand intesity features:\\n# arrival rate, case creation rate, and case completion rate\\ndef calculate_arrival_rate(df, time_window=\\\"1H\\\"):\\n    # Convert 'timestamp' to datetime if not already in datetime format\\n    df[\\\"timestamp\\\"] = pd.to_datetime(df[\\\"timestamp\\\"])\\n\\n    # Sort the DataFrame by 'timestamp'\\n    df = df.sort_values(\\\"timestamp\\\")\\n\\n    # Calculate the number of cases created within the specified time window\\n    df[\\\"arrival_rate\\\"] = (\\n        df.resample(time_window, on=\\\"timestamp\\\").size().fillna(0).reset_index(drop=True)\\n    )\\n    # Calculate the number of cases created and completed within the specified time window\\n    df[\\\"case_creation_rate\\\"] = (\\n        df.resample(time_window, on=\\\"timestamp\\\")[\\\"case_id\\\"]\\n        .nunique()\\n        .diff()\\n        .fillna(0)\\n        .reset_index(drop=True)\\n    )\\n    df[\\\"case_completion_rate\\\"] = (\\n        df.resample(time_window, on=\\\"timestamp\\\")[\\\"case_id\\\"]\\n        .nunique()\\n        .shift(-1)\\n        .diff()\\n        .fillna(0)\\n        .reset_index(drop=True)\\n    )\\n\\n    # Set negative values to 0\\n    df[\\\"case_creation_rate\\\"] = df[\\\"case_creation_rate\\\"].clip(lower=0)\\n    df[\\\"case_completion_rate\\\"] = df[\\\"case_completion_rate\\\"].clip(lower=0)\\n    df[\\\"arrival_rate\\\"] = df[\\\"arrival_rate\\\"].clip(lower=0)\\n\\n    df.fillna(0, inplace=True)\\n\\n    return df\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Inter-case 1\n",
    "def add_nr_ongoing_cases(df):\n",
    "    # Function to get the number of cases that have been started and did not finish\n",
    "\n",
    "    # Sort the DataFrame by 'timestamp' column\n",
    "    df = df.sort_values(['timestamp', 'event_nr'])#.reset_index(drop=True)\n",
    "    \n",
    "    # Extract relevant columns as NumPy arrays\n",
    "    case_ids = df['case_id'].values\n",
    "    event_nrs = df['event_nr'].values \n",
    "    case_lengths = df['case_length'].values\n",
    "    \n",
    "    # Initialize an array to store the inter-case feature values\n",
    "    inter_case_feature = np.zeros_like(case_ids)\n",
    "    \n",
    "    # Keep track of the cases that have been started but not finished\n",
    "    started_cases = set()    \n",
    "    # Keep track of the cases that have been finished\n",
    "    finished_cases = set()   \n",
    "    \n",
    "    \n",
    "    # Iterate over each row in the DataFrame\n",
    "    for i in range(len(df)):\n",
    "        case_id = case_ids[i]\n",
    "        # Check if the case has been started but not finished\n",
    "        if case_id in started_cases:\n",
    "            inter_case_feature[i] = len(started_cases) \n",
    "        else:\n",
    "            started_cases.add(case_id)\n",
    "            inter_case_feature[i] = len(started_cases)   # Exclude the current case\n",
    "        \n",
    "        # Check if the case has been finished\n",
    "\n",
    "        if event_nrs[i] == case_lengths[i]:\n",
    "            finished_cases.add(case_id)\n",
    "            started_cases.remove(case_id)\n",
    "\n",
    "            inter_case_feature[i] = len(started_cases) #- 1\n",
    "    \n",
    "    # Add the inter-case feature as a new column to the DataFrame\n",
    "    df['nr_ongoing_cases'] = inter_case_feature\n",
    "    return df\n",
    "\n",
    "# Inter-case 2\n",
    "def add_nr_past_events_in_60_minute_intervals(df):\n",
    "    # Function to calculate the number of events per 60 minute intervals\n",
    "\n",
    "    # Convert timestamp to datetime if not already in datetime format\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    \n",
    "    # Sort the DataFrame by 'timestamp' column\n",
    "    df = df.sort_values(['timestamp', 'event_nr'])\n",
    "    \n",
    "    # Get the start and end timestamps\n",
    "    start_timestamp = df['timestamp'].iloc[0]\n",
    "    end_timestamp = df['timestamp'].iloc[-1]\n",
    "    \n",
    "    # Create intervals of 60 minutes\n",
    "    intervals = pd.date_range(start=start_timestamp.floor('60min'), end=end_timestamp.ceil('60min'), freq='60min')\n",
    "    \n",
    "    # Bin the timestamps into intervals\n",
    "    df['interval'] = pd.cut(df['timestamp'], bins=intervals, right=False, labels=False)\n",
    "    \n",
    "    # Group by intervals and sum the events\n",
    "    event_counts = df.groupby('interval').size()\n",
    "    #print(event_counts)\n",
    "    \n",
    "    # Fill missing intervals with 0 events\n",
    "    event_counts = event_counts.reindex(range(len(intervals)), fill_value=0)\n",
    "    \n",
    "    # Calculate cumulative sum of event counts\n",
    "    cumulative_events = event_counts\n",
    "    \n",
    "    # Map cumulative events to the DataFrame\n",
    "    df['nr_past_events'] = df['interval'].map(cumulative_events)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# Demand intesity features:\n",
    "# arrival rate, case creation rate, and case completion rate\n",
    "def calculate_arrival_rate(df, time_window='1H'):\n",
    "    # Convert 'timestamp' to datetime if not already in datetime format\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    \n",
    "    # Sort the DataFrame by 'timestamp'\n",
    "    df = df.sort_values('timestamp')\n",
    "    \n",
    "    # Calculate the number of cases created within the specified time window\n",
    "    df['arrival_rate'] = df.resample(time_window, on='timestamp').size().fillna(0).reset_index(drop=True)\n",
    "    # Calculate the number of cases created and completed within the specified time window\n",
    "    df['case_creation_rate'] = df.resample(time_window, on='timestamp')['case_id'].nunique().diff().fillna(0).reset_index(drop=True)\n",
    "    df['case_completion_rate'] = df.resample(time_window, on='timestamp')['case_id'].nunique().shift(-1).diff().fillna(0).reset_index(drop=True)\n",
    "    \n",
    "    # Set negative values to 0\n",
    "    df['case_creation_rate'] = df['case_creation_rate'].clip(lower=0)\n",
    "    df['case_completion_rate'] = df['case_completion_rate'].clip(lower=0)\n",
    "    df['arrival_rate'] = df['arrival_rate'].clip(lower=0)\n",
    "    \n",
    "    \n",
    "    df.fillna(0, inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 4;\n                var nbb_unformatted_code = \"# Add remaining and elapsed time columns to the DataFrame\\n# remaining time: the remaining time untill last event occurs\\n# elapsed time: the elapsed time from the first event to the current event\\n# the remaining time is different from the time_to_last_event.\\n# the time_to_last_event is used for survivial analysis.\\n# the time_to_last_event represents the time from the first event to the negative event, i.e., the event that represents the negative outcome.\\n\\ndef add_remtime_column(data):\\n    # Convert timestamp_col to NumPy array\\n    timestamps = data[timestamp_col].values\\n    \\n    # Find start and end dates for each case\\n    start_dates = data.groupby(case_id_col)[timestamp_col].transform('min')\\n    end_dates = data.groupby(case_id_col)[timestamp_col].transform('max')\\n        \\n    \\n    # Calculate elapsed time for the current case\\n    elapsed = timestamps - start_dates\\n    elapsed[np.isnan(elapsed)] = np.timedelta64(0)  # Replace NaN values with 0\\n    elapsed = elapsed / np.timedelta64(1, 'D')  # Convert to days\\n    \\n    # Calculate remaining time for the current case\\n    remtime = end_dates - timestamps\\n    remtime[np.isnan(remtime)] = np.timedelta64(0)  # Replace NaN values with 0\\n    remtime = remtime / np.timedelta64(1, 'D')  # Convert to days\\n\\n    \\n    # Assign elapsed and remaining time to DataFrame\\n    data[\\\"elapsed\\\"] = elapsed\\n    data[\\\"remtime\\\"] = remtime # this column will work with bot remaining time and time-to-negative-event\\n    \\n    #data['time_to_negative_event'] = \\n    \\n    #data['time_to_negative_event'] = np.where(data['label'] == 'deviant', data['remtime'], 0)\\n    \\n    # Sort data by timestamp_col in ascending order\\n    data.sort_values(timestamp_col, ascending=True, inplace=True)\\n    \\n    return data\";\n                var nbb_formatted_code = \"# Add remaining and elapsed time columns to the DataFrame\\n# remaining time: the remaining time untill last event occurs\\n# elapsed time: the elapsed time from the first event to the current event\\n# the remaining time is different from the time_to_last_event.\\n# the time_to_last_event is used for survivial analysis.\\n# the time_to_last_event represents the time from the first event to the negative event, i.e., the event that represents the negative outcome.\\n\\n\\ndef add_remtime_column(data):\\n    # Convert timestamp_col to NumPy array\\n    timestamps = data[timestamp_col].values\\n\\n    # Find start and end dates for each case\\n    start_dates = data.groupby(case_id_col)[timestamp_col].transform(\\\"min\\\")\\n    end_dates = data.groupby(case_id_col)[timestamp_col].transform(\\\"max\\\")\\n\\n    # Calculate elapsed time for the current case\\n    elapsed = timestamps - start_dates\\n    elapsed[np.isnan(elapsed)] = np.timedelta64(0)  # Replace NaN values with 0\\n    elapsed = elapsed / np.timedelta64(1, \\\"D\\\")  # Convert to days\\n\\n    # Calculate remaining time for the current case\\n    remtime = end_dates - timestamps\\n    remtime[np.isnan(remtime)] = np.timedelta64(0)  # Replace NaN values with 0\\n    remtime = remtime / np.timedelta64(1, \\\"D\\\")  # Convert to days\\n\\n    # Assign elapsed and remaining time to DataFrame\\n    data[\\\"elapsed\\\"] = elapsed\\n    data[\\n        \\\"remtime\\\"\\n    ] = remtime  # this column will work with bot remaining time and time-to-negative-event\\n\\n    # data['time_to_negative_event'] =\\n\\n    # data['time_to_negative_event'] = np.where(data['label'] == 'deviant', data['remtime'], 0)\\n\\n    # Sort data by timestamp_col in ascending order\\n    data.sort_values(timestamp_col, ascending=True, inplace=True)\\n\\n    return data\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Add remaining and elapsed time columns to the DataFrame\n",
    "# remaining time: the remaining time untill last event occurs\n",
    "# elapsed time: the elapsed time from the first event to the current event\n",
    "# the remaining time is different from the time_to_last_event.\n",
    "# the time_to_last_event is used for survivial analysis.\n",
    "# the time_to_last_event represents the time from the first event to the negative event, i.e., the event that represents the negative outcome.\n",
    "\n",
    "def add_remtime_column(data):\n",
    "    # Convert timestamp_col to NumPy array\n",
    "    timestamps = data[timestamp_col].values\n",
    "    \n",
    "    # Find start and end dates for each case\n",
    "    start_dates = data.groupby(case_id_col)[timestamp_col].transform('min')\n",
    "    end_dates = data.groupby(case_id_col)[timestamp_col].transform('max')\n",
    "        \n",
    "    \n",
    "    # Calculate elapsed time for the current case\n",
    "    elapsed = timestamps - start_dates\n",
    "    elapsed[np.isnan(elapsed)] = np.timedelta64(0)  # Replace NaN values with 0\n",
    "    elapsed = elapsed / np.timedelta64(1, 'D')  # Convert to days\n",
    "    \n",
    "    # Calculate remaining time for the current case\n",
    "    remtime = end_dates - timestamps\n",
    "    remtime[np.isnan(remtime)] = np.timedelta64(0)  # Replace NaN values with 0\n",
    "    remtime = remtime / np.timedelta64(1, 'D')  # Convert to days\n",
    "\n",
    "    \n",
    "    # Assign elapsed and remaining time to DataFrame\n",
    "    data[\"elapsed\"] = elapsed\n",
    "    data[\"remtime\"] = remtime # this column will work with bot remaining time and time-to-negative-event\n",
    "    \n",
    "    #data['time_to_negative_event'] = \n",
    "    \n",
    "    #data['time_to_negative_event'] = np.where(data['label'] == 'deviant', data['remtime'], 0)\n",
    "    \n",
    "    # Sort data by timestamp_col in ascending order\n",
    "    data.sort_values(timestamp_col, ascending=True, inplace=True)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 5;\n                var nbb_unformatted_code = \"\\ndef label_cases(data, p_activities):\\n    # Create a set of all positive activities\\n    positive_activities = set()\\n    for activities in {index: [value] for index, value in enumerate(p_activities)}.values():        \\n        positive_activities.update(activities)\\n    \\n    # Convert positive activities to numpy array for efficient masking\\n    positive_activities_array = np.array(list(positive_activities))\\n    \\n    # Create a dictionary to store case activities\\n    case_activities_dict = dict(data.groupby(case_id_col)[activity_col].apply(lambda x: tuple(np.unique(x))))\\n    \\n    # Create a boolean mask to check if any positive activities exist for each case ID\\n    has_positive_activity = np.array([np.any(np.isin(case_activities, positive_activities_array)) \\n                                      for case_activities in case_activities_dict.values()])\\n    \\n    # Assign labels based on the boolean mask\\n    labels = np.where(has_positive_activity, 'regular', 'deviant')\\n    \\n    # Map labels back to case IDs\\n    label_map = dict(zip(case_activities_dict.keys(), labels))\\n    data['label'] = data[case_id_col].map(label_map).fillna('deviant')\\n    # The event column is 1 for regular cases (where the negative event do not occurr) and 0 for deviant cases.\\n    data['event'] = np.where(data['label'] == 'deviant', 1, 0)\\n    \\n    \\n    return data\";\n                var nbb_formatted_code = \"def label_cases(data, p_activities):\\n    # Create a set of all positive activities\\n    positive_activities = set()\\n    for activities in {\\n        index: [value] for index, value in enumerate(p_activities)\\n    }.values():\\n        positive_activities.update(activities)\\n\\n    # Convert positive activities to numpy array for efficient masking\\n    positive_activities_array = np.array(list(positive_activities))\\n\\n    # Create a dictionary to store case activities\\n    case_activities_dict = dict(\\n        data.groupby(case_id_col)[activity_col].apply(lambda x: tuple(np.unique(x)))\\n    )\\n\\n    # Create a boolean mask to check if any positive activities exist for each case ID\\n    has_positive_activity = np.array(\\n        [\\n            np.any(np.isin(case_activities, positive_activities_array))\\n            for case_activities in case_activities_dict.values()\\n        ]\\n    )\\n\\n    # Assign labels based on the boolean mask\\n    labels = np.where(has_positive_activity, \\\"regular\\\", \\\"deviant\\\")\\n\\n    # Map labels back to case IDs\\n    label_map = dict(zip(case_activities_dict.keys(), labels))\\n    data[\\\"label\\\"] = data[case_id_col].map(label_map).fillna(\\\"deviant\\\")\\n    # The event column is 1 for regular cases (where the negative event do not occurr) and 0 for deviant cases.\\n    data[\\\"event\\\"] = np.where(data[\\\"label\\\"] == \\\"deviant\\\", 1, 0)\\n\\n    return data\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def label_cases(data, p_activities):\n",
    "    # Create a set of all positive activities\n",
    "    positive_activities = set()\n",
    "    for activities in {index: [value] for index, value in enumerate(p_activities)}.values():        \n",
    "        positive_activities.update(activities)\n",
    "    \n",
    "    # Convert positive activities to numpy array for efficient masking\n",
    "    positive_activities_array = np.array(list(positive_activities))\n",
    "    \n",
    "    # Create a dictionary to store case activities\n",
    "    case_activities_dict = dict(data.groupby(case_id_col)[activity_col].apply(lambda x: tuple(np.unique(x))))\n",
    "    \n",
    "    # Create a boolean mask to check if any positive activities exist for each case ID\n",
    "    has_positive_activity = np.array([np.any(np.isin(case_activities, positive_activities_array)) \n",
    "                                      for case_activities in case_activities_dict.values()])\n",
    "    \n",
    "    # Assign labels based on the boolean mask\n",
    "    labels = np.where(has_positive_activity, 'regular', 'deviant')\n",
    "    \n",
    "    # Map labels back to case IDs\n",
    "    label_map = dict(zip(case_activities_dict.keys(), labels))\n",
    "    # positive outcome: regular\n",
    "    # negative outcome: deviant\n",
    "    data['label'] = data[case_id_col].map(label_map).fillna('deviant')\n",
    "    \n",
    "    # The event column is 1 for regular cases (where the negative event do not occurr) and 0 for deviant cases.\n",
    "    data['event'] = np.where(data['label'] == 'deviant', 1, 0)\n",
    "    \n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 6;\n                var nbb_unformatted_code = \"def get_columns_details(data):\\n    \\n    print(data.shape)\\n\\n\\n    # Get the list of all column names\\n    all_columns = data.columns\\n    print(len(all_columns))\\n\\n    # Identify the column types\\n    numeric_cols = data.select_dtypes(include='number').columns\\n    categorical_cols = data.select_dtypes(include='object').columns\\n\\n    # Exclude specified columns from consideration\\n    exclude_cols = [case_id_col, label_col, treatment_col, timestamp_col, 'remtime']\\n\\n    # Remove specified columns from all columns\\n    considered_cols = [col for col in all_columns if col not in exclude_cols]\\n\\n    # Create empty lists to store inferred static and dynamic columns\\n    static_num_cols = []\\n    dynamic_num_cols = []\\n    static_cat_cols = []\\n    dynamic_cat_cols = []\\n\\n    # Define threshold for unique values to consider a column as static categorical\\n    unique_value_threshold = 5\\n\\n    # Group data by 'case_id'\\n    grouped_data = data.groupby(case_id_col)\\n\\n    # Calculate standard deviation for numeric columns within each group\\n    std_dev = grouped_data[numeric_cols].std()\\n\\n    # Calculate number of unique values for categorical columns within each group\\n    unique_values = grouped_data[categorical_cols].nunique()\\n\\n    # Identify static and dynamic columns based on characteristics\\n    for col in considered_cols:\\n        if col in numeric_cols:\\n            # If the standard deviation is close to zero, consider it static\\n            if std_dev[col].max() < 1e-6:\\n                static_num_cols.append(col)\\n            else:\\n                dynamic_num_cols.append(col)\\n        elif col in categorical_cols:\\n            # If the number of unique values is below the threshold, consider it static\\n            if unique_values[col].max() <= unique_value_threshold:\\n                static_cat_cols.append(col)\\n            else:\\n                dynamic_cat_cols.append(col)\\n\\n    # Print or use the results as needed\\n    print(\\\"Static Numeric Columns:\\\", static_num_cols)\\n    print(\\\"Dynamic Numeric Columns:\\\", dynamic_num_cols)\\n    print(\\\"Static Categorical Columns:\\\", static_cat_cols)\\n    print(\\\"Dynamic Categorical Columns:\\\", dynamic_cat_cols)\\n    print(\\\"Total columns:\\\", len(static_num_cols) + len(static_cat_cols) + len(dynamic_num_cols) + len(dynamic_cat_cols) + len(exclude_cols))\";\n                var nbb_formatted_code = \"def get_columns_details(data):\\n    print(data.shape)\\n\\n    # Get the list of all column names\\n    all_columns = data.columns\\n    print(len(all_columns))\\n\\n    # Identify the column types\\n    numeric_cols = data.select_dtypes(include=\\\"number\\\").columns\\n    categorical_cols = data.select_dtypes(include=\\\"object\\\").columns\\n\\n    # Exclude specified columns from consideration\\n    exclude_cols = [case_id_col, label_col, treatment_col, timestamp_col, \\\"remtime\\\"]\\n\\n    # Remove specified columns from all columns\\n    considered_cols = [col for col in all_columns if col not in exclude_cols]\\n\\n    # Create empty lists to store inferred static and dynamic columns\\n    static_num_cols = []\\n    dynamic_num_cols = []\\n    static_cat_cols = []\\n    dynamic_cat_cols = []\\n\\n    # Define threshold for unique values to consider a column as static categorical\\n    unique_value_threshold = 5\\n\\n    # Group data by 'case_id'\\n    grouped_data = data.groupby(case_id_col)\\n\\n    # Calculate standard deviation for numeric columns within each group\\n    std_dev = grouped_data[numeric_cols].std()\\n\\n    # Calculate number of unique values for categorical columns within each group\\n    unique_values = grouped_data[categorical_cols].nunique()\\n\\n    # Identify static and dynamic columns based on characteristics\\n    for col in considered_cols:\\n        if col in numeric_cols:\\n            # If the standard deviation is close to zero, consider it static\\n            if std_dev[col].max() < 1e-6:\\n                static_num_cols.append(col)\\n            else:\\n                dynamic_num_cols.append(col)\\n        elif col in categorical_cols:\\n            # If the number of unique values is below the threshold, consider it static\\n            if unique_values[col].max() <= unique_value_threshold:\\n                static_cat_cols.append(col)\\n            else:\\n                dynamic_cat_cols.append(col)\\n\\n    # Print or use the results as needed\\n    print(\\\"Static Numeric Columns:\\\", static_num_cols)\\n    print(\\\"Dynamic Numeric Columns:\\\", dynamic_num_cols)\\n    print(\\\"Static Categorical Columns:\\\", static_cat_cols)\\n    print(\\\"Dynamic Categorical Columns:\\\", dynamic_cat_cols)\\n    print(\\n        \\\"Total columns:\\\",\\n        len(static_num_cols)\\n        + len(static_cat_cols)\\n        + len(dynamic_num_cols)\\n        + len(dynamic_cat_cols)\\n        + len(exclude_cols),\\n    )\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_columns_details(data):\n",
    "    \n",
    "    print(data.shape)\n",
    "\n",
    "\n",
    "    # Get the list of all column names\n",
    "    all_columns = data.columns\n",
    "    print(len(all_columns))\n",
    "\n",
    "    # Identify the column types\n",
    "    numeric_cols = data.select_dtypes(include='number').columns\n",
    "    categorical_cols = data.select_dtypes(include='object').columns\n",
    "\n",
    "    # Exclude specified columns from consideration\n",
    "    exclude_cols = [case_id_col, label_col, treatment_col, timestamp_col, 'remtime']\n",
    "\n",
    "    # Remove specified columns from all columns\n",
    "    considered_cols = [col for col in all_columns if col not in exclude_cols]\n",
    "\n",
    "    # Create empty lists to store inferred static and dynamic columns\n",
    "    static_num_cols = []\n",
    "    dynamic_num_cols = []\n",
    "    static_cat_cols = []\n",
    "    dynamic_cat_cols = []\n",
    "\n",
    "    # Define threshold for unique values to consider a column as static categorical\n",
    "    unique_value_threshold = 5\n",
    "\n",
    "    # Group data by 'case_id'\n",
    "    grouped_data = data.groupby(case_id_col)\n",
    "\n",
    "    # Calculate standard deviation for numeric columns within each group\n",
    "    std_dev = grouped_data[numeric_cols].std()\n",
    "\n",
    "    # Calculate number of unique values for categorical columns within each group\n",
    "    unique_values = grouped_data[categorical_cols].nunique()\n",
    "\n",
    "    # Identify static and dynamic columns based on characteristics\n",
    "    for col in considered_cols:\n",
    "        if col in numeric_cols:\n",
    "            # If the standard deviation is close to zero, consider it static\n",
    "            if std_dev[col].max() < 1e-6:\n",
    "                static_num_cols.append(col)\n",
    "            else:\n",
    "                dynamic_num_cols.append(col)\n",
    "        elif col in categorical_cols:\n",
    "            # If the number of unique values is below the threshold, consider it static\n",
    "            if unique_values[col].max() <= unique_value_threshold:\n",
    "                static_cat_cols.append(col)\n",
    "            else:\n",
    "                dynamic_cat_cols.append(col)\n",
    "\n",
    "    # Print or use the results as needed\n",
    "    print(\"Static Numeric Columns:\", static_num_cols)\n",
    "    print(\"Dynamic Numeric Columns:\", dynamic_num_cols)\n",
    "    print(\"Static Categorical Columns:\", static_cat_cols)\n",
    "    print(\"Dynamic Categorical Columns:\", dynamic_cat_cols)\n",
    "    print(\"Total columns:\", len(static_num_cols) + len(static_cat_cols) + len(dynamic_num_cols) + len(dynamic_cat_cols) + len(exclude_cols))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log...bpic2012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(156962, 14)\n",
      "Add number of cases that started but not finished...\n",
      "(156962, 15) 1\n",
      "Add number of past events that occured in the last 60 minutes...\n",
      "(156962, 17) 2\n",
      "Add demand intensity inter-case features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_341472/2791391609.py:90: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  df['arrival_rate'] = df.resample(time_window, on='timestamp').size().fillna(0).reset_index(drop=True)\n",
      "/tmp/ipykernel_341472/2791391609.py:92: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  df['case_creation_rate'] = df.resample(time_window, on='timestamp')['case_id'].nunique().diff().fillna(0).reset_index(drop=True)\n",
      "/tmp/ipykernel_341472/2791391609.py:93: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  df['case_completion_rate'] = df.resample(time_window, on='timestamp')['case_id'].nunique().shift(-1).diff().fillna(0).reset_index(drop=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(156962, 20) 3\n",
      "Add remaining and elapsed times...\n",
      "(156962, 22) 4\n",
      "Label cases...\n",
      "(156962, 24) 5\n",
      "Getting columns details...\n",
      "(156962, 24)\n",
      "24\n",
      "Static Numeric Columns: ['amount_req', 'case_length', 'Total_Offers', 'event']\n",
      "Dynamic Numeric Columns: ['event_nr', 'hour_of_day', 'day_of_week', 'day_of_month', 'month_of_year', 'time_to_last_event_days', 'nr_ongoing_cases', 'interval', 'nr_past_events', 'arrival_rate', 'case_creation_rate', 'case_completion_rate', 'elapsed']\n",
      "Static Categorical Columns: []\n",
      "Dynamic Categorical Columns: ['activity', 'resource']\n",
      "Total columns: 24\n",
      "Saving csv file...\n",
      "Done!\n",
      "\n",
      "\n",
      "Log...bpic2017\n",
      "(1198319, 33)\n",
      "Add number of cases that started but not finished...\n",
      "(1198319, 34) 1\n",
      "Add number of past events that occured in the last 60 minutes...\n",
      "(1198319, 36) 2\n",
      "Add demand intensity inter-case features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_341472/2791391609.py:90: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  df['arrival_rate'] = df.resample(time_window, on='timestamp').size().fillna(0).reset_index(drop=True)\n",
      "/tmp/ipykernel_341472/2791391609.py:92: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  df['case_creation_rate'] = df.resample(time_window, on='timestamp')['case_id'].nunique().diff().fillna(0).reset_index(drop=True)\n",
      "/tmp/ipykernel_341472/2791391609.py:93: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  df['case_completion_rate'] = df.resample(time_window, on='timestamp')['case_id'].nunique().shift(-1).diff().fillna(0).reset_index(drop=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1198319, 39) 3\n",
      "Add remaining and elapsed times...\n",
      "(1198319, 41) 4\n",
      "Label cases...\n",
      "(1198319, 42) 5\n",
      "Getting columns details...\n",
      "(1198319, 42)\n",
      "42\n",
      "Static Numeric Columns: ['requestedamount', 'case_length', 'Mail_and_Online_Count', 'Online_Only_Count', 'Total_Offers', 'event']\n",
      "Dynamic Numeric Columns: ['firstwithdrawalamount', 'monthlycost', 'numberofterms', 'offeredamount', 'creditscore', 'event_nr', 'month', 'weekday', 'hour', 'open_cases', 'hour_of_day', 'day_of_week', 'day_of_month', 'month_of_year', 'time_to_last_event_days', 'interval', 'nr_past_events', 'arrival_rate', 'case_creation_rate', 'case_completion_rate', 'elapsed']\n",
      "Static Categorical Columns: ['applicationtype', 'loangoal', 'action', 'eventorigin', 'accepted', 'selected']\n",
      "Dynamic Categorical Columns: ['activity', 'resource', 'lifecycle:transition', 'nr_ongoing_cases']\n",
      "Total columns: 42\n",
      "Saving csv file...\n",
      "Done!\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 7;\n                var nbb_unformatted_code = \"import os\\nfrom pandas import read_csv\\n\\ndfs = []\\n\\nlogs = [\\n        'bpic2012',\\n        'bpic2017', \\n        #'trafficFines',\\n        ]\\n\\n\\n\\nfor log_name in logs:\\n    print(f\\\"Log...{log_name}\\\")\\n   \\n    log = pd.read_parquet(os.path.join(f\\\"./../prepared_data/{log_name}/date_with_treatments_{log_name}.parquet\\\"))\\n    \\n    # read the log\\n    #log = read_csv(os.path.join(f\\\"./../prepared_data/{log_name}/date_with_treatments_{log_name}.csv\\\"), sep=';')\\n    print(log.shape)\\n    print(\\\"Add number of cases that started but not finished...\\\")\\n    df = add_nr_ongoing_cases(log)\\n    print(df.shape, \\\"1\\\")\\n    print(\\\"Add number of past events that occured in the last 60 minutes...\\\")\\n    df = add_nr_past_events_in_60_minute_intervals(df)\\n    print(df.shape, \\\"2\\\")\\n    print(\\\"Add demand intensity inter-case features...\\\")\\n    df = calculate_arrival_rate(df)\\n    print(df.shape, \\\"3\\\")\\n    print(\\\"Add remaining and elapsed times...\\\")\\n    df = add_remtime_column(df)\\n    print(df.shape, \\\"4\\\")\\n    print(\\\"Label cases...\\\")\\n    df = label_cases(df,  positive_activities_dict[log_name])\\n    print(df.shape, \\\"5\\\")\\n    dfs.append(df)\\n    \\n    df.name = \\\"data_with_inter_case_features_%s.parquet\\\" % log_name\\n    \\n    print(\\\"Getting columns details...\\\")\\n    get_columns_details(df)\\n    \\n    \\n    \\n    print(\\\"Saving csv file...\\\")\\n    results_dir = \\\"./../prepared_data/%s/\\\" % log_name\\n    import os\\n\\n    if not os.path.exists(os.path.join(results_dir)):\\n        os.makedirs(os.path.join(results_dir))\\n        \\n    df.to_parquet( os.path.join(\\n            results_dir, df.name \\n        ))\\n\\n    print(\\\"Done!\\\\n\\\")    \\n    print(\\\"\\\")\";\n                var nbb_formatted_code = \"import os\\nfrom pandas import read_csv\\n\\ndfs = []\\n\\nlogs = [\\n    \\\"bpic2012\\\",\\n    \\\"bpic2017\\\",\\n    #'trafficFines',\\n]\\n\\n\\nfor log_name in logs:\\n    print(f\\\"Log...{log_name}\\\")\\n\\n    log = pd.read_parquet(\\n        os.path.join(\\n            f\\\"./../prepared_data/{log_name}/date_with_treatments_{log_name}.parquet\\\"\\n        )\\n    )\\n\\n    # read the log\\n    # log = read_csv(os.path.join(f\\\"./../prepared_data/{log_name}/date_with_treatments_{log_name}.csv\\\"), sep=';')\\n    print(log.shape)\\n    print(\\\"Add number of cases that started but not finished...\\\")\\n    df = add_nr_ongoing_cases(log)\\n    print(df.shape, \\\"1\\\")\\n    print(\\\"Add number of past events that occured in the last 60 minutes...\\\")\\n    df = add_nr_past_events_in_60_minute_intervals(df)\\n    print(df.shape, \\\"2\\\")\\n    print(\\\"Add demand intensity inter-case features...\\\")\\n    df = calculate_arrival_rate(df)\\n    print(df.shape, \\\"3\\\")\\n    print(\\\"Add remaining and elapsed times...\\\")\\n    df = add_remtime_column(df)\\n    print(df.shape, \\\"4\\\")\\n    print(\\\"Label cases...\\\")\\n    df = label_cases(df, positive_activities_dict[log_name])\\n    print(df.shape, \\\"5\\\")\\n    dfs.append(df)\\n\\n    df.name = \\\"data_with_inter_case_features_%s.parquet\\\" % log_name\\n\\n    print(\\\"Getting columns details...\\\")\\n    get_columns_details(df)\\n\\n    print(\\\"Saving csv file...\\\")\\n    results_dir = \\\"./../prepared_data/%s/\\\" % log_name\\n    import os\\n\\n    if not os.path.exists(os.path.join(results_dir)):\\n        os.makedirs(os.path.join(results_dir))\\n\\n    df.to_parquet(os.path.join(results_dir, df.name))\\n\\n    print(\\\"Done!\\\\n\\\")\\n    print(\\\"\\\")\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from pandas import read_csv\n",
    "\n",
    "dfs = []\n",
    "\n",
    "logs = [\n",
    "        'bpic2012',\n",
    "        'bpic2017', \n",
    "        #'trafficFines',\n",
    "        ]\n",
    "\n",
    "\n",
    "\n",
    "for log_name in logs:\n",
    "    print(f\"Log...{log_name}\")\n",
    "   \n",
    "    log = pd.read_parquet(os.path.join(f\"./../prepared_data/{log_name}/date_with_treatments_{log_name}.parquet\"))\n",
    "    \n",
    "    # read the log\n",
    "    #log = read_csv(os.path.join(f\"./../prepared_data/{log_name}/date_with_treatments_{log_name}.csv\"), sep=';')\n",
    "    print(log.shape)\n",
    "    print(\"Add number of cases that started but not finished...\")\n",
    "    df = add_nr_ongoing_cases(log)\n",
    "    print(df.shape, \"1\")\n",
    "    print(\"Add number of past events that occured in the last 60 minutes...\")\n",
    "    df = add_nr_past_events_in_60_minute_intervals(df)\n",
    "    print(df.shape, \"2\")\n",
    "    print(\"Add demand intensity inter-case features...\")\n",
    "    df = calculate_arrival_rate(df)\n",
    "    print(df.shape, \"3\")\n",
    "    print(\"Add remaining and elapsed times...\")\n",
    "    df = add_remtime_column(df)\n",
    "    print(df.shape, \"4\")\n",
    "    print(\"Label cases...\")\n",
    "    df = label_cases(df,  positive_activities_dict[log_name])\n",
    "    print(df.shape, \"5\")\n",
    "    dfs.append(df)\n",
    "    \n",
    "    df.name = \"data_with_inter_case_features_%s.parquet\" % log_name\n",
    "    \n",
    "    print(\"Getting columns details...\")\n",
    "    get_columns_details(df)\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(\"Saving csv file...\")\n",
    "    results_dir = \"./../prepared_data/%s/\" % log_name\n",
    "    import os\n",
    "\n",
    "    if not os.path.exists(os.path.join(results_dir)):\n",
    "        os.makedirs(os.path.join(results_dir))\n",
    "        \n",
    "    df.to_parquet( os.path.join(\n",
    "            results_dir, df.name \n",
    "        ))\n",
    "\n",
    "    print(\"Done!\\n\")    \n",
    "    print(\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Treatment1\n",
       "Treatment    1195809\n",
       "Control         2510\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 8;\n                var nbb_unformatted_code = \"df17 = pd.read_parquet(\\\"./../prepared_data/bpic2017/data_with_inter_case_features_bpic2017.parquet\\\")\\ndf17.head()\\ndf17.Treatment1.value_counts()\";\n                var nbb_formatted_code = \"df17 = pd.read_parquet(\\n    \\\"./../prepared_data/bpic2017/data_with_inter_case_features_bpic2017.parquet\\\"\\n)\\ndf17.head()\\ndf17.Treatment1.value_counts()\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df17 = pd.read_parquet(\"./../prepared_data/bpic2017/data_with_inter_case_features_bpic2017.parquet\")\n",
    "df17.head()\n",
    "df17.Treatment1.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "applicationtype                    object\n",
       "loangoal                           object\n",
       "requestedamount                   float64\n",
       "case_id                            object\n",
       "label                              object\n",
       "activity                           object\n",
       "resource                           object\n",
       "action                             object\n",
       "eventorigin                        object\n",
       "lifecycle:transition               object\n",
       "accepted                           object\n",
       "selected                           object\n",
       "firstwithdrawalamount             float64\n",
       "monthlycost                       float64\n",
       "numberofterms                     float64\n",
       "offeredamount                     float64\n",
       "creditscore                       float64\n",
       "event_nr                            int64\n",
       "month                               int64\n",
       "weekday                             int64\n",
       "hour                                int64\n",
       "open_cases                          int64\n",
       "timestamp                  datetime64[ns]\n",
       "case_length                         int64\n",
       "hour_of_day                         int32\n",
       "day_of_week                         int32\n",
       "day_of_month                        int32\n",
       "month_of_year                       int32\n",
       "Mail_and_Online_Count               int64\n",
       "Online_Only_Count                   int64\n",
       "Total_Offers                        int64\n",
       "Treatment1                         object\n",
       "time_to_last_event_days           float64\n",
       "nr_ongoing_cases                    int64\n",
       "interval                            int64\n",
       "nr_past_events                      int64\n",
       "arrival_rate                      float64\n",
       "case_creation_rate                float64\n",
       "case_completion_rate              float64\n",
       "elapsed                           float64\n",
       "remtime                           float64\n",
       "event                               int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 9;\n                var nbb_unformatted_code = \"df17.dtypes\";\n                var nbb_formatted_code = \"df17.dtypes\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df17.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 10;\n                var nbb_unformatted_code = \"#df17[df17[case_id_col]==\\\"Application_652823628\\\"]\";\n                var nbb_formatted_code = \"# df17[df17[case_id_col]==\\\"Application_652823628\\\"]\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#df17[df17[case_id_col]==\"Application_652823628\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Treatment1\n",
       "Treatment    115125\n",
       "Control       41837\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 11;\n                var nbb_unformatted_code = \"\\ndf12 = pd.read_parquet(\\\"./../prepared_data/bpic2012/data_with_inter_case_features_bpic2012.parquet\\\")\\ndf12.Treatment1.value_counts()\";\n                var nbb_formatted_code = \"df12 = pd.read_parquet(\\n    \\\"./../prepared_data/bpic2012/data_with_inter_case_features_bpic2012.parquet\\\"\\n)\\ndf12.Treatment1.value_counts()\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "df12 = pd.read_parquet(\"./../prepared_data/bpic2012/data_with_inter_case_features_bpic2012.parquet\")\n",
    "df12.Treatment1.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 12;\n                var nbb_unformatted_code = \"# df12.dtypes\";\n                var nbb_formatted_code = \"# df12.dtypes\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# df12.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 13;\n                var nbb_unformatted_code = \"# def get_columns_details(data):\\n    \\n#     print(data.shape)\\n\\n\\n#     # Get the list of all column names\\n#     all_columns = data.columns\\n#     print(len(all_columns))\\n\\n#     # Identify the column types\\n#     numeric_cols = data.select_dtypes(include='number').columns\\n#     categorical_cols = data.select_dtypes(include='object').columns\\n\\n#     # Exclude specified columns from consideration\\n#     exclude_cols = [case_id_col, label_col, treatment_col, timestamp_col, 'remtime']\\n\\n#     # Remove specified columns from all columns\\n#     considered_cols = [col for col in all_columns if col not in exclude_cols]\\n\\n#     # Create empty lists to store inferred static and dynamic columns\\n#     static_num_cols = []\\n#     dynamic_num_cols = []\\n#     static_cat_cols = []\\n#     dynamic_cat_cols = []\\n\\n#     # Define threshold for unique values to consider a column as static categorical\\n#     unique_value_threshold = 5\\n\\n#     # Group data by 'case_id'\\n#     grouped_data = data.groupby(case_id_col)\\n\\n#     # Calculate standard deviation for numeric columns within each group\\n#     std_dev = grouped_data[numeric_cols].std()\\n\\n#     # Calculate number of unique values for categorical columns within each group\\n#     unique_values = grouped_data[categorical_cols].nunique()\\n\\n#     # Identify static and dynamic columns based on characteristics\\n#     for col in considered_cols:\\n#         if col in numeric_cols:\\n#             # If the standard deviation is close to zero, consider it static\\n#             if std_dev[col].max() < 1e-6:\\n#                 static_num_cols.append(col)\\n#             else:\\n#                 dynamic_num_cols.append(col)\\n#         elif col in categorical_cols:\\n#             # If the number of unique values is below the threshold, consider it static\\n#             if unique_values[col].max() <= unique_value_threshold:\\n#                 static_cat_cols.append(col)\\n#             else:\\n#                 dynamic_cat_cols.append(col)\\n\\n#     # Print or use the results as needed\\n#     print(\\\"Static Numeric Columns:\\\", static_num_cols)\\n#     print(\\\"Dynamic Numeric Columns:\\\", dynamic_num_cols)\\n#     print(\\\"Static Categorical Columns:\\\", static_cat_cols)\\n#     print(\\\"Dynamic Categorical Columns:\\\", dynamic_cat_cols)\\n#     print(\\\"Total columns:\\\", len(static_num_cols) + len(static_cat_cols) + len(dynamic_num_cols) + len(dynamic_cat_cols) + len(exclude_cols))\";\n                var nbb_formatted_code = \"# def get_columns_details(data):\\n\\n#     print(data.shape)\\n\\n\\n#     # Get the list of all column names\\n#     all_columns = data.columns\\n#     print(len(all_columns))\\n\\n#     # Identify the column types\\n#     numeric_cols = data.select_dtypes(include='number').columns\\n#     categorical_cols = data.select_dtypes(include='object').columns\\n\\n#     # Exclude specified columns from consideration\\n#     exclude_cols = [case_id_col, label_col, treatment_col, timestamp_col, 'remtime']\\n\\n#     # Remove specified columns from all columns\\n#     considered_cols = [col for col in all_columns if col not in exclude_cols]\\n\\n#     # Create empty lists to store inferred static and dynamic columns\\n#     static_num_cols = []\\n#     dynamic_num_cols = []\\n#     static_cat_cols = []\\n#     dynamic_cat_cols = []\\n\\n#     # Define threshold for unique values to consider a column as static categorical\\n#     unique_value_threshold = 5\\n\\n#     # Group data by 'case_id'\\n#     grouped_data = data.groupby(case_id_col)\\n\\n#     # Calculate standard deviation for numeric columns within each group\\n#     std_dev = grouped_data[numeric_cols].std()\\n\\n#     # Calculate number of unique values for categorical columns within each group\\n#     unique_values = grouped_data[categorical_cols].nunique()\\n\\n#     # Identify static and dynamic columns based on characteristics\\n#     for col in considered_cols:\\n#         if col in numeric_cols:\\n#             # If the standard deviation is close to zero, consider it static\\n#             if std_dev[col].max() < 1e-6:\\n#                 static_num_cols.append(col)\\n#             else:\\n#                 dynamic_num_cols.append(col)\\n#         elif col in categorical_cols:\\n#             # If the number of unique values is below the threshold, consider it static\\n#             if unique_values[col].max() <= unique_value_threshold:\\n#                 static_cat_cols.append(col)\\n#             else:\\n#                 dynamic_cat_cols.append(col)\\n\\n#     # Print or use the results as needed\\n#     print(\\\"Static Numeric Columns:\\\", static_num_cols)\\n#     print(\\\"Dynamic Numeric Columns:\\\", dynamic_num_cols)\\n#     print(\\\"Static Categorical Columns:\\\", static_cat_cols)\\n#     print(\\\"Dynamic Categorical Columns:\\\", dynamic_cat_cols)\\n#     print(\\\"Total columns:\\\", len(static_num_cols) + len(static_cat_cols) + len(dynamic_num_cols) + len(dynamic_cat_cols) + len(exclude_cols))\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# def get_columns_details(data):\n",
    "    \n",
    "#     print(data.shape)\n",
    "\n",
    "\n",
    "#     # Get the list of all column names\n",
    "#     all_columns = data.columns\n",
    "#     print(len(all_columns))\n",
    "\n",
    "#     # Identify the column types\n",
    "#     numeric_cols = data.select_dtypes(include='number').columns\n",
    "#     categorical_cols = data.select_dtypes(include='object').columns\n",
    "\n",
    "#     # Exclude specified columns from consideration\n",
    "#     exclude_cols = [case_id_col, label_col, treatment_col, timestamp_col, 'remtime']\n",
    "\n",
    "#     # Remove specified columns from all columns\n",
    "#     considered_cols = [col for col in all_columns if col not in exclude_cols]\n",
    "\n",
    "#     # Create empty lists to store inferred static and dynamic columns\n",
    "#     static_num_cols = []\n",
    "#     dynamic_num_cols = []\n",
    "#     static_cat_cols = []\n",
    "#     dynamic_cat_cols = []\n",
    "\n",
    "#     # Define threshold for unique values to consider a column as static categorical\n",
    "#     unique_value_threshold = 5\n",
    "\n",
    "#     # Group data by 'case_id'\n",
    "#     grouped_data = data.groupby(case_id_col)\n",
    "\n",
    "#     # Calculate standard deviation for numeric columns within each group\n",
    "#     std_dev = grouped_data[numeric_cols].std()\n",
    "\n",
    "#     # Calculate number of unique values for categorical columns within each group\n",
    "#     unique_values = grouped_data[categorical_cols].nunique()\n",
    "\n",
    "#     # Identify static and dynamic columns based on characteristics\n",
    "#     for col in considered_cols:\n",
    "#         if col in numeric_cols:\n",
    "#             # If the standard deviation is close to zero, consider it static\n",
    "#             if std_dev[col].max() < 1e-6:\n",
    "#                 static_num_cols.append(col)\n",
    "#             else:\n",
    "#                 dynamic_num_cols.append(col)\n",
    "#         elif col in categorical_cols:\n",
    "#             # If the number of unique values is below the threshold, consider it static\n",
    "#             if unique_values[col].max() <= unique_value_threshold:\n",
    "#                 static_cat_cols.append(col)\n",
    "#             else:\n",
    "#                 dynamic_cat_cols.append(col)\n",
    "\n",
    "#     # Print or use the results as needed\n",
    "#     print(\"Static Numeric Columns:\", static_num_cols)\n",
    "#     print(\"Dynamic Numeric Columns:\", dynamic_num_cols)\n",
    "#     print(\"Static Categorical Columns:\", static_cat_cols)\n",
    "#     print(\"Dynamic Categorical Columns:\", dynamic_cat_cols)\n",
    "#     print(\"Total columns:\", len(static_num_cols) + len(static_cat_cols) + len(dynamic_num_cols) + len(dynamic_cat_cols) + len(exclude_cols))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 14;\n                var nbb_unformatted_code = \"# import pandas as pd\\n# import numpy as np\\n\\n# # Assuming 'df12' is your DataFrame\\n# data = df17.copy(deep=True)\\n# print(data.shape)\\n\\n\\n# # Get the list of all column names\\n# all_columns = data.columns\\n# print(len(all_columns))\\n\\n# # Identify the column types\\n# numeric_cols = data.select_dtypes(include='number').columns\\n# categorical_cols = data.select_dtypes(include='object').columns\\n\\n# # Exclude specified columns from consideration\\n# exclude_cols = [case_id_col, label_col, treatment_col, timestamp_col]\\n\\n# # Remove specified columns from all columns\\n# considered_cols = [col for col in all_columns if col not in exclude_cols]\\n\\n# # Create empty lists to store inferred static and dynamic columns\\n# static_num_cols = []\\n# dynamic_num_cols = []\\n# static_cat_cols = []\\n# dynamic_cat_cols = []\\n\\n# # Define threshold for unique values to consider a column as static categorical\\n# unique_value_threshold = 5\\n\\n# # Group data by 'case_id'\\n# grouped_data = data.groupby(case_id_col)\\n\\n# # Calculate standard deviation for numeric columns within each group\\n# std_dev = grouped_data[numeric_cols].std()\\n\\n# # Calculate number of unique values for categorical columns within each group\\n# unique_values = grouped_data[categorical_cols].nunique()\\n\\n# # Identify static and dynamic columns based on characteristics\\n# for col in considered_cols:\\n#     if col in numeric_cols:\\n#         # If the standard deviation is close to zero, consider it static\\n#         if std_dev[col].max() < 1e-6:\\n#             static_num_cols.append(col)\\n#         else:\\n#             dynamic_num_cols.append(col)\\n#     elif col in categorical_cols:\\n#         # If the number of unique values is below the threshold, consider it static\\n#         if unique_values[col].max() <= unique_value_threshold:\\n#             static_cat_cols.append(col)\\n#         else:\\n#             dynamic_cat_cols.append(col)\\n\\n# # Print or use the results as needed\\n# print(\\\"Static Numeric Columns:\\\", static_num_cols)\\n# print(\\\"Dynamic Numeric Columns:\\\", dynamic_num_cols)\\n# print(\\\"Static Categorical Columns:\\\", static_cat_cols)\\n# print(\\\"Dynamic Categorical Columns:\\\", dynamic_cat_cols)\\n# print(\\\"Total columns:\\\", len(static_num_cols) + len(static_cat_cols) + len(dynamic_num_cols) + len(dynamic_cat_cols) + len(exclude_cols))\";\n                var nbb_formatted_code = \"# import pandas as pd\\n# import numpy as np\\n\\n# # Assuming 'df12' is your DataFrame\\n# data = df17.copy(deep=True)\\n# print(data.shape)\\n\\n\\n# # Get the list of all column names\\n# all_columns = data.columns\\n# print(len(all_columns))\\n\\n# # Identify the column types\\n# numeric_cols = data.select_dtypes(include='number').columns\\n# categorical_cols = data.select_dtypes(include='object').columns\\n\\n# # Exclude specified columns from consideration\\n# exclude_cols = [case_id_col, label_col, treatment_col, timestamp_col]\\n\\n# # Remove specified columns from all columns\\n# considered_cols = [col for col in all_columns if col not in exclude_cols]\\n\\n# # Create empty lists to store inferred static and dynamic columns\\n# static_num_cols = []\\n# dynamic_num_cols = []\\n# static_cat_cols = []\\n# dynamic_cat_cols = []\\n\\n# # Define threshold for unique values to consider a column as static categorical\\n# unique_value_threshold = 5\\n\\n# # Group data by 'case_id'\\n# grouped_data = data.groupby(case_id_col)\\n\\n# # Calculate standard deviation for numeric columns within each group\\n# std_dev = grouped_data[numeric_cols].std()\\n\\n# # Calculate number of unique values for categorical columns within each group\\n# unique_values = grouped_data[categorical_cols].nunique()\\n\\n# # Identify static and dynamic columns based on characteristics\\n# for col in considered_cols:\\n#     if col in numeric_cols:\\n#         # If the standard deviation is close to zero, consider it static\\n#         if std_dev[col].max() < 1e-6:\\n#             static_num_cols.append(col)\\n#         else:\\n#             dynamic_num_cols.append(col)\\n#     elif col in categorical_cols:\\n#         # If the number of unique values is below the threshold, consider it static\\n#         if unique_values[col].max() <= unique_value_threshold:\\n#             static_cat_cols.append(col)\\n#         else:\\n#             dynamic_cat_cols.append(col)\\n\\n# # Print or use the results as needed\\n# print(\\\"Static Numeric Columns:\\\", static_num_cols)\\n# print(\\\"Dynamic Numeric Columns:\\\", dynamic_num_cols)\\n# print(\\\"Static Categorical Columns:\\\", static_cat_cols)\\n# print(\\\"Dynamic Categorical Columns:\\\", dynamic_cat_cols)\\n# print(\\\"Total columns:\\\", len(static_num_cols) + len(static_cat_cols) + len(dynamic_num_cols) + len(dynamic_cat_cols) + len(exclude_cols))\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# # Assuming 'df12' is your DataFrame\n",
    "# data = df17.copy(deep=True)\n",
    "# print(data.shape)\n",
    "\n",
    "\n",
    "# # Get the list of all column names\n",
    "# all_columns = data.columns\n",
    "# print(len(all_columns))\n",
    "\n",
    "# # Identify the column types\n",
    "# numeric_cols = data.select_dtypes(include='number').columns\n",
    "# categorical_cols = data.select_dtypes(include='object').columns\n",
    "\n",
    "# # Exclude specified columns from consideration\n",
    "# exclude_cols = [case_id_col, label_col, treatment_col, timestamp_col]\n",
    "\n",
    "# # Remove specified columns from all columns\n",
    "# considered_cols = [col for col in all_columns if col not in exclude_cols]\n",
    "\n",
    "# # Create empty lists to store inferred static and dynamic columns\n",
    "# static_num_cols = []\n",
    "# dynamic_num_cols = []\n",
    "# static_cat_cols = []\n",
    "# dynamic_cat_cols = []\n",
    "\n",
    "# # Define threshold for unique values to consider a column as static categorical\n",
    "# unique_value_threshold = 5\n",
    "\n",
    "# # Group data by 'case_id'\n",
    "# grouped_data = data.groupby(case_id_col)\n",
    "\n",
    "# # Calculate standard deviation for numeric columns within each group\n",
    "# std_dev = grouped_data[numeric_cols].std()\n",
    "\n",
    "# # Calculate number of unique values for categorical columns within each group\n",
    "# unique_values = grouped_data[categorical_cols].nunique()\n",
    "\n",
    "# # Identify static and dynamic columns based on characteristics\n",
    "# for col in considered_cols:\n",
    "#     if col in numeric_cols:\n",
    "#         # If the standard deviation is close to zero, consider it static\n",
    "#         if std_dev[col].max() < 1e-6:\n",
    "#             static_num_cols.append(col)\n",
    "#         else:\n",
    "#             dynamic_num_cols.append(col)\n",
    "#     elif col in categorical_cols:\n",
    "#         # If the number of unique values is below the threshold, consider it static\n",
    "#         if unique_values[col].max() <= unique_value_threshold:\n",
    "#             static_cat_cols.append(col)\n",
    "#         else:\n",
    "#             dynamic_cat_cols.append(col)\n",
    "\n",
    "# # Print or use the results as needed\n",
    "# print(\"Static Numeric Columns:\", static_num_cols)\n",
    "# print(\"Dynamic Numeric Columns:\", dynamic_num_cols)\n",
    "# print(\"Static Categorical Columns:\", static_cat_cols)\n",
    "# print(\"Dynamic Categorical Columns:\", dynamic_cat_cols)\n",
    "# print(\"Total columns:\", len(static_num_cols) + len(static_cat_cols) + len(dynamic_num_cols) + len(dynamic_cat_cols) + len(exclude_cols))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 15;\n                var nbb_unformatted_code = \"# import pandas as pd\\n\\n# # Assuming 'df12' is your DataFrame\\n# data = df12.copy(deep=True)\\n# print(data.shape)\\n\\n# # Assuming 'case_id_col', 'timestamp_col', 'label_col', and 'treatment_col' are defined elsewhere in your code\\n# case_id_col = 'case_id'\\n# timestamp_col = 'timestamp'\\n# label_col = 'label'\\n# treatment_col = 'treatment'\\n\\n# # Get the list of all column names\\n# all_columns = data.columns\\n\\n# # Identify the column types\\n# numeric_cols = data.select_dtypes(include='number').columns\\n# categorical_cols = data.select_dtypes(include='object').columns\\n\\n# # Create empty lists to store inferred static and dynamic columns\\n# static_num_cols = []\\n# dynamic_num_cols = []\\n# static_cat_cols = []\\n# dynamic_cat_cols = []\\n\\n# # Define threshold for unique values to consider a column as static categorical\\n# unique_value_threshold = 5\\n\\n# # Group data by 'case_id'\\n# grouped_data = data.groupby(case_id_col)\\n\\n# # Iterate over each group (case)\\n# for case_id, group in grouped_data:\\n#     # Identify static and dynamic columns based on characteristics\\n#     for col in all_columns:\\n#         if col in numeric_cols:\\n#             # If the standard deviation is close to zero, consider it static\\n#             if group[col].std() < 1e-6:\\n#                 static_num_cols.append(col)\\n#             else:\\n#                 dynamic_num_cols.append(col)\\n#         elif col in categorical_cols:\\n#             # If the number of unique values is below the threshold, consider it static\\n#             if group[col].nunique() <= unique_value_threshold:\\n#                 static_cat_cols.append(col)\\n#             else:\\n#                 dynamic_cat_cols.append(col)\\n\\n# # Remove duplicates from static and dynamic column lists\\n# static_num_cols = list(set(static_num_cols))\\n# dynamic_num_cols = list(set(dynamic_num_cols))\\n# static_cat_cols = list(set(static_cat_cols))\\n# dynamic_cat_cols = list(set(dynamic_cat_cols))\\n\\n# # Print or use the results as needed\\n# print(\\\"Static Numeric Columns:\\\", static_num_cols)\\n# print(\\\"Dynamic Numeric Columns:\\\", dynamic_num_cols)\\n# print(\\\"Static Categorical Columns:\\\", static_cat_cols)\\n# print(\\\"Dynamic Categorical Columns:\\\", dynamic_cat_cols)\\n# print(\\\"Total columns:\\\", len(static_num_cols) + len(static_cat_cols) + len(dynamic_num_cols) + len(dynamic_cat_cols))\";\n                var nbb_formatted_code = \"# import pandas as pd\\n\\n# # Assuming 'df12' is your DataFrame\\n# data = df12.copy(deep=True)\\n# print(data.shape)\\n\\n# # Assuming 'case_id_col', 'timestamp_col', 'label_col', and 'treatment_col' are defined elsewhere in your code\\n# case_id_col = 'case_id'\\n# timestamp_col = 'timestamp'\\n# label_col = 'label'\\n# treatment_col = 'treatment'\\n\\n# # Get the list of all column names\\n# all_columns = data.columns\\n\\n# # Identify the column types\\n# numeric_cols = data.select_dtypes(include='number').columns\\n# categorical_cols = data.select_dtypes(include='object').columns\\n\\n# # Create empty lists to store inferred static and dynamic columns\\n# static_num_cols = []\\n# dynamic_num_cols = []\\n# static_cat_cols = []\\n# dynamic_cat_cols = []\\n\\n# # Define threshold for unique values to consider a column as static categorical\\n# unique_value_threshold = 5\\n\\n# # Group data by 'case_id'\\n# grouped_data = data.groupby(case_id_col)\\n\\n# # Iterate over each group (case)\\n# for case_id, group in grouped_data:\\n#     # Identify static and dynamic columns based on characteristics\\n#     for col in all_columns:\\n#         if col in numeric_cols:\\n#             # If the standard deviation is close to zero, consider it static\\n#             if group[col].std() < 1e-6:\\n#                 static_num_cols.append(col)\\n#             else:\\n#                 dynamic_num_cols.append(col)\\n#         elif col in categorical_cols:\\n#             # If the number of unique values is below the threshold, consider it static\\n#             if group[col].nunique() <= unique_value_threshold:\\n#                 static_cat_cols.append(col)\\n#             else:\\n#                 dynamic_cat_cols.append(col)\\n\\n# # Remove duplicates from static and dynamic column lists\\n# static_num_cols = list(set(static_num_cols))\\n# dynamic_num_cols = list(set(dynamic_num_cols))\\n# static_cat_cols = list(set(static_cat_cols))\\n# dynamic_cat_cols = list(set(dynamic_cat_cols))\\n\\n# # Print or use the results as needed\\n# print(\\\"Static Numeric Columns:\\\", static_num_cols)\\n# print(\\\"Dynamic Numeric Columns:\\\", dynamic_num_cols)\\n# print(\\\"Static Categorical Columns:\\\", static_cat_cols)\\n# print(\\\"Dynamic Categorical Columns:\\\", dynamic_cat_cols)\\n# print(\\\"Total columns:\\\", len(static_num_cols) + len(static_cat_cols) + len(dynamic_num_cols) + len(dynamic_cat_cols))\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Assuming 'df12' is your DataFrame\n",
    "# data = df12.copy(deep=True)\n",
    "# print(data.shape)\n",
    "\n",
    "# # Assuming 'case_id_col', 'timestamp_col', 'label_col', and 'treatment_col' are defined elsewhere in your code\n",
    "# case_id_col = 'case_id'\n",
    "# timestamp_col = 'timestamp'\n",
    "# label_col = 'label'\n",
    "# treatment_col = 'treatment'\n",
    "\n",
    "# # Get the list of all column names\n",
    "# all_columns = data.columns\n",
    "\n",
    "# # Identify the column types\n",
    "# numeric_cols = data.select_dtypes(include='number').columns\n",
    "# categorical_cols = data.select_dtypes(include='object').columns\n",
    "\n",
    "# # Create empty lists to store inferred static and dynamic columns\n",
    "# static_num_cols = []\n",
    "# dynamic_num_cols = []\n",
    "# static_cat_cols = []\n",
    "# dynamic_cat_cols = []\n",
    "\n",
    "# # Define threshold for unique values to consider a column as static categorical\n",
    "# unique_value_threshold = 5\n",
    "\n",
    "# # Group data by 'case_id'\n",
    "# grouped_data = data.groupby(case_id_col)\n",
    "\n",
    "# # Iterate over each group (case)\n",
    "# for case_id, group in grouped_data:\n",
    "#     # Identify static and dynamic columns based on characteristics\n",
    "#     for col in all_columns:\n",
    "#         if col in numeric_cols:\n",
    "#             # If the standard deviation is close to zero, consider it static\n",
    "#             if group[col].std() < 1e-6:\n",
    "#                 static_num_cols.append(col)\n",
    "#             else:\n",
    "#                 dynamic_num_cols.append(col)\n",
    "#         elif col in categorical_cols:\n",
    "#             # If the number of unique values is below the threshold, consider it static\n",
    "#             if group[col].nunique() <= unique_value_threshold:\n",
    "#                 static_cat_cols.append(col)\n",
    "#             else:\n",
    "#                 dynamic_cat_cols.append(col)\n",
    "\n",
    "# # Remove duplicates from static and dynamic column lists\n",
    "# static_num_cols = list(set(static_num_cols))\n",
    "# dynamic_num_cols = list(set(dynamic_num_cols))\n",
    "# static_cat_cols = list(set(static_cat_cols))\n",
    "# dynamic_cat_cols = list(set(dynamic_cat_cols))\n",
    "\n",
    "# # Print or use the results as needed\n",
    "# print(\"Static Numeric Columns:\", static_num_cols)\n",
    "# print(\"Dynamic Numeric Columns:\", dynamic_num_cols)\n",
    "# print(\"Static Categorical Columns:\", static_cat_cols)\n",
    "# print(\"Dynamic Categorical Columns:\", dynamic_cat_cols)\n",
    "# print(\"Total columns:\", len(static_num_cols) + len(static_cat_cols) + len(dynamic_num_cols) + len(dynamic_cat_cols))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 16;\n                var nbb_unformatted_code = \"# data = df12.copy(deep=True)\\n# print(data.shape)\\n# # Assuming your dataset is loaded into a DataFrame named 'data'\\n# # Assuming 'all_columns' is a list of all column names\\n# all_columns = data.columns\\n# # Identify the column types\\n# numeric_cols = data.select_dtypes(include='number').columns\\n# categorical_cols = data.select_dtypes(include='object').columns\\n\\n# # Create empty lists to store inferred static and dynamic columns\\n# static_cols = []\\n# dynamic_cols = []\\n# # Define threshold for unique values to consider a column as static categorical\\n# unique_value_threshold = 5\\n\\n# # Infer static and dynamic columns based on characteristics\\n# for col in all_columns:\\n#     if col in numeric_cols:\\n#         # If the standard deviation is close to zero, consider it static\\n#         if data[col].std() < 1e-6:\\n#             static_cols.append(col)\\n#         else:\\n#             dynamic_cols.append(col)\\n#     elif col in categorical_cols:\\n#         # If the number of unique values is below the threshold, consider it static\\n#         if data[col].nunique() <= unique_value_threshold:\\n#             static_cols.append(col)\\n#         else:\\n#             dynamic_cols.append(col)\\n\\n# # Exclude specified columns from static categorical columns\\n# exclude_static_cat_cols = [case_id_col] + [label_col] + [treatment_col] #['label', 'Treatment1', 'Treatment2', 'Treatment3', 'Treatment4']\\n# static_cat_cols = [col for col in static_cols if col in categorical_cols and col not in exclude_static_cat_cols]\\n\\n# # Exclude specified columns from dynamic categorical columns\\n# exclude_dynamic_cat_cols = case_id_col + timestamp_col #['caseid', 'timestamp']\\n# dynamic_cat_cols = [col for col in dynamic_cols if col in categorical_cols and col not in exclude_dynamic_cat_cols]\\n\\n# # Now, separate numeric and categorical columns for static and dynamic features\\n# static_num_cols = [col for col in static_cols if col in numeric_cols]\\n# dynamic_num_cols = [col for col in dynamic_cols if col in numeric_cols]\\n\\n# # Print or use the results as needed\\n# print(\\\"Static Numeric Columns:\\\", static_num_cols)\\n# print(\\\"Static Categorical Columns:\\\", static_cat_cols)\\n# print(\\\"Dynamic Numeric Columns:\\\", dynamic_num_cols)\\n# print(\\\"Dynamic Categorical Columns:\\\", dynamic_cat_cols)\";\n                var nbb_formatted_code = \"# data = df12.copy(deep=True)\\n# print(data.shape)\\n# # Assuming your dataset is loaded into a DataFrame named 'data'\\n# # Assuming 'all_columns' is a list of all column names\\n# all_columns = data.columns\\n# # Identify the column types\\n# numeric_cols = data.select_dtypes(include='number').columns\\n# categorical_cols = data.select_dtypes(include='object').columns\\n\\n# # Create empty lists to store inferred static and dynamic columns\\n# static_cols = []\\n# dynamic_cols = []\\n# # Define threshold for unique values to consider a column as static categorical\\n# unique_value_threshold = 5\\n\\n# # Infer static and dynamic columns based on characteristics\\n# for col in all_columns:\\n#     if col in numeric_cols:\\n#         # If the standard deviation is close to zero, consider it static\\n#         if data[col].std() < 1e-6:\\n#             static_cols.append(col)\\n#         else:\\n#             dynamic_cols.append(col)\\n#     elif col in categorical_cols:\\n#         # If the number of unique values is below the threshold, consider it static\\n#         if data[col].nunique() <= unique_value_threshold:\\n#             static_cols.append(col)\\n#         else:\\n#             dynamic_cols.append(col)\\n\\n# # Exclude specified columns from static categorical columns\\n# exclude_static_cat_cols = [case_id_col] + [label_col] + [treatment_col] #['label', 'Treatment1', 'Treatment2', 'Treatment3', 'Treatment4']\\n# static_cat_cols = [col for col in static_cols if col in categorical_cols and col not in exclude_static_cat_cols]\\n\\n# # Exclude specified columns from dynamic categorical columns\\n# exclude_dynamic_cat_cols = case_id_col + timestamp_col #['caseid', 'timestamp']\\n# dynamic_cat_cols = [col for col in dynamic_cols if col in categorical_cols and col not in exclude_dynamic_cat_cols]\\n\\n# # Now, separate numeric and categorical columns for static and dynamic features\\n# static_num_cols = [col for col in static_cols if col in numeric_cols]\\n# dynamic_num_cols = [col for col in dynamic_cols if col in numeric_cols]\\n\\n# # Print or use the results as needed\\n# print(\\\"Static Numeric Columns:\\\", static_num_cols)\\n# print(\\\"Static Categorical Columns:\\\", static_cat_cols)\\n# print(\\\"Dynamic Numeric Columns:\\\", dynamic_num_cols)\\n# print(\\\"Dynamic Categorical Columns:\\\", dynamic_cat_cols)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# data = df12.copy(deep=True)\n",
    "# print(data.shape)\n",
    "# # Assuming your dataset is loaded into a DataFrame named 'data'\n",
    "# # Assuming 'all_columns' is a list of all column names\n",
    "# all_columns = data.columns\n",
    "# # Identify the column types\n",
    "# numeric_cols = data.select_dtypes(include='number').columns\n",
    "# categorical_cols = data.select_dtypes(include='object').columns\n",
    "\n",
    "# # Create empty lists to store inferred static and dynamic columns\n",
    "# static_cols = []\n",
    "# dynamic_cols = []\n",
    "# # Define threshold for unique values to consider a column as static categorical\n",
    "# unique_value_threshold = 5\n",
    "\n",
    "# # Infer static and dynamic columns based on characteristics\n",
    "# for col in all_columns:\n",
    "#     if col in numeric_cols:\n",
    "#         # If the standard deviation is close to zero, consider it static\n",
    "#         if data[col].std() < 1e-6:\n",
    "#             static_cols.append(col)\n",
    "#         else:\n",
    "#             dynamic_cols.append(col)\n",
    "#     elif col in categorical_cols:\n",
    "#         # If the number of unique values is below the threshold, consider it static\n",
    "#         if data[col].nunique() <= unique_value_threshold:\n",
    "#             static_cols.append(col)\n",
    "#         else:\n",
    "#             dynamic_cols.append(col)\n",
    "\n",
    "# # Exclude specified columns from static categorical columns\n",
    "# exclude_static_cat_cols = [case_id_col] + [label_col] + [treatment_col] #['label', 'Treatment1', 'Treatment2', 'Treatment3', 'Treatment4']\n",
    "# static_cat_cols = [col for col in static_cols if col in categorical_cols and col not in exclude_static_cat_cols]\n",
    "\n",
    "# # Exclude specified columns from dynamic categorical columns\n",
    "# exclude_dynamic_cat_cols = case_id_col + timestamp_col #['caseid', 'timestamp']\n",
    "# dynamic_cat_cols = [col for col in dynamic_cols if col in categorical_cols and col not in exclude_dynamic_cat_cols]\n",
    "\n",
    "# # Now, separate numeric and categorical columns for static and dynamic features\n",
    "# static_num_cols = [col for col in static_cols if col in numeric_cols]\n",
    "# dynamic_num_cols = [col for col in dynamic_cols if col in numeric_cols]\n",
    "\n",
    "# # Print or use the results as needed\n",
    "# print(\"Static Numeric Columns:\", static_num_cols)\n",
    "# print(\"Static Categorical Columns:\", static_cat_cols)\n",
    "# print(\"Dynamic Numeric Columns:\", dynamic_num_cols)\n",
    "# print(\"Dynamic Categorical Columns:\", dynamic_cat_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 17;\n                var nbb_unformatted_code = \"# # file_path = \\\"./../prepared_data/bpic2017/date_with_treatments_bpic2017.csv\\\"\\n# # data = pd.read_csv(file_path, sep=\\\";\\\")\\n\\n\\n# import os\\n# import pandas as pd\\n# from sklearn.pipeline import FeatureUnion\\n\\n# # Define a function to encode prefixes\\n# def encode_prefixes(prefixes, feature_combiner, treatment_cols, label_col, results_dir, data_type, log_name):\\n#     x = feature_combiner.fit_transform(prefixes)\\n#     y = prefixes[label_col]\\n#     y_numeric = [1 if label == 'deviant' else 0 for label in y]\\n\\n#     T_numeric = []\\n    \\n#     if log_name == \\\"bpic2017\\\":\\n#         treatment_cols = ['Treatment1', 'Treatment2', 'Treatment3', 'Treatment4',]\\n#     else:\\n#         treatment_cols = [\\\"Treatment1\\\"]\\n\\n#     for treatment in treatment_cols:\\n#         y_treatment = prefixes[treatment]\\n#         y_treatment_numeric = [1 if label == \\\"Treatment\\\" else 0 for label in y_treatment]\\n#         T_numeric.append(y_treatment_numeric)\\n#         #y_numeric = np.vstack((y_numeric, y_treatment_numeric)).T\\n\\n#     print(\\\"Read encoded data...\\\")\\n#     df_agg = pd.read_csv(os.path.join(results_dir, 'dt_transformed_agg_%s.csv'%dataset_name), low_memory=False,  sep=';')\\n#     try:\\n#         df_static = pd.read_csv(os.path.join(results_dir, 'dt_transformed_static_%s.csv'%dataset_name), low_memory=False,  sep=';')\\n#         static_agg_df = pd.concat([df_static, df_agg], axis=1)\\n#     except:\\n#         static_agg_df = df_agg#pd.concat([df_static, df_agg], axis=1)\\n#     #static_agg_df = pd.concat([df_static, df_agg], axis=1)\\n#     print(list(static_agg_df.columns))\\n\\n#     data = pd.concat([\\n#         pd.DataFrame(x, columns=list(static_agg_df.columns)),\\n#         pd.DataFrame(y_numeric, columns=['Outcome']),\\n#         pd.DataFrame(np.array(T_numeric).T, columns=treatment_cols),\\n#         #pd.DataFrame(prefixes.groupby('case_id').first()[treatment_cols].values, columns=treatment_cols),\\n#     ], axis=1)\\n    \\n#     data.to_pickle(os.path.join(results_dir, f\\\"{data_type}_.pkl\\\"))\\n\\n#     return data\\n# # data\\n\\n# for log_name in logs:\\n#     print(\\\"\\\\n==================\\\\n Log: %s\\\\n==================\\\\n\\\" % (log_name,))\\n#     dataset_name = log_name\\n#     results_dir = \\\"./../prepared_data/%s/\\\" % dataset_name\\n\\n#     data = results_data3[logs.index(log_name)]\\n\\n\\n\\n#     # Convert timestamp column to datetime\\n#     data[timestamp_col] = pd.to_datetime(data[timestamp_col], format=\\\"mixed\\\", infer_datetime_format=True)\\n\\n#     import pandas as pd\\n\\n#     # Assuming your dataset is loaded into a DataFrame named 'data'\\n#     # Assuming 'all_columns' is a list of all column names\\n#     all_columns = data.columns\\n#     # Identify the column types\\n#     numeric_cols = data.select_dtypes(include='number').columns\\n#     categorical_cols = data.select_dtypes(include='object').columns\\n\\n#     # Create empty lists to store inferred static and dynamic columns\\n#     static_cols = []\\n#     dynamic_cols = []\\n\\n#     # Define threshold for unique values to consider a column as static categorical\\n#     unique_value_threshold = 5\\n\\n#     # Infer static and dynamic columns based on characteristics\\n#     for col in all_columns:\\n#         if col in numeric_cols:\\n#             # If the standard deviation is close to zero, consider it static\\n#             if data[col].std() < 1e-6:\\n#                 static_cols.append(col)\\n#             else:\\n#                 dynamic_cols.append(col)\\n#         elif col in categorical_cols:\\n#             # If the number of unique values is below the threshold, consider it static\\n#             if data[col].nunique() <= unique_value_threshold:\\n#                 static_cols.append(col)\\n#             else:\\n#                 dynamic_cols.append(col)\\n\\n#     # Exclude specified columns from static categorical columns\\n#     exclude_static_cat_cols = [label_col] + treatment_cols #['label', 'Treatment1', 'Treatment2', 'Treatment3', 'Treatment4']\\n#     static_cat_cols = [col for col in static_cols if col in categorical_cols and col not in exclude_static_cat_cols]\\n\\n#     # Exclude specified columns from dynamic categorical columns\\n#     exclude_dynamic_cat_cols = case_id_col + timestamp_col #['caseid', 'timestamp']\\n#     dynamic_cat_cols = [col for col in dynamic_cols if col in categorical_cols and col not in exclude_dynamic_cat_cols]\\n\\n#     # Now, separate numeric and categorical columns for static and dynamic features\\n#     static_num_cols = [col for col in static_cols if col in numeric_cols]\\n#     dynamic_num_cols = [col for col in dynamic_cols if col in numeric_cols]\\n\\n#     # Print or use the results as needed\\n#     print(\\\"Static Numeric Columns:\\\", static_num_cols)\\n#     print(\\\"Static Categorical Columns:\\\", static_cat_cols)\\n#     print(\\\"Dynamic Numeric Columns:\\\", dynamic_num_cols)\\n#     print(\\\"Dynamic Categorical Columns:\\\", dynamic_cat_cols)\\n\\n#     dataset_name = \\\"bpic2017\\\"\\n#     results_dir = \\\"./../prepared_data/%s/\\\" % dataset_name\\n#     cls_encoder_args = {'case_id_col': case_id_col,\\n#                         'static_cat_cols': static_cat_cols,\\n#                         'static_num_cols': static_num_cols,\\n#                         'dynamic_cat_cols': dynamic_cat_cols,\\n#                         'dynamic_num_cols': dynamic_num_cols,\\n#                         'fillna': True,\\n#                         'dataset_name':dataset_name,\\n#                         \\\"results_dir\\\":results_dir}\\n\\n#     min_prefix_length = 1\\n#     max_prefix_length = int(\\n#         np.ceil(data.groupby(case_id_col).size().quantile(1))\\n#     )\\n\\n\\n#     import numpy as np\\n\\n#     def split_data(data, train_ratio, val_ratio, split_type=\\\"temporal\\\", seed=22):\\n#         # Split data into train, val, and test sets based on the specified ratios\\n#         grouped = data.groupby(case_id_col)\\n#         start_timestamps = grouped[timestamp_col].min().reset_index()\\n\\n#         # Sort start_timestamps based on the split_type\\n#         if split_type == \\\"temporal\\\":\\n#             start_timestamps = start_timestamps.sort_values(timestamp_col, ascending=True, kind=\\\"mergesort\\\")\\n#         elif split_type == \\\"random\\\":\\n#             np.random.seed(seed)\\n#             start_timestamps = start_timestamps.reindex(np.random.permutation(start_timestamps.index))\\n\\n#         train_size = int(train_ratio * len(start_timestamps))\\n#         val_size = int(val_ratio * len(start_timestamps))\\n#         test_size = len(start_timestamps) - train_size - val_size\\n\\n#         train_ids = list(start_timestamps[case_id_col])[:train_size]\\n#         val_ids = list(start_timestamps[case_id_col])[train_size:train_size + val_size]\\n#         test_ids = list(start_timestamps[case_id_col])[train_size + val_size:]\\n\\n#         train = data[data[case_id_col].isin(train_ids)].sort_values(sorting_cols, ascending=True, kind='mergesort').reset_index(drop=True)\\n#         val = data[data[case_id_col].isin(val_ids)].sort_values(sorting_cols, ascending=True, kind='mergesort').reset_index(drop=True)\\n#         test = data[data[case_id_col].isin(test_ids)].sort_values(sorting_cols, ascending=True, kind='mergesort').reset_index(drop=True)\\n\\n#         return train, val, test\\n\\n#     # \\n#     sorting_cols = [timestamp_col, activity_col]\\n\\n\\n#     # Specify the desired ratios\\n#     train_ratio = 0.5\\n#     val_ratio = 0.3\\n#     test_ratio = 0.2\\n\\n#     # Split the data into train, val, and test\\n#     train, val, test = split_data(data, train_ratio, val_ratio, split_type=\\\"temporal\\\", seed=22)\\n#     print(\\\"Number of training cases: \\\", train[case_id_col].nunique())\\n#     print(\\\"Number of validation cases: \\\", val[case_id_col].nunique())\\n#     print(\\\"Number of test cases: \\\", test[case_id_col].nunique())\\n\\n#     print(\\\"shape of train: \\\", train.shape)\\n#     print(\\\"shape of val: \\\", val.shape)\\n#     print(\\\"shape of test: \\\", test.shape)\\n\\n#     # Save the train, val, and test sets to separate files\\n#     print(\\\"Saving the train, val, and test sets to separate files...\\\")\\n#     train.to_csv(results_dir + \\\"train_%s.csv\\\"%dataset_name, sep=\\\";\\\", index=False)\\n#     val.to_csv(results_dir + \\\"val_%s.csv\\\"%dataset_name, sep=\\\";\\\", index=False)\\n#     test.to_csv(results_dir + \\\"test_%s.csv\\\"%dataset_name, sep=\\\";\\\", index=False)\\n\\n#     print(\\\"Done!\\\")\\n\\n#     # Create a FeatureUnion with specified methods\\n#     feature_combiner = FeatureUnion([\\n#         (method, EncoderFactory.get_encoder(method, **cls_encoder_args))\\n#         for method in [\\\"static\\\", \\\"agg\\\"]\\n#     ])\\n#     print(\\\"Start encoding...\\\")\\n\\n#     # train = pd.read_csv(results_dir + \\\"train_%s.csv\\\"%dataset_name, sep=\\\";\\\")\\n#     # val = pd.read_csv(results_dir + \\\"val_%s.csv\\\"%dataset_name, sep=\\\";\\\")\\n#     # test = pd.read_csv(results_dir + \\\"test_%s.csv\\\"%dataset_name, sep=\\\";\\\")\\n#     # print(train.columns)\\n\\n#     if log_name == \\\"bpic2017\\\":\\n#         treatment_cols = ['Treatment1', 'Treatment2', 'Treatment3', 'Treatment4',]\\n#     else:\\n#         treatment_cols = [\\\"Treatment1\\\"]\\n\\n\\n#     # Encode training data\\n#     train_encoded = encode_prefixes(train, feature_combiner, treatment_cols, label_col, results_dir, \\\"train\\\", log_name)\\n#     val_encoded = encode_prefixes(val, feature_combiner, treatment_cols, label_col, results_dir, \\\"valid\\\", log_name)\\n#     test_encoded = encode_prefixes(test, feature_combiner, treatment_cols, label_col, results_dir, \\\"test\\\", log_name)\\n\\n#     print(\\\"Train Encoded data shape: \\\", train_encoded.shape)\\n#     print(\\\"Val Encoded data shape: \\\",  val_encoded.shape)\\n#     print(\\\"Test Encoded data shape: \\\",  test_encoded.shape)\\n\\n#     # save encoded data\\n#     print(\\\"Save encoded data...\\\")\\n#     train_encoded.to_csv(results_dir + \\\"train_encoded_%s.csv\\\"%dataset_name, sep=\\\";\\\", index=False)\\n#     val_encoded.to_csv(results_dir + \\\"val_encoded_%s.csv\\\"%dataset_name, sep=\\\";\\\", index=False)\\n#     test_encoded.to_csv(results_dir + \\\"test_encoded_%s.csv\\\"%dataset_name, sep=\\\";\\\", index=False)\";\n                var nbb_formatted_code = \"# # file_path = \\\"./../prepared_data/bpic2017/date_with_treatments_bpic2017.csv\\\"\\n# # data = pd.read_csv(file_path, sep=\\\";\\\")\\n\\n\\n# import os\\n# import pandas as pd\\n# from sklearn.pipeline import FeatureUnion\\n\\n# # Define a function to encode prefixes\\n# def encode_prefixes(prefixes, feature_combiner, treatment_cols, label_col, results_dir, data_type, log_name):\\n#     x = feature_combiner.fit_transform(prefixes)\\n#     y = prefixes[label_col]\\n#     y_numeric = [1 if label == 'deviant' else 0 for label in y]\\n\\n#     T_numeric = []\\n\\n#     if log_name == \\\"bpic2017\\\":\\n#         treatment_cols = ['Treatment1', 'Treatment2', 'Treatment3', 'Treatment4',]\\n#     else:\\n#         treatment_cols = [\\\"Treatment1\\\"]\\n\\n#     for treatment in treatment_cols:\\n#         y_treatment = prefixes[treatment]\\n#         y_treatment_numeric = [1 if label == \\\"Treatment\\\" else 0 for label in y_treatment]\\n#         T_numeric.append(y_treatment_numeric)\\n#         #y_numeric = np.vstack((y_numeric, y_treatment_numeric)).T\\n\\n#     print(\\\"Read encoded data...\\\")\\n#     df_agg = pd.read_csv(os.path.join(results_dir, 'dt_transformed_agg_%s.csv'%dataset_name), low_memory=False,  sep=';')\\n#     try:\\n#         df_static = pd.read_csv(os.path.join(results_dir, 'dt_transformed_static_%s.csv'%dataset_name), low_memory=False,  sep=';')\\n#         static_agg_df = pd.concat([df_static, df_agg], axis=1)\\n#     except:\\n#         static_agg_df = df_agg#pd.concat([df_static, df_agg], axis=1)\\n#     #static_agg_df = pd.concat([df_static, df_agg], axis=1)\\n#     print(list(static_agg_df.columns))\\n\\n#     data = pd.concat([\\n#         pd.DataFrame(x, columns=list(static_agg_df.columns)),\\n#         pd.DataFrame(y_numeric, columns=['Outcome']),\\n#         pd.DataFrame(np.array(T_numeric).T, columns=treatment_cols),\\n#         #pd.DataFrame(prefixes.groupby('case_id').first()[treatment_cols].values, columns=treatment_cols),\\n#     ], axis=1)\\n\\n#     data.to_pickle(os.path.join(results_dir, f\\\"{data_type}_.pkl\\\"))\\n\\n#     return data\\n# # data\\n\\n# for log_name in logs:\\n#     print(\\\"\\\\n==================\\\\n Log: %s\\\\n==================\\\\n\\\" % (log_name,))\\n#     dataset_name = log_name\\n#     results_dir = \\\"./../prepared_data/%s/\\\" % dataset_name\\n\\n#     data = results_data3[logs.index(log_name)]\\n\\n\\n#     # Convert timestamp column to datetime\\n#     data[timestamp_col] = pd.to_datetime(data[timestamp_col], format=\\\"mixed\\\", infer_datetime_format=True)\\n\\n#     import pandas as pd\\n\\n#     # Assuming your dataset is loaded into a DataFrame named 'data'\\n#     # Assuming 'all_columns' is a list of all column names\\n#     all_columns = data.columns\\n#     # Identify the column types\\n#     numeric_cols = data.select_dtypes(include='number').columns\\n#     categorical_cols = data.select_dtypes(include='object').columns\\n\\n#     # Create empty lists to store inferred static and dynamic columns\\n#     static_cols = []\\n#     dynamic_cols = []\\n\\n#     # Define threshold for unique values to consider a column as static categorical\\n#     unique_value_threshold = 5\\n\\n#     # Infer static and dynamic columns based on characteristics\\n#     for col in all_columns:\\n#         if col in numeric_cols:\\n#             # If the standard deviation is close to zero, consider it static\\n#             if data[col].std() < 1e-6:\\n#                 static_cols.append(col)\\n#             else:\\n#                 dynamic_cols.append(col)\\n#         elif col in categorical_cols:\\n#             # If the number of unique values is below the threshold, consider it static\\n#             if data[col].nunique() <= unique_value_threshold:\\n#                 static_cols.append(col)\\n#             else:\\n#                 dynamic_cols.append(col)\\n\\n#     # Exclude specified columns from static categorical columns\\n#     exclude_static_cat_cols = [label_col] + treatment_cols #['label', 'Treatment1', 'Treatment2', 'Treatment3', 'Treatment4']\\n#     static_cat_cols = [col for col in static_cols if col in categorical_cols and col not in exclude_static_cat_cols]\\n\\n#     # Exclude specified columns from dynamic categorical columns\\n#     exclude_dynamic_cat_cols = case_id_col + timestamp_col #['caseid', 'timestamp']\\n#     dynamic_cat_cols = [col for col in dynamic_cols if col in categorical_cols and col not in exclude_dynamic_cat_cols]\\n\\n#     # Now, separate numeric and categorical columns for static and dynamic features\\n#     static_num_cols = [col for col in static_cols if col in numeric_cols]\\n#     dynamic_num_cols = [col for col in dynamic_cols if col in numeric_cols]\\n\\n#     # Print or use the results as needed\\n#     print(\\\"Static Numeric Columns:\\\", static_num_cols)\\n#     print(\\\"Static Categorical Columns:\\\", static_cat_cols)\\n#     print(\\\"Dynamic Numeric Columns:\\\", dynamic_num_cols)\\n#     print(\\\"Dynamic Categorical Columns:\\\", dynamic_cat_cols)\\n\\n#     dataset_name = \\\"bpic2017\\\"\\n#     results_dir = \\\"./../prepared_data/%s/\\\" % dataset_name\\n#     cls_encoder_args = {'case_id_col': case_id_col,\\n#                         'static_cat_cols': static_cat_cols,\\n#                         'static_num_cols': static_num_cols,\\n#                         'dynamic_cat_cols': dynamic_cat_cols,\\n#                         'dynamic_num_cols': dynamic_num_cols,\\n#                         'fillna': True,\\n#                         'dataset_name':dataset_name,\\n#                         \\\"results_dir\\\":results_dir}\\n\\n#     min_prefix_length = 1\\n#     max_prefix_length = int(\\n#         np.ceil(data.groupby(case_id_col).size().quantile(1))\\n#     )\\n\\n\\n#     import numpy as np\\n\\n#     def split_data(data, train_ratio, val_ratio, split_type=\\\"temporal\\\", seed=22):\\n#         # Split data into train, val, and test sets based on the specified ratios\\n#         grouped = data.groupby(case_id_col)\\n#         start_timestamps = grouped[timestamp_col].min().reset_index()\\n\\n#         # Sort start_timestamps based on the split_type\\n#         if split_type == \\\"temporal\\\":\\n#             start_timestamps = start_timestamps.sort_values(timestamp_col, ascending=True, kind=\\\"mergesort\\\")\\n#         elif split_type == \\\"random\\\":\\n#             np.random.seed(seed)\\n#             start_timestamps = start_timestamps.reindex(np.random.permutation(start_timestamps.index))\\n\\n#         train_size = int(train_ratio * len(start_timestamps))\\n#         val_size = int(val_ratio * len(start_timestamps))\\n#         test_size = len(start_timestamps) - train_size - val_size\\n\\n#         train_ids = list(start_timestamps[case_id_col])[:train_size]\\n#         val_ids = list(start_timestamps[case_id_col])[train_size:train_size + val_size]\\n#         test_ids = list(start_timestamps[case_id_col])[train_size + val_size:]\\n\\n#         train = data[data[case_id_col].isin(train_ids)].sort_values(sorting_cols, ascending=True, kind='mergesort').reset_index(drop=True)\\n#         val = data[data[case_id_col].isin(val_ids)].sort_values(sorting_cols, ascending=True, kind='mergesort').reset_index(drop=True)\\n#         test = data[data[case_id_col].isin(test_ids)].sort_values(sorting_cols, ascending=True, kind='mergesort').reset_index(drop=True)\\n\\n#         return train, val, test\\n\\n#     #\\n#     sorting_cols = [timestamp_col, activity_col]\\n\\n\\n#     # Specify the desired ratios\\n#     train_ratio = 0.5\\n#     val_ratio = 0.3\\n#     test_ratio = 0.2\\n\\n#     # Split the data into train, val, and test\\n#     train, val, test = split_data(data, train_ratio, val_ratio, split_type=\\\"temporal\\\", seed=22)\\n#     print(\\\"Number of training cases: \\\", train[case_id_col].nunique())\\n#     print(\\\"Number of validation cases: \\\", val[case_id_col].nunique())\\n#     print(\\\"Number of test cases: \\\", test[case_id_col].nunique())\\n\\n#     print(\\\"shape of train: \\\", train.shape)\\n#     print(\\\"shape of val: \\\", val.shape)\\n#     print(\\\"shape of test: \\\", test.shape)\\n\\n#     # Save the train, val, and test sets to separate files\\n#     print(\\\"Saving the train, val, and test sets to separate files...\\\")\\n#     train.to_csv(results_dir + \\\"train_%s.csv\\\"%dataset_name, sep=\\\";\\\", index=False)\\n#     val.to_csv(results_dir + \\\"val_%s.csv\\\"%dataset_name, sep=\\\";\\\", index=False)\\n#     test.to_csv(results_dir + \\\"test_%s.csv\\\"%dataset_name, sep=\\\";\\\", index=False)\\n\\n#     print(\\\"Done!\\\")\\n\\n#     # Create a FeatureUnion with specified methods\\n#     feature_combiner = FeatureUnion([\\n#         (method, EncoderFactory.get_encoder(method, **cls_encoder_args))\\n#         for method in [\\\"static\\\", \\\"agg\\\"]\\n#     ])\\n#     print(\\\"Start encoding...\\\")\\n\\n#     # train = pd.read_csv(results_dir + \\\"train_%s.csv\\\"%dataset_name, sep=\\\";\\\")\\n#     # val = pd.read_csv(results_dir + \\\"val_%s.csv\\\"%dataset_name, sep=\\\";\\\")\\n#     # test = pd.read_csv(results_dir + \\\"test_%s.csv\\\"%dataset_name, sep=\\\";\\\")\\n#     # print(train.columns)\\n\\n#     if log_name == \\\"bpic2017\\\":\\n#         treatment_cols = ['Treatment1', 'Treatment2', 'Treatment3', 'Treatment4',]\\n#     else:\\n#         treatment_cols = [\\\"Treatment1\\\"]\\n\\n\\n#     # Encode training data\\n#     train_encoded = encode_prefixes(train, feature_combiner, treatment_cols, label_col, results_dir, \\\"train\\\", log_name)\\n#     val_encoded = encode_prefixes(val, feature_combiner, treatment_cols, label_col, results_dir, \\\"valid\\\", log_name)\\n#     test_encoded = encode_prefixes(test, feature_combiner, treatment_cols, label_col, results_dir, \\\"test\\\", log_name)\\n\\n#     print(\\\"Train Encoded data shape: \\\", train_encoded.shape)\\n#     print(\\\"Val Encoded data shape: \\\",  val_encoded.shape)\\n#     print(\\\"Test Encoded data shape: \\\",  test_encoded.shape)\\n\\n#     # save encoded data\\n#     print(\\\"Save encoded data...\\\")\\n#     train_encoded.to_csv(results_dir + \\\"train_encoded_%s.csv\\\"%dataset_name, sep=\\\";\\\", index=False)\\n#     val_encoded.to_csv(results_dir + \\\"val_encoded_%s.csv\\\"%dataset_name, sep=\\\";\\\", index=False)\\n#     test_encoded.to_csv(results_dir + \\\"test_encoded_%s.csv\\\"%dataset_name, sep=\\\";\\\", index=False)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # file_path = \"./../prepared_data/bpic2017/date_with_treatments_bpic2017.csv\"\n",
    "# # data = pd.read_csv(file_path, sep=\";\")\n",
    "\n",
    "\n",
    "# import os\n",
    "# import pandas as pd\n",
    "# from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "# # Define a function to encode prefixes\n",
    "# def encode_prefixes(prefixes, feature_combiner, treatment_cols, label_col, results_dir, data_type, log_name):\n",
    "#     x = feature_combiner.fit_transform(prefixes)\n",
    "#     y = prefixes[label_col]\n",
    "#     y_numeric = [1 if label == 'deviant' else 0 for label in y]\n",
    "\n",
    "#     T_numeric = []\n",
    "    \n",
    "#     if log_name == \"bpic2017\":\n",
    "#         treatment_cols = ['Treatment1', 'Treatment2', 'Treatment3', 'Treatment4',]\n",
    "#     else:\n",
    "#         treatment_cols = [\"Treatment1\"]\n",
    "\n",
    "#     for treatment in treatment_cols:\n",
    "#         y_treatment = prefixes[treatment]\n",
    "#         y_treatment_numeric = [1 if label == \"Treatment\" else 0 for label in y_treatment]\n",
    "#         T_numeric.append(y_treatment_numeric)\n",
    "#         #y_numeric = np.vstack((y_numeric, y_treatment_numeric)).T\n",
    "\n",
    "#     print(\"Read encoded data...\")\n",
    "#     df_agg = pd.read_csv(os.path.join(results_dir, 'dt_transformed_agg_%s.csv'%dataset_name), low_memory=False,  sep=';')\n",
    "#     try:\n",
    "#         df_static = pd.read_csv(os.path.join(results_dir, 'dt_transformed_static_%s.csv'%dataset_name), low_memory=False,  sep=';')\n",
    "#         static_agg_df = pd.concat([df_static, df_agg], axis=1)\n",
    "#     except:\n",
    "#         static_agg_df = df_agg#pd.concat([df_static, df_agg], axis=1)\n",
    "#     #static_agg_df = pd.concat([df_static, df_agg], axis=1)\n",
    "#     print(list(static_agg_df.columns))\n",
    "\n",
    "#     data = pd.concat([\n",
    "#         pd.DataFrame(x, columns=list(static_agg_df.columns)),\n",
    "#         pd.DataFrame(y_numeric, columns=['Outcome']),\n",
    "#         pd.DataFrame(np.array(T_numeric).T, columns=treatment_cols),\n",
    "#         #pd.DataFrame(prefixes.groupby('case_id').first()[treatment_cols].values, columns=treatment_cols),\n",
    "#     ], axis=1)\n",
    "    \n",
    "#     data.to_pickle(os.path.join(results_dir, f\"{data_type}_.pkl\"))\n",
    "\n",
    "#     return data\n",
    "# # data\n",
    "\n",
    "# for log_name in logs:\n",
    "#     print(\"\\n==================\\n Log: %s\\n==================\\n\" % (log_name,))\n",
    "#     dataset_name = log_name\n",
    "#     results_dir = \"./../prepared_data/%s/\" % dataset_name\n",
    "\n",
    "#     data = results_data3[logs.index(log_name)]\n",
    "\n",
    "\n",
    "\n",
    "#     # Convert timestamp column to datetime\n",
    "#     data[timestamp_col] = pd.to_datetime(data[timestamp_col], format=\"mixed\", infer_datetime_format=True)\n",
    "\n",
    "#     import pandas as pd\n",
    "\n",
    "#     # Assuming your dataset is loaded into a DataFrame named 'data'\n",
    "#     # Assuming 'all_columns' is a list of all column names\n",
    "#     all_columns = data.columns\n",
    "#     # Identify the column types\n",
    "#     numeric_cols = data.select_dtypes(include='number').columns\n",
    "#     categorical_cols = data.select_dtypes(include='object').columns\n",
    "\n",
    "#     # Create empty lists to store inferred static and dynamic columns\n",
    "#     static_cols = []\n",
    "#     dynamic_cols = []\n",
    "\n",
    "#     # Define threshold for unique values to consider a column as static categorical\n",
    "#     unique_value_threshold = 5\n",
    "\n",
    "#     # Infer static and dynamic columns based on characteristics\n",
    "#     for col in all_columns:\n",
    "#         if col in numeric_cols:\n",
    "#             # If the standard deviation is close to zero, consider it static\n",
    "#             if data[col].std() < 1e-6:\n",
    "#                 static_cols.append(col)\n",
    "#             else:\n",
    "#                 dynamic_cols.append(col)\n",
    "#         elif col in categorical_cols:\n",
    "#             # If the number of unique values is below the threshold, consider it static\n",
    "#             if data[col].nunique() <= unique_value_threshold:\n",
    "#                 static_cols.append(col)\n",
    "#             else:\n",
    "#                 dynamic_cols.append(col)\n",
    "\n",
    "#     # Exclude specified columns from static categorical columns\n",
    "#     exclude_static_cat_cols = [label_col] + treatment_cols #['label', 'Treatment1', 'Treatment2', 'Treatment3', 'Treatment4']\n",
    "#     static_cat_cols = [col for col in static_cols if col in categorical_cols and col not in exclude_static_cat_cols]\n",
    "\n",
    "#     # Exclude specified columns from dynamic categorical columns\n",
    "#     exclude_dynamic_cat_cols = case_id_col + timestamp_col #['caseid', 'timestamp']\n",
    "#     dynamic_cat_cols = [col for col in dynamic_cols if col in categorical_cols and col not in exclude_dynamic_cat_cols]\n",
    "\n",
    "#     # Now, separate numeric and categorical columns for static and dynamic features\n",
    "#     static_num_cols = [col for col in static_cols if col in numeric_cols]\n",
    "#     dynamic_num_cols = [col for col in dynamic_cols if col in numeric_cols]\n",
    "\n",
    "#     # Print or use the results as needed\n",
    "#     print(\"Static Numeric Columns:\", static_num_cols)\n",
    "#     print(\"Static Categorical Columns:\", static_cat_cols)\n",
    "#     print(\"Dynamic Numeric Columns:\", dynamic_num_cols)\n",
    "#     print(\"Dynamic Categorical Columns:\", dynamic_cat_cols)\n",
    "\n",
    "#     dataset_name = \"bpic2017\"\n",
    "#     results_dir = \"./../prepared_data/%s/\" % dataset_name\n",
    "#     cls_encoder_args = {'case_id_col': case_id_col,\n",
    "#                         'static_cat_cols': static_cat_cols,\n",
    "#                         'static_num_cols': static_num_cols,\n",
    "#                         'dynamic_cat_cols': dynamic_cat_cols,\n",
    "#                         'dynamic_num_cols': dynamic_num_cols,\n",
    "#                         'fillna': True,\n",
    "#                         'dataset_name':dataset_name,\n",
    "#                         \"results_dir\":results_dir}\n",
    "\n",
    "#     min_prefix_length = 1\n",
    "#     max_prefix_length = int(\n",
    "#         np.ceil(data.groupby(case_id_col).size().quantile(1))\n",
    "#     )\n",
    "\n",
    "\n",
    "#     import numpy as np\n",
    "\n",
    "#     def split_data(data, train_ratio, val_ratio, split_type=\"temporal\", seed=22):\n",
    "#         # Split data into train, val, and test sets based on the specified ratios\n",
    "#         grouped = data.groupby(case_id_col)\n",
    "#         start_timestamps = grouped[timestamp_col].min().reset_index()\n",
    "\n",
    "#         # Sort start_timestamps based on the split_type\n",
    "#         if split_type == \"temporal\":\n",
    "#             start_timestamps = start_timestamps.sort_values(timestamp_col, ascending=True, kind=\"mergesort\")\n",
    "#         elif split_type == \"random\":\n",
    "#             np.random.seed(seed)\n",
    "#             start_timestamps = start_timestamps.reindex(np.random.permutation(start_timestamps.index))\n",
    "\n",
    "#         train_size = int(train_ratio * len(start_timestamps))\n",
    "#         val_size = int(val_ratio * len(start_timestamps))\n",
    "#         test_size = len(start_timestamps) - train_size - val_size\n",
    "\n",
    "#         train_ids = list(start_timestamps[case_id_col])[:train_size]\n",
    "#         val_ids = list(start_timestamps[case_id_col])[train_size:train_size + val_size]\n",
    "#         test_ids = list(start_timestamps[case_id_col])[train_size + val_size:]\n",
    "\n",
    "#         train = data[data[case_id_col].isin(train_ids)].sort_values(sorting_cols, ascending=True, kind='mergesort').reset_index(drop=True)\n",
    "#         val = data[data[case_id_col].isin(val_ids)].sort_values(sorting_cols, ascending=True, kind='mergesort').reset_index(drop=True)\n",
    "#         test = data[data[case_id_col].isin(test_ids)].sort_values(sorting_cols, ascending=True, kind='mergesort').reset_index(drop=True)\n",
    "\n",
    "#         return train, val, test\n",
    "\n",
    "#     # \n",
    "#     sorting_cols = [timestamp_col, activity_col]\n",
    "\n",
    "\n",
    "#     # Specify the desired ratios\n",
    "#     train_ratio = 0.5\n",
    "#     val_ratio = 0.3\n",
    "#     test_ratio = 0.2\n",
    "\n",
    "#     # Split the data into train, val, and test\n",
    "#     train, val, test = split_data(data, train_ratio, val_ratio, split_type=\"temporal\", seed=22)\n",
    "#     print(\"Number of training cases: \", train[case_id_col].nunique())\n",
    "#     print(\"Number of validation cases: \", val[case_id_col].nunique())\n",
    "#     print(\"Number of test cases: \", test[case_id_col].nunique())\n",
    "\n",
    "#     print(\"shape of train: \", train.shape)\n",
    "#     print(\"shape of val: \", val.shape)\n",
    "#     print(\"shape of test: \", test.shape)\n",
    "\n",
    "#     # Save the train, val, and test sets to separate files\n",
    "#     print(\"Saving the train, val, and test sets to separate files...\")\n",
    "#     train.to_csv(results_dir + \"train_%s.csv\"%dataset_name, sep=\";\", index=False)\n",
    "#     val.to_csv(results_dir + \"val_%s.csv\"%dataset_name, sep=\";\", index=False)\n",
    "#     test.to_csv(results_dir + \"test_%s.csv\"%dataset_name, sep=\";\", index=False)\n",
    "\n",
    "#     print(\"Done!\")\n",
    "\n",
    "#     # Create a FeatureUnion with specified methods\n",
    "#     feature_combiner = FeatureUnion([\n",
    "#         (method, EncoderFactory.get_encoder(method, **cls_encoder_args))\n",
    "#         for method in [\"static\", \"agg\"]\n",
    "#     ])\n",
    "#     print(\"Start encoding...\")\n",
    "\n",
    "#     # train = pd.read_csv(results_dir + \"train_%s.csv\"%dataset_name, sep=\";\")\n",
    "#     # val = pd.read_csv(results_dir + \"val_%s.csv\"%dataset_name, sep=\";\")\n",
    "#     # test = pd.read_csv(results_dir + \"test_%s.csv\"%dataset_name, sep=\";\")\n",
    "#     # print(train.columns)\n",
    "\n",
    "#     if log_name == \"bpic2017\":\n",
    "#         treatment_cols = ['Treatment1', 'Treatment2', 'Treatment3', 'Treatment4',]\n",
    "#     else:\n",
    "#         treatment_cols = [\"Treatment1\"]\n",
    "\n",
    "\n",
    "#     # Encode training data\n",
    "#     train_encoded = encode_prefixes(train, feature_combiner, treatment_cols, label_col, results_dir, \"train\", log_name)\n",
    "#     val_encoded = encode_prefixes(val, feature_combiner, treatment_cols, label_col, results_dir, \"valid\", log_name)\n",
    "#     test_encoded = encode_prefixes(test, feature_combiner, treatment_cols, label_col, results_dir, \"test\", log_name)\n",
    "\n",
    "#     print(\"Train Encoded data shape: \", train_encoded.shape)\n",
    "#     print(\"Val Encoded data shape: \",  val_encoded.shape)\n",
    "#     print(\"Test Encoded data shape: \",  test_encoded.shape)\n",
    "\n",
    "#     # save encoded data\n",
    "#     print(\"Save encoded data...\")\n",
    "#     train_encoded.to_csv(results_dir + \"train_encoded_%s.csv\"%dataset_name, sep=\";\", index=False)\n",
    "#     val_encoded.to_csv(results_dir + \"val_encoded_%s.csv\"%dataset_name, sep=\";\", index=False)\n",
    "#     test_encoded.to_csv(results_dir + \"test_encoded_%s.csv\"%dataset_name, sep=\";\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 18;\n                var nbb_unformatted_code = \"# df12.dtypes\";\n                var nbb_formatted_code = \"# df12.dtypes\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# df12.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prpm_py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
