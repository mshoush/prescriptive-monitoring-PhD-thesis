{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pandarallel\n",
    "# !pip install swifter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/mshoush/5th/common_files')  \n",
    "\n",
    "\n",
    "from DatasetManager import DatasetManager\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "import os\n",
    "import EncoderFactory\n",
    "\n",
    "\n",
    "case_id_col = 'case_id'\n",
    "activity_col = 'activity'\n",
    "resource_col = 'resource'\n",
    "timestamp_col = 'timestamp'\n",
    "label_col = 'label'\n",
    "treatment_col = \"Treatment1\"\n",
    "\n",
    "results_dir = \"./../prepared_data/\"\n",
    "cls_methods = [\"catboost\", \"other\",] # \"other\", \n",
    "\n",
    "dataset_ref_to_datasets = {\n",
    "    \"bpic2012\": [\"bpic2012\"], \n",
    "    \"bpic2017\": [\"bpic2017\"],\n",
    "        \n",
    "}\n",
    "\n",
    "encoding_dict = {  \n",
    "    \"index\": [\"static\", \"index\"],  \n",
    "    \"laststate\": [\"static\", \"last\"],\n",
    "    \"agg\": [\"static\", \"agg\"],       \n",
    "    \"combined\": [\"static\", \"last\", \"agg\"]\n",
    "    }\n",
    "\n",
    "task_types = [\"regression\", \"classification\",]\n",
    "\n",
    "\n",
    "    \n",
    "fillna = True\n",
    "\n",
    "# Specify the desired ratios\n",
    "train_ratio = 0.5\n",
    "val_ratio = 0.3\n",
    "test_ratio = 0.2\n",
    "\n",
    "    \n",
    "def save_file_to_parquet(file, results_dir, name, dataset_name, cat_cols=None):\n",
    "    if not os.path.exists(results_dir):\n",
    "        os.makedirs(results_dir)\n",
    "        \n",
    "    if cat_cols is not None:\n",
    "        file.iloc[:, cat_cols] = file.iloc[:, cat_cols].astype(str)\n",
    "\n",
    "    try:     \n",
    "        file[case_id_col] = file[case_id_col].astype(str)\n",
    "    except:\n",
    "        pass      \n",
    "    file.to_parquet(os.path.join(results_dir, name+\"_%s.parquet\" % dataset_name))\n",
    "    \n",
    "    \n",
    "    \n",
    "def transform_chunked(feature_combiner, prefixes, chunk_size):\n",
    "    x = []\n",
    "    unique_groups = prefixes[\"case_id\"].unique()\n",
    "    print(len(unique_groups))\n",
    "    \n",
    "    for start_idx in range(0, len(unique_groups), chunk_size):\n",
    "        end_idx = min(start_idx + chunk_size, len(unique_groups))\n",
    "        chunk_groups = unique_groups[start_idx:end_idx]\n",
    "        \n",
    "        chunk_prefixes = prefixes[prefixes[\"case_id\"].isin(chunk_groups)]\n",
    "        transformed_chunk = feature_combiner.transform(chunk_prefixes)\n",
    "        x.append(transformed_chunk)\n",
    "    \n",
    "    return np.vstack(x)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def encode_data(feature_combiner, prefixes, task_type, cls_encoding='index'):\n",
    "    if cls_encoding=='index':\n",
    "        x = transform_chunked(feature_combiner, prefixes, chunk_size=1000)         \n",
    "    else:\n",
    "        x = feature_combiner.transform(prefixes)\n",
    "                   \n",
    "    if task_type == \"classification\":        \n",
    "        y = dataset_manager.get_label_numeric(prefixes)\n",
    "    elif task_type == \"regression\":\n",
    "        y = dataset_manager.get_label_regression(prefixes)\n",
    "            \n",
    "    t = dataset_manager.get_treatment_numeric(prefixes)\n",
    "    \n",
    "    data_np = np.column_stack((x, y, t))\n",
    "    data = pd.DataFrame(data_np)\n",
    "    \n",
    "    return data\n",
    "\n",
    "  \n",
    "  \n",
    "def process_data(dataset, data_type):\n",
    "    gc.collect()\n",
    "    dt_prefixes = dataset_manager.generate_prefix_data(dataset, min_prefix_length, max_prefix_length)\n",
    "    save_file_to_parquet(dt_prefixes, results_dir, f\"{data_type}_prefixes\", dataset_name)\n",
    "    #print(f\"dt_{data_type}_prefixes.shape: \", dt_prefixes.shape)\n",
    "    encoded_data = encode_data(feature_combiner, dt_prefixes, task_type)\n",
    "    del dt_prefixes\n",
    "    gc.collect()\n",
    "    #print(\" \")\n",
    "    return encoded_data\n",
    "\n",
    "\n",
    "def read_encoded_data(cls_encoding, dataset_name, results_dir):\n",
    "    gc.collect()\n",
    "    if cls_encoding == \"combined\":\n",
    "        df_agg = pd.read_parquet(os.path.join(results_dir, 'dt_transformed_agg_%s_%s.parquet' % (cls_method, dataset_name)))\n",
    "        df_static = pd.read_parquet(os.path.join(results_dir, 'dt_transformed_static_%s_%s.parquet' % (cls_method, dataset_name)))\n",
    "        df_last = pd.read_parquet(os.path.join(results_dir, 'dt_transformed_laststate_%s_%s.parquet' % (cls_method, dataset_name)))\n",
    "        static_agg_df = pd.concat([df_static, df_last, df_agg], axis=1)\n",
    "        del df_agg, df_static, df_last\n",
    "    else:\n",
    "        #print(\"read agg and static\")\n",
    "        df_agg = pd.read_parquet(os.path.join(results_dir, 'dt_transformed_%s_%s_%s.parquet' % (cls_encoding, cls_method, dataset_name)))\n",
    "        df_static = pd.read_parquet(os.path.join(results_dir, 'dt_transformed_static_%s_%s.parquet' % (cls_method, dataset_name)))\n",
    "        static_agg_df = pd.concat([df_static, df_agg], axis=1)\n",
    "        del df_agg, df_static\n",
    "    gc.collect()\n",
    "    return static_agg_df\n",
    "\n",
    "def save_encoded_data(encoded_data, results_dir, dataset_name, cls_encoding, data_type):\n",
    "    gc.collect()\n",
    "    #print(\"read static_agg_df\")\n",
    "    static_agg_df = read_encoded_data(cls_encoding, dataset_name, results_dir).head()\n",
    "    encoded_data.columns = list(static_agg_df.columns) + [str(dataset_manager.label_col)] + [\"Treatment\"]\n",
    "    encoded_data = encoded_data.astype(static_agg_df.dtypes.to_dict())\n",
    "    del static_agg_df\n",
    "    gc.collect()\n",
    "    #print(\"del static_agg_df\")\n",
    "    \n",
    "    # Remove duplicated columns in place without creating a copy\n",
    "    #print(\"encoded_data\")\n",
    "    encoded_data = encoded_data.loc[:, ~encoded_data.columns.duplicated()]                                \n",
    "    cat_feat_idx = np.where((encoded_data.dtypes == 'object') & ~encoded_data.columns.isin([str(dataset_manager.label_col), \"Treatment\"]))[0]           \n",
    "    #print(f\"cat_feat_idx: {cat_feat_idx}\")\n",
    "\n",
    "    if cls_encoding == 'index':\n",
    "        #print(\"save_file_to_parquet\")\n",
    "        save_file_to_parquet(encoded_data, results_dir, f\"{data_type}_{cls_method}_{cls_encoding}_encoded\", dataset_name, cat_cols=cat_feat_idx)\n",
    "    else:\n",
    "        save_file_to_parquet(encoded_data, results_dir, f\"{data_type}_{cls_method}_{cls_encoding}_encoded\", dataset_name)\n",
    "    gc.collect()\n",
    "\n",
    "        \n",
    "\n",
    "  \n",
    "    \n",
    "for cls_method in cls_methods:    \n",
    "    gc.collect()\n",
    "    #print(f\"cls_method: {cls_method}\")\n",
    "    \n",
    "\n",
    "    for task_type in task_types:           \n",
    "        gc.collect()\n",
    "\n",
    "        for dataset_name in dataset_ref_to_datasets.keys():            \n",
    "            gc.collect()\n",
    "            results_dir = \"./../prepared_data/%s/%s/\" % (task_type, dataset_name)\n",
    "            dataset_manager = DatasetManager(dataset_name, task_type)\n",
    "            data = dataset_manager.read_dataset()\n",
    "            train, val, test = dataset_manager.split_data(data, train_ratio, val_ratio, split_type=\"temporal\", seed=22)\n",
    "            save_file_to_parquet(train, results_dir, \"train\", dataset_name)\n",
    "            save_file_to_parquet(val, results_dir, \"val\", dataset_name)\n",
    "            save_file_to_parquet(test, results_dir, \"test\", dataset_name)\n",
    "\n",
    "\n",
    "            min_prefix_length = 1\n",
    "            max_prefix_length = min(20, dataset_manager.get_pos_case_length_quantile(data, 0.90))\n",
    "            del data\n",
    "            \n",
    "            \n",
    "            cls_encoder_args = {'case_id_col': dataset_manager.case_id_col,\n",
    "                                'static_cat_cols': dataset_manager.static_cat_cols,\n",
    "                                'static_num_cols': dataset_manager.static_num_cols,\n",
    "                                'dynamic_cat_cols': dataset_manager.dynamic_cat_cols,\n",
    "                                'dynamic_num_cols': dataset_manager.dynamic_num_cols,\n",
    "                                'fillna': True,\n",
    "                                'model': cls_method,\n",
    "                                'dataset_name':dataset_name,\n",
    "                                \"results_dir\":results_dir}\n",
    "            \n",
    "            for cls_encoding in encoding_dict.keys():\n",
    "                gc.collect()\n",
    "                print(f\"Dataset: {dataset_name}, Encoding: {cls_encoding}, Method: {cls_method}, Task: {task_type}\")\n",
    "                methods = encoding_dict[cls_encoding]\n",
    "                \n",
    "                # Create FeatureUnion\n",
    "                feature_combiner = FeatureUnion([(method, EncoderFactory.get_encoder(method, **cls_encoder_args)) for method in methods])\n",
    "          \n",
    "                # Call the function to save encoded data                \n",
    "                #print(\"train\")\n",
    "                train_encoded = process_data(train, \"train\")\n",
    "                save_encoded_data(train_encoded, results_dir, dataset_name, cls_encoding, data_type=\"train\")\n",
    "                del train_encoded\n",
    "                gc.collect()\n",
    "                \n",
    "                # test\n",
    "                #print(\"test\")\n",
    "                test_encoded = process_data(test, \"test\")\n",
    "                save_encoded_data(test_encoded, results_dir, dataset_name, cls_encoding, data_type=\"test\")\n",
    "                del test_encoded\n",
    "                gc.collect()\n",
    "                \n",
    "                # val\n",
    "                #print(\"val\")\n",
    "                val_encoded = process_data(val, \"val\")\n",
    "                save_encoded_data(val_encoded, results_dir, dataset_name, cls_encoding, data_type=\"val\")\n",
    "                del val_encoded\n",
    "                gc.collect()\n",
    "                #print(\"Done!\\n\")\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prpm_py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
