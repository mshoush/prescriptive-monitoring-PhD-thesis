{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_name: bpic2012, task_type: regression, cls_method: catboost, cls_encoding: laststate\n",
      "bpic2012\n",
      "[17 18]\n",
      "0                                                    \n",
      "Trial 0 out of 1                                     \n",
      "100%|██████████| 1/1 [03:09<00:00, 189.49s/trial, best loss: 61.10785811724414]\n",
      "dataset_name: bpic2012, task_type: classification, cls_method: catboost, cls_encoding: laststate\n",
      "bpic2012\n",
      "[17 18]\n",
      "0                                                    \n",
      "Trial 0 out of 1                                     \n",
      "100%|██████████| 1/1 [01:51<00:00, 111.05s/trial, best loss: -0.6037291591803658]\n",
      "dataset_name: bpic2012, task_type: regression, cls_method: catboost, cls_encoding: agg\n",
      "bpic2012\n",
      "[4 5]\n",
      "0                                                    \n",
      "Trial 0 out of 1                                     \n",
      "  0%|          | 0/1 [00:00<?, ?trial/s, best loss=?]"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/mshoush/5th/common_files') \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.metrics import roc_auc_score, mean_squared_error\n",
    "from hyperopt import Trials, STATUS_OK, tpe, fmin, hp\n",
    "from catboost import Pool, CatBoostClassifier, CatBoostRegressor\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from lightgbm import LGBMClassifier, LGBMRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from DatasetManager import DatasetManager\n",
    "import gc\n",
    "import hyperopt\n",
    "\n",
    "\n",
    "from catboost import CatBoostClassifier, CatBoostRegressor\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from lightgbm import LGBMClassifier, LGBMRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.metrics import roc_auc_score, mean_squared_error\n",
    "from hyperopt import STATUS_OK\n",
    "\n",
    "def create_and_evaluate_model(args):\n",
    "    global trial_nr\n",
    "    if trial_nr % 50 == 0:\n",
    "        print(trial_nr)\n",
    "    print(\"Trial %s out of %s\" % (trial_nr, n_iter))\n",
    "    trial_nr += 1\n",
    "\n",
    "    score = 0\n",
    "    for current_train_names, current_test_names in dataset_manager.get_idx_split_generator(dt_for_splitting, n_splits=3):\n",
    "        train_idxs = case_ids.isin(current_train_names)\n",
    "        X_train = X_all[train_idxs]\n",
    "        y_train = y_all[train_idxs]\n",
    "        X_test = X_all[~train_idxs]\n",
    "        y_test = y_all[~train_idxs]\n",
    "\n",
    "        if task_type == \"classification\":\n",
    "            if cls_method == \"catboost\":\n",
    "                model = CatBoostClassifier(loss_function='Logloss',\n",
    "                                           learning_rate=args['learning_rate'],\n",
    "                                           depth=int(args['max_depth']),\n",
    "                                           subsample=args['subsample'],\n",
    "                                           bootstrap_type='Bernoulli',\n",
    "                                           verbose=False,\n",
    "                                           random_seed=22,\n",
    "                                           posterior_sampling=True,\n",
    "                                           thread_count=8)\n",
    "            elif cls_method == \"xgboost\":\n",
    "                model = XGBClassifier(learning_rate=args['learning_rate'],\n",
    "                                      max_depth=int(args['max_depth']),\n",
    "                                      subsample=args['subsample'],\n",
    "                                      verbosity=0,\n",
    "                                      random_state=22)\n",
    "            elif cls_method == \"lightgbm\":\n",
    "                model = LGBMClassifier(learning_rate=args['learning_rate'],\n",
    "                                       max_depth=int(args['max_depth']),\n",
    "                                       subsample=args['subsample'],\n",
    "                                       verbosity=-1,\n",
    "                                       random_state=22)\n",
    "            elif cls_method == \"randomforest\":\n",
    "                model = RandomForestClassifier(n_estimators=args['n_estimators'],\n",
    "                                                max_depth=int(args['max_depth']),\n",
    "                                                random_state=22)\n",
    "            else:\n",
    "                raise ValueError(\"Invalid cls_method for classification\")\n",
    "            score_function = roc_auc_score\n",
    "        elif task_type == \"regression\":\n",
    "            if cls_method == \"catboost\":\n",
    "                model = CatBoostRegressor(loss_function='RMSE',\n",
    "                                          learning_rate=args['learning_rate'],\n",
    "                                          depth=int(args['max_depth']),\n",
    "                                          subsample=args['subsample'],\n",
    "                                          bootstrap_type='Bernoulli',\n",
    "                                          verbose=False,\n",
    "                                          random_seed=22,\n",
    "                                          thread_count=8)\n",
    "            elif cls_method == \"xgboost\":\n",
    "                model = XGBRegressor(learning_rate=args['learning_rate'],\n",
    "                                     max_depth=int(args['max_depth']),\n",
    "                                     subsample=args['subsample'],\n",
    "                                     verbosity=0,\n",
    "                                     random_state=22)\n",
    "            elif cls_method == \"lightgbm\":\n",
    "                model = LGBMRegressor(learning_rate=args['learning_rate'],\n",
    "                                       max_depth=int(args['max_depth']),\n",
    "                                       subsample=args['subsample'],\n",
    "                                       verbosity=-1,\n",
    "                                       random_state=22)\n",
    "            elif cls_method == \"randomforest\":\n",
    "                model = RandomForestRegressor(n_estimators=args['n_estimators'],\n",
    "                                               max_depth=int(args['max_depth']),\n",
    "                                               random_state=22)\n",
    "            else:\n",
    "                raise ValueError(\"Invalid cls_method for regression\")\n",
    "            score_function = mean_squared_error\n",
    "        else:\n",
    "            raise ValueError(\"Invalid task_type\")\n",
    "        \n",
    "        if cls_method!=\"catboost\":            \n",
    "            model.fit(X_train, y_train)\n",
    "        else:\n",
    "            model.fit(X_train, y_train, cat_features=cat_feat_idx)\n",
    "        preds = model.predict(X_test)\n",
    "        score += score_function(y_test, preds)\n",
    "\n",
    "    if task_type == \"classification\":\n",
    "        return {'loss': -score / n_splits, 'status': STATUS_OK, 'model': model}\n",
    "    elif task_type == \"regression\":\n",
    "        return {'loss': score / n_splits, 'status': STATUS_OK, 'model': model}\n",
    "\n",
    "\n",
    "\n",
    "case_id_col = 'case_id'\n",
    "activity_col = 'activity'\n",
    "resource_col = 'resource'\n",
    "timestamp_col = 'timestamp'\n",
    "label_col = 'label'\n",
    "treatment_col = \"Treatment1\"\n",
    "\n",
    "dataset_ref_to_datasets = {\n",
    "    #\"bpic2012\": [\"bpic2012\"],\n",
    "    \"bpic2017\": [\"bpic2017\"],\n",
    "}\n",
    "\n",
    "encoding_dict = {  \n",
    "    \"laststate\": [\"static\", \"last\"],\n",
    "    \"agg\": [\"static\", \"agg\"], \n",
    "    \"index\": [\"static\", \"index\"],             \n",
    "    \"combined\": [\"static\", \"last\", \"agg\"]\n",
    "}\n",
    "\n",
    "task_types = [\"regression\", \"classification\",]\n",
    "\n",
    "cls_methods = ['xgboost', 'lightgbm', 'randomforest', 'catboost'] \n",
    "\n",
    "for cls_method in cls_methods:       \n",
    "    for cls_encoding in encoding_dict.keys():        \n",
    "        \n",
    "        for task_type in task_types:   \n",
    "            \n",
    "            for dataset_name in dataset_ref_to_datasets.keys():\n",
    "                n_iter = 1  # Update this value as needed\n",
    "                trial_nr = 0\n",
    "                n_splits = 3\n",
    "                print(f\"dataset_name: {dataset_name}, task_type: {task_type}, cls_method: {cls_method}, cls_encoding: {cls_encoding}\")\n",
    "                            \n",
    "                params_dir = f\"./../predictive_results/{task_type}/{dataset_name}/\"           \n",
    "                # Check if params_dir exists, otherwise create it\n",
    "                if not os.path.exists(params_dir):\n",
    "                    os.makedirs(params_dir)\n",
    "                \n",
    "                dataset_manager = DatasetManager(dataset_name, task_type)\n",
    "                #print(f\"Label_col: {str(dataset_manager.label_col)}\")\n",
    "                \n",
    "                # Load the training data\n",
    "                if cls_method!=\"catboost\":\n",
    "                    cls_method_all = \"other\"\n",
    "                else:\n",
    "                    cls_method_all = cls_method\n",
    "                train = pd.read_parquet(f\"./../prepared_data/{task_type}/{dataset_name}/train_{cls_method_all}_{cls_encoding}_encoded_{dataset_name}.parquet\")\n",
    "                cat_feat_idx = np.where((train.dtypes == 'object') & ~train.columns.isin([str(dataset_manager.label_col), \"Treatment\"]))[0]\n",
    "                print(cat_feat_idx)                \n",
    "                \n",
    "                \n",
    "                # Load the prefix data\n",
    "                dt_prefixes = pd.read_parquet(f\"./../prepared_data/{task_type}/{dataset_name}/train_prefixes_{dataset_name}.parquet\")\n",
    "                \n",
    "                y_all = train[dataset_manager.label_col]\n",
    "                if task_type == \"classification\":\n",
    "                    y_all = y_all.astype(int)  # Ensure the target variable is integer type for classification\n",
    "                elif task_type == \"regression\":\n",
    "                    y_all = y_all.astype(float)  # Ensure the target variable is float type for regression\n",
    "                else:\n",
    "                    raise ValueError(\"Invalid task_type\")\n",
    "                                \n",
    "                X_all = train.drop([str(dataset_manager.label_col)], axis=1)\n",
    "                \n",
    "                case_ids = dt_prefixes.groupby(dataset_manager.case_id_col).first()[\"orig_case_id\"].reset_index(drop=True)\n",
    "                dt_for_splitting = pd.DataFrame({dataset_manager.case_id_col: case_ids, dataset_manager.label_col: y_all}).drop_duplicates().reset_index(drop=True)\n",
    "                \n",
    "                #print('Optimizing parameters...')\n",
    "                \n",
    "                if cls_method == \"catboost\":\n",
    "                    space = {\n",
    "                        'learning_rate': hp.uniform(\"learning_rate\", 0.01, 0.8),\n",
    "                        'one_hot_max_size': hp.quniform('one_hot_max_size', 4, 255, 1),\n",
    "                        'subsample': hp.uniform(\"subsample\", 0.5, 1),\n",
    "                        'max_depth': hp.quniform('max_depth', 6, 16, 1),\n",
    "                        'colsample_bytree': hp.uniform(\"colsample_bytree\", 0.5, 1),\n",
    "                        'bagging_temperature': hp.uniform('bagging_temperature', 0.0, 100),\n",
    "                        'random_strength': hp.uniform('random_strength', 0.0, 100),\n",
    "                        'l2_leaf_reg': hp.loguniform('l2_leaf_reg', 0, np.log(10)),\n",
    "                        'n_estimators': hp.choice('n_estimators', [250, 500, 1000]),\n",
    "                        'min_child_weight': hp.quniform('min_child_weight', 1, 6, 1)\n",
    "                    }\n",
    "                elif cls_method == \"xgboost\":\n",
    "                    space = {\n",
    "                        'learning_rate': hp.uniform(\"learning_rate\", 0.01, 0.8),\n",
    "                        'max_depth': hp.quniform('max_depth', 2, 10, 1),\n",
    "                        'min_child_weight': hp.quniform('min_child_weight', 1, 6, 1),\n",
    "                        'subsample': hp.uniform(\"subsample\", 0.5, 1),\n",
    "                        'colsample_bytree': hp.uniform(\"colsample_bytree\", 0.5, 1),\n",
    "                        'gamma': hp.uniform(\"gamma\", 0, 10),\n",
    "                        'n_estimators': hp.choice('n_estimators', [100, 250, 500]),\n",
    "                    }\n",
    "                elif cls_method == \"lightgbm\":\n",
    "                    space = {\n",
    "                        'learning_rate': hp.uniform(\"learning_rate\", 0.01, 0.8),\n",
    "                        'max_depth': hp.quniform('max_depth', 2, 10, 1),\n",
    "                        'min_child_weight': hp.quniform('min_child_weight', 1, 6, 1),\n",
    "                        'subsample': hp.uniform(\"subsample\", 0.5, 1),\n",
    "                        'colsample_bytree': hp.uniform(\"colsample_bytree\", 0.5, 1),\n",
    "                        'bagging_fraction': hp.uniform(\"bagging_fraction\", 0.5, 1),\n",
    "                        'feature_fraction': hp.uniform(\"feature_fraction\", 0.5, 1),\n",
    "                        'n_estimators': hp.choice('n_estimators', [100, 250, 500]),\n",
    "                    }\n",
    "                elif cls_method == \"randomforest\":\n",
    "                    space = {\n",
    "                        'n_estimators': hp.choice('n_estimators', [100, 250, 500]),\n",
    "                        'max_depth': hp.quniform('max_depth', 2, 10, 1),\n",
    "                        'min_samples_split': hp.quniform('min_samples_split', 2, 20, 1),\n",
    "                        'min_samples_leaf': hp.quniform('min_samples_leaf', 1, 10, 1),\n",
    "                        'max_features': hp.choice('max_features', ['auto', 'sqrt', 'log2', None]),\n",
    "                    }\n",
    "                else:\n",
    "                    print(\"No Valid cls_method\")\n",
    "                    \n",
    "                trials = Trials()\n",
    "                best = fmin(create_and_evaluate_model, space, algo=tpe.suggest, max_evals=n_iter, trials=trials)\n",
    "\n",
    "                best_params = hyperopt.space_eval(space, best)\n",
    "                \n",
    "                best_params_df = pd.DataFrame([best_params])\n",
    "                #print(best_params_df)\n",
    "                outfile = os.path.join(params_dir, f\"optimal_params_{cls_encoding}_{cls_method}_{dataset_name}.parquet\")\n",
    "                best_params_df.to_parquet(outfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/mshoush/5th/common_files') \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.metrics import roc_auc_score, mean_squared_error\n",
    "from hyperopt import Trials, STATUS_OK, tpe, fmin, hp\n",
    "from catboost import Pool, CatBoostClassifier, CatBoostRegressor\n",
    "from DatasetManager import DatasetManager\n",
    "import gc\n",
    "import hyperopt\n",
    "\n",
    "\n",
    "def create_and_evaluate_model(args):\n",
    "    global trial_nr\n",
    "    if trial_nr % 50 == 0:\n",
    "        print(trial_nr)\n",
    "    print(\"Trial %s out of %s\" % (trial_nr, n_iter))\n",
    "    trial_nr += 1\n",
    "\n",
    "    score = 0\n",
    "    for current_train_names, current_test_names in dataset_manager.get_idx_split_generator(dt_for_splitting, n_splits=3):\n",
    "        train_idxs = case_ids.isin(current_train_names)\n",
    "        X_train = X_all[train_idxs]\n",
    "        y_train = y_all[train_idxs]\n",
    "        X_test = X_all[~train_idxs]\n",
    "        y_test = y_all[~train_idxs]\n",
    "\n",
    "        if task_type == \"classification\":\n",
    "            model = CatBoostClassifier(loss_function='Logloss',\n",
    "                                       learning_rate=args['learning_rate'],\n",
    "                                       depth=int(args['max_depth']),\n",
    "                                       subsample=args['subsample'],\n",
    "                                       bootstrap_type='Bernoulli',\n",
    "                                       verbose=False,\n",
    "                                       random_seed=22,\n",
    "                                       posterior_sampling=True,\n",
    "                                       thread_count=8)\n",
    "            score_function = roc_auc_score\n",
    "        elif task_type == \"regression\":\n",
    "            model = CatBoostRegressor(loss_function='RMSE',\n",
    "                                      learning_rate=args['learning_rate'],\n",
    "                                      depth=int(args['max_depth']),\n",
    "                                      subsample=args['subsample'],\n",
    "                                      bootstrap_type='Bernoulli',\n",
    "                                      verbose=False,\n",
    "                                      random_seed=22,\n",
    "                                      thread_count=8)\n",
    "            score_function = mean_squared_error\n",
    "        else:\n",
    "            raise ValueError(\"Invalid task_type\")\n",
    "\n",
    "        pool_train = Pool(X_train, y_train, cat_features=cat_feat_idx)\n",
    "        pool_test = Pool(X_test, cat_features=cat_feat_idx)\n",
    "\n",
    "        model.fit(pool_train)\n",
    "        preds = model.predict(pool_test)\n",
    "        score += score_function(y_test, preds)\n",
    "\n",
    "    if task_type == \"classification\":\n",
    "        return {'loss': -score / n_splits, 'status': STATUS_OK, 'model': model}\n",
    "    elif task_type == \"regression\":\n",
    "        return {'loss': score / n_splits, 'status': STATUS_OK, 'model': model}\n",
    "\n",
    "\n",
    "case_id_col = 'case_id'\n",
    "activity_col = 'activity'\n",
    "resource_col = 'resource'\n",
    "timestamp_col = 'timestamp'\n",
    "label_col = 'label'\n",
    "treatment_col = \"Treatment1\"\n",
    "\n",
    "\n",
    "\n",
    "dataset_ref_to_datasets = {\n",
    "    \"bpic2012\": [\"bpic2012\"],\n",
    "    #\"bpic2017\": [\"bpic2017\"],\n",
    "}\n",
    "\n",
    "encoding_dict = {  \n",
    "    \"index\": [\"static\", \"index\"],  \n",
    "    \"laststate\": [\"static\", \"last\"],\n",
    "    \"agg\": [\"static\", \"agg\"],        \n",
    "    \"combined\": [\"static\", \"last\", \"agg\"]\n",
    "}\n",
    "\n",
    "task_types = [\"regression\", \"classification\",]\n",
    "\n",
    "cls_methods = ['catboost']\n",
    "\n",
    "for cls_method in cls_methods:\n",
    "    print(f\"cls_method: {cls_method}\")\n",
    "    \n",
    "    for cls_encoding in encoding_dict.keys():\n",
    "        print(f\"cls_encoding: {cls_encoding}\")\n",
    "        \n",
    "        for task_type in task_types:    \n",
    "            print(f\"task_type: {task_type}\")\n",
    "            #for dataset_name in datasets:  \n",
    "            for dataset_name in dataset_ref_to_datasets.keys():\n",
    "                n_iter = 1  # Update this value as needed\n",
    "                trial_nr = 0\n",
    "                n_splits = 3\n",
    "                print(f\"dataset_name: {dataset_name}\")\n",
    "                            \n",
    "                params_dir = \"./../predictive_results/%s/%s/\" % (task_type, dataset_name)            \n",
    "                # Check if params_dir exists, otherwise create it\n",
    "                if not os.path.exists(params_dir):\n",
    "                    os.makedirs(params_dir)\n",
    "                \n",
    "                dataset_manager = DatasetManager(dataset_name, task_type)\n",
    "                print(f\"Label_col: {str(dataset_manager.label_col)}\")\n",
    "                \n",
    "                # Load the training data\n",
    "                train = pd.read_parquet(f\"./../prepared_data/{task_type}/{dataset_name}/train_{cls_method}_{cls_encoding}_encoded_{dataset_name}.parquet\")\n",
    "                #print(train.head())\n",
    "                cat_feat_idx = np.where((train.dtypes == 'object') & ~train.columns.isin([str(dataset_manager.label_col), \"Treatment\"]))[0]\n",
    "                print(f\"cat_feat_idx: {cat_feat_idx}\")\n",
    "                \n",
    "                # Load the prefix data\n",
    "                dt_prefixes = pd.read_parquet(f\"./../prepared_data/{task_type}/{dataset_name}/train_prefixes_{dataset_name}.parquet\")\n",
    "                \n",
    "                y_all = train[dataset_manager.label_col]\n",
    "                #print(y_all)\n",
    "                # Inside the loop where you extract labels and features from the training data\n",
    "                if task_type == \"classification\":\n",
    "                    y_all = y_all.astype(int)  # Ensure the target variable is integer type for classification\n",
    "                elif task_type == \"regression\":\n",
    "                    y_all = y_all.astype(float)  # Ensure the target variable is float type for regression\n",
    "                else:\n",
    "                    raise ValueError(\"Invalid task_type\")\n",
    "                                \n",
    "                # Extract labels and features from the training data\n",
    "                \n",
    "                X_all = train.drop([str(dataset_manager.label_col)], axis=1)\n",
    "                \n",
    "                case_ids = dt_prefixes.groupby(dataset_manager.case_id_col).first()[\"orig_case_id\"].reset_index(drop=True)\n",
    "                dt_for_splitting = pd.DataFrame({dataset_manager.case_id_col: case_ids, dataset_manager.label_col: y_all}).drop_duplicates().reset_index(drop=True)\n",
    "                \n",
    "                print('Optimizing parameters...')\n",
    "                \n",
    "                if cls_method == \"catboost\":\n",
    "                    space = {\n",
    "                        'learning_rate': hp.uniform(\"learning_rate\", 0.01, 0.8),\n",
    "                        'one_hot_max_size': hp.quniform('one_hot_max_size', 4, 255, 1),\n",
    "                        'subsample': hp.uniform(\"subsample\", 0.5, 1),\n",
    "                        'max_depth': hp.quniform('max_depth', 6, 16, 1),\n",
    "                        'colsample_bytree': hp.uniform(\"colsample_bytree\", 0.5, 1),\n",
    "                        'bagging_temperature': hp.uniform('bagging_temperature', 0.0, 100),\n",
    "                        'random_strength': hp.uniform('random_strength', 0.0, 100),\n",
    "                        'l2_leaf_reg': hp.loguniform('l2_leaf_reg', 0, np.log(10)),\n",
    "                        'n_estimators': hp.choice('n_estimators', [250, 500, 1000]),\n",
    "                        'min_child_weight': hp.quniform('min_child_weight', 1, 6, 1)\n",
    "                    }\n",
    "                else:\n",
    "                    print(\"No Valid cls_method\")\n",
    "                    \n",
    "                trials = Trials()\n",
    "                best = fmin(create_and_evaluate_model, space, algo=tpe.suggest, max_evals=n_iter, trials=trials)\n",
    "\n",
    "                best_params = hyperopt.space_eval(space, best)\n",
    "                \n",
    "                best_params_df = pd.DataFrame([best_params])\n",
    "                print(best_params_df)\n",
    "                outfile = os.path.join(params_dir, f\"optimal_params_{cls_encoding}_{cls_method}_{dataset_name}.parquet\")\n",
    "                best_params_df.to_parquet(outfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/mshoush/5th/common_files') \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from hyperopt import Trials, STATUS_OK, tpe, fmin, hp\n",
    "from catboost import Pool, CatBoostClassifier\n",
    "from DatasetManager import DatasetManager\n",
    "import gc\n",
    "import hyperopt\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "def create_and_evaluate_model(args):\n",
    "    global trial_nr\n",
    "    if trial_nr % 50 == 0:\n",
    "        print(trial_nr)\n",
    "    print(\"Trial %s out of %s\" % (trial_nr, n_iter))\n",
    "    trial_nr += 1\n",
    "\n",
    "    score = 0\n",
    "    for current_train_names, current_test_names in dataset_manager.get_idx_split_generator(dt_for_splitting, n_splits=3):\n",
    "        train_idxs = case_ids.isin(current_train_names)\n",
    "        X_train = X_all[train_idxs]\n",
    "        y_train = y_all[train_idxs]\n",
    "        X_test = X_all[~train_idxs]\n",
    "        y_test = y_all[~train_idxs]\n",
    "\n",
    "        cls = CatBoostClassifier(loss_function='Logloss',\n",
    "                                 learning_rate=args['learning_rate'],\n",
    "                                 depth=int(args['max_depth']),\n",
    "                                 subsample=args['subsample'],\n",
    "                                 bootstrap_type='Bernoulli',\n",
    "                                 verbose=False,\n",
    "                                 random_seed=22,\n",
    "                                 posterior_sampling=True,\n",
    "                                 thread_count=8)\n",
    "\n",
    "        pool_train = Pool(X_train, y_train, cat_features=cat_feat_idx)\n",
    "        pool_test = Pool(X_test, cat_features=cat_feat_idx)\n",
    "\n",
    "        cls.fit(pool_train)\n",
    "        preds_pos_label_idx = np.where(cls.classes_ == 1)[0][0]\n",
    "        preds = cls.predict_proba(pool_test)[:, preds_pos_label_idx]\n",
    "        score += roc_auc_score(y_test, preds)\n",
    "    return {'loss': -score / n_splits, 'status': STATUS_OK, 'model': cls}\n",
    "\n",
    "\n",
    "\n",
    "case_id_col = 'case_id'\n",
    "activity_col = 'activity'\n",
    "resource_col = 'resource'\n",
    "timestamp_col = 'timestamp'\n",
    "label_col = 'label'\n",
    "treatment_col = \"Treatment1\"\n",
    "\n",
    "\n",
    "n_iter = 50  # Update this value as needed\n",
    "trial_nr = 0\n",
    "n_splits = 3\n",
    "\n",
    "dataset_ref_to_datasets = {\n",
    "    \"bpic2012\": [\"bpic2012\"],\n",
    "    #\"bpic2017\": [\"bpic2017\"],\n",
    "    \n",
    "    \n",
    "}\n",
    "\n",
    "encoding_dict = {    \n",
    "    \"laststate\": [\"static\", \"last\"],\n",
    "    \"agg\": [\"static\", \"agg\"],    \n",
    "    \"index\": [\"static\", \"index\"],\n",
    "    \"combined\": [\"static\", \"last\", \"agg\"]\n",
    "    }\n",
    "\n",
    "task_types = [\"classification\", ] # \"regression\"\n",
    "\n",
    "cls_methods = ['catboost']\n",
    "\n",
    "\n",
    "for cls_method in cls_methods:\n",
    "    print(f\"cls_method: {cls_method}\")\n",
    "    \n",
    "    for cls_encoding in encoding_dict.keys():\n",
    "        print(f\"cls_encoding: {cls_encoding}\")\n",
    "        \n",
    "        for task_type in task_types:    \n",
    "            print(f\"task_type: {task_type}\")\n",
    "            #for dataset_name in datasets:  \n",
    "            for dataset_name in dataset_ref_to_datasets.keys():\n",
    "                print(f\"dataset_name: {dataset_name}\")\n",
    "                            \n",
    "                params_dir = \"./../predictive_results/%s/%s/\" % (task_type, dataset_name)            \n",
    "                # Check if params_dir exists, otherwise create it\n",
    "                if not os.path.exists(params_dir):\n",
    "                    os.makedirs(params_dir)\n",
    "                \n",
    "                \n",
    "                \n",
    "                dataset_manager = DatasetManager(dataset_name, task_type)\n",
    "                print(f\"Label_col: {str(dataset_manager.label_col)}\")\n",
    "                # Load the training data\n",
    "                train = pd.read_parquet(f\"./../prepared_data/{task_type}/{dataset_name}/train_{cls_encoding}_encoded_{dataset_name}.parquet\")\n",
    "                cat_feat_idx = np.where((train.dtypes == 'object') & ~train.columns.isin([str(dataset_manager.label_col), \"Treatment\"]))[0]\n",
    "                print(f\"cat_feat_idx: {cat_feat_idx}\")\n",
    "                \n",
    "                # Load the prefix data\n",
    "                dt_prefixes = pd.read_parquet(f\"./../prepared_data/{task_type}/{dataset_name}/train_prefixes_{dataset_name}.parquet\")\n",
    "                \n",
    "                # Extract labels and features from the training data\n",
    "                y_all = train[str(dataset_manager.label_col)]\n",
    "                X_all = train.drop([str(dataset_manager.label_col)], axis=1)\n",
    "                \n",
    "                case_ids = dt_prefixes.groupby(dataset_manager.case_id_col).first()[\"orig_case_id\"].reset_index(drop=True)\n",
    "                dt_for_splitting = pd.DataFrame({dataset_manager.case_id_col: case_ids, dataset_manager.label_col: y_all}).drop_duplicates().reset_index(drop=True)\n",
    "                \n",
    "                print('Optimizing parameters...')\n",
    "                \n",
    "                if cls_method==\"catboost\":\n",
    "                    space = {\n",
    "                    'learning_rate': hp.uniform(\"learning_rate\", 0.01, 0.8),\n",
    "                    'one_hot_max_size': hp.quniform('one_hot_max_size', 4, 255, 1),\n",
    "                    'subsample': hp.uniform(\"subsample\", 0.5, 1),\n",
    "                    'max_depth': hp.quniform('max_depth', 6, 16, 1),\n",
    "                    'colsample_bytree': hp.uniform(\"colsample_bytree\", 0.5, 1),\n",
    "                    'bagging_temperature': hp.uniform('bagging_temperature', 0.0, 100),\n",
    "                    'random_strength': hp.uniform('random_strength', 0.0, 100),\n",
    "                    'l2_leaf_reg': hp.loguniform('l2_leaf_reg', 0, np.log(10)),\n",
    "                    'n_estimators': hp.choice('n_estimators', [250, 500, 1000]),\n",
    "                    'min_child_weight': hp.quniform('min_child_weight', 1, 6, 1)\n",
    "                    }\n",
    "                else:\n",
    "                    print(\"No Valid cls_method\")\n",
    "                    \n",
    "                trials = Trials()\n",
    "                best = fmin(create_and_evaluate_model, space, algo=tpe.suggest, max_evals=n_iter, trials=trials)\n",
    "\n",
    "                best_params = hyperopt.space_eval(space, best)\n",
    "                \n",
    "                best_params_df = pd.DataFrame([best_params])\n",
    "                print(best_params_df)\n",
    "                outfile = os.path.join(params_dir, f\"optimal_params_{cls_encoding}_{cls_method}_{dataset_name}.parquet\")\n",
    "                best_params_df.to_parquet(outfile)\n",
    "                \n",
    "                \n",
    "                \n",
    "                # outfile = os.path.join(params_dir, f\"optimal_params_{cls_encoding}_{cls_method}_{dataset_name}.parquet\")\n",
    "                # best_params.to_parquet(os.path.join(params_dir, outfile))\n",
    "\n",
    "                                                \n",
    "\n",
    "                                   \n",
    "                \n",
    "                \n",
    "                                \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prpm_py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
