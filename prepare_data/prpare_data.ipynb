{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 1;\n                var nbb_unformatted_code = \"# !pip install pandas\\n# !pip install modin[all]\\n# !pip install bokeh==2.4.2\\n# !pip install psutil memory-profiler\\n# !pip install jupyter-server-proxy\\n# !pip install pm4py\";\n                var nbb_formatted_code = \"# !pip install pandas\\n# !pip install modin[all]\\n# !pip install bokeh==2.4.2\\n# !pip install psutil memory-profiler\\n# !pip install jupyter-server-proxy\\n# !pip install pm4py\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# !pip install pandas\n",
    "# !pip install modin[all]\n",
    "# !pip install bokeh==2.4.2\n",
    "# !pip install psutil memory-profiler\n",
    "# !pip install jupyter-server-proxy\n",
    "# !pip install pm4py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 2;\n                var nbb_unformatted_code = \"\\nlogs = [\\n        'bpic2012',\\n        'bpic2017', \\n        #'trafficFines',\\n        ]\\n\\n\\n# standardize column names: ['case_id', 'activity', 'resource', 'timestamp'] for all logs\\ncase_id_col = 'case_id'\\nactivity_col = 'activity'\\nresource_col = 'resource'\\ntimestamp_col = 'timestamp'\\n\\n# dataset_name: [case_id_col, activity_col, resource_col, timestamp_col]\\n# NOTE - Basic column names are defined by users. These names will be renamed and standardized later \\n# standardize column names: ['case_id', 'activity', 'resource', 'timestamp']\\ndataset_dict = {\\n    'bpic2012': ['case_id', 'activity', 'resource', 'start_time'],\\n    'bpic2017': ['case_id', 'activity', 'org:resource', 'time:timestamp'],\\n    #'trafficFines': ['case:concept:name', 'concept:name', 'org:resource', \\\"time:timestamp\\\"], \\n}\\nincomplete_dict = {\\n    'bpic2012': [\\\"A_APPROVED\\\", \\\"A_REGISTERED\\\", \\\"A_ACTIVATED\\\", \\\"A_CANCELLED\\\", \\\"A_DECLINED\\\"],\\n    'bpic2017': [\\\"A_Pending\\\", \\\"A_Denied\\\", \\\"A_Cancelled\\\",],\\n    #'trafficFines': [\\\"Send for Credit Collection\\\"]\\n}\\n\\nnegative_events = {\\n    'bpic2012': [\\\"A_CANCELLED\\\", \\\"A_DECLINED\\\"],\\n    'bpic2017': [\\\"A_Denied\\\", \\\"A_Cancelled\\\",],\\n    #'trafficFines': [\\\"Send for Credit Collection\\\"]\\n}\\n\\n\\n# Define positive and negative labels\\nlabel_old = \\\"label\\\"\\nneg_label = \\\"deviant\\\"\\npos_label = \\\"regular\\\"\\npositive_activities_dict = {\\n    'bpic2017': [\\\"A_Pending\\\"],\\n    'bpic2012': [\\\"A_APPROVED\\\", \\\"A_REGISTERED\\\", \\\"A_ACTIVATED\\\",],\\n    #'trafficFines': [\\\"Send for Credit Collection\\\"]\\n}\\n\\nchunk_s = 10000\";\n                var nbb_formatted_code = \"logs = [\\n    \\\"bpic2012\\\",\\n    \\\"bpic2017\\\",\\n    #'trafficFines',\\n]\\n\\n\\n# standardize column names: ['case_id', 'activity', 'resource', 'timestamp'] for all logs\\ncase_id_col = \\\"case_id\\\"\\nactivity_col = \\\"activity\\\"\\nresource_col = \\\"resource\\\"\\ntimestamp_col = \\\"timestamp\\\"\\n\\n# dataset_name: [case_id_col, activity_col, resource_col, timestamp_col]\\n# NOTE - Basic column names are defined by users. These names will be renamed and standardized later\\n# standardize column names: ['case_id', 'activity', 'resource', 'timestamp']\\ndataset_dict = {\\n    \\\"bpic2012\\\": [\\\"case_id\\\", \\\"activity\\\", \\\"resource\\\", \\\"start_time\\\"],\\n    \\\"bpic2017\\\": [\\\"case_id\\\", \\\"activity\\\", \\\"org:resource\\\", \\\"time:timestamp\\\"],\\n    #'trafficFines': ['case:concept:name', 'concept:name', 'org:resource', \\\"time:timestamp\\\"],\\n}\\nincomplete_dict = {\\n    \\\"bpic2012\\\": [\\n        \\\"A_APPROVED\\\",\\n        \\\"A_REGISTERED\\\",\\n        \\\"A_ACTIVATED\\\",\\n        \\\"A_CANCELLED\\\",\\n        \\\"A_DECLINED\\\",\\n    ],\\n    \\\"bpic2017\\\": [\\n        \\\"A_Pending\\\",\\n        \\\"A_Denied\\\",\\n        \\\"A_Cancelled\\\",\\n    ],\\n    #'trafficFines': [\\\"Send for Credit Collection\\\"]\\n}\\n\\nnegative_events = {\\n    \\\"bpic2012\\\": [\\\"A_CANCELLED\\\", \\\"A_DECLINED\\\"],\\n    \\\"bpic2017\\\": [\\n        \\\"A_Denied\\\",\\n        \\\"A_Cancelled\\\",\\n    ],\\n    #'trafficFines': [\\\"Send for Credit Collection\\\"]\\n}\\n\\n\\n# Define positive and negative labels\\nlabel_old = \\\"label\\\"\\nneg_label = \\\"deviant\\\"\\npos_label = \\\"regular\\\"\\npositive_activities_dict = {\\n    \\\"bpic2017\\\": [\\\"A_Pending\\\"],\\n    \\\"bpic2012\\\": [\\n        \\\"A_APPROVED\\\",\\n        \\\"A_REGISTERED\\\",\\n        \\\"A_ACTIVATED\\\",\\n    ],\\n    #'trafficFines': [\\\"Send for Credit Collection\\\"]\\n}\\n\\nchunk_s = 10000\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "logs = [\n",
    "        'bpic2012',\n",
    "        'bpic2017', \n",
    "        #'trafficFines',\n",
    "        ]\n",
    "\n",
    "\n",
    "# standardize column names: ['case_id', 'activity', 'resource', 'timestamp'] for all logs\n",
    "case_id_col = 'case_id'\n",
    "activity_col = 'activity'\n",
    "resource_col = 'resource'\n",
    "timestamp_col = 'timestamp'\n",
    "\n",
    "# dataset_name: [case_id_col, activity_col, resource_col, timestamp_col]\n",
    "# NOTE - Basic column names are defined by users. These names will be renamed and standardized later \n",
    "# standardize column names: ['case_id', 'activity', 'resource', 'timestamp']\n",
    "dataset_dict = {\n",
    "    'bpic2012': ['case_id', 'activity', 'resource', 'start_time'],\n",
    "    'bpic2017': ['case_id', 'activity', 'org:resource', 'time:timestamp'],\n",
    "    #'trafficFines': ['case:concept:name', 'concept:name', 'org:resource', \"time:timestamp\"], \n",
    "}\n",
    "incomplete_dict = {\n",
    "    'bpic2012': [\"A_APPROVED\", \"A_REGISTERED\", \"A_ACTIVATED\", \"A_CANCELLED\", \"A_DECLINED\"],\n",
    "    'bpic2017': [\"A_Pending\", \"A_Denied\", \"A_Cancelled\",],\n",
    "    #'trafficFines': [\"Send for Credit Collection\"]\n",
    "}\n",
    "\n",
    "negative_events = {\n",
    "    'bpic2012': [\"A_CANCELLED\", \"A_DECLINED\"],\n",
    "    'bpic2017': [\"A_Denied\", \"A_Cancelled\",],\n",
    "    #'trafficFines': [\"Send for Credit Collection\"]\n",
    "}\n",
    "\n",
    "\n",
    "# Define positive and negative labels\n",
    "label_old = \"label\"\n",
    "neg_label = \"deviant\"\n",
    "pos_label = \"regular\"\n",
    "positive_activities_dict = {\n",
    "    'bpic2017': [\"A_Pending\"],\n",
    "    'bpic2012': [\"A_APPROVED\", \"A_REGISTERED\", \"A_ACTIVATED\",],\n",
    "    #'trafficFines': [\"Send for Credit Collection\"]\n",
    "}\n",
    "\n",
    "chunk_s = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 3;\n                var nbb_unformatted_code = \"import warnings\\nwarnings.filterwarnings('ignore')\\n\\nimport os\\nimport csv\\n\\nimport psutil\\nimport time\\nfrom memory_profiler import profile\\nfrom pandas import Timestamp\\nimport pm4py\\nimport pandas as pd\\nimport numpy as np\\n\\nfrom concurrent.futures import ProcessPoolExecutor, as_completed\\n\\n\\nclass DataPreprocessorModinDask:\\n    def __init__(self, log_name, input_data_folder, output_data_folder):\\n        self.log_name = log_name\\n        self.input_data_folder = input_data_folder\\n        self.output_data_folder = output_data_folder\\n\\n    def _print_metrics(self, elapsed_time):\\n        # Helper function to print resource usage metrics\\n        cpu_percent = psutil.cpu_percent()\\n        memory_info = psutil.virtual_memory()\\n        # print(f\\\"Time: {elapsed_time} seconds\\\")\\n        # print(f\\\"CPU Usage: {cpu_percent}%\\\")\\n        # print(f\\\"Memory Usage: {memory_info.percent}%\\\")\\n\\n    @profile\\n    def read_log(self):\\n        start_time = time.time()\\n        print(\\\"Reading log...\\\")\\n\\n        # Get the first file in the input folder\\n        files = os.listdir(self.input_data_folder)[0]\\n        log_path = os.path.join(self.input_data_folder, files)\\n\\n        # Read the log based on its extension\\n        if log_path.lower().endswith('.csv'):\\n            with open(log_path, 'r') as file:\\n                # Use Sniffer to infer the delimiter\\n                dialect = csv.Sniffer().sniff(file.read(10000))\\n            try:\\n                # Read CSV with Dask, specifying dtype for certain columns\\n                log_file = pd.read_csv(log_path, sep=dialect.delimiter, dtype={'Resource': 'object', 'article': 'object'})\\n            except:\\n                log_file = pd.read_csv(log_path, sep=dialect.delimiter)\\n            log_file = log_file.rename(columns=lambda x: x.strip().lower().replace(' ', '_'))\\n            log_file = log_file.rename(\\n                columns=dict(zip(dataset_dict[self.log_name], [case_id_col, activity_col, resource_col, timestamp_col])))\\n        elif log_path.lower().endswith('.xes') or log_path.lower().endswith('.xes.gz'):\\n            # Read XES log using pm4py and convert to Dask DataFrame\\n            log_file = pm4py.read_xes(log_path)\\n            log_file = dataframe_utils.convert_traces_dataframe(log_file)\\n            log_file = from_pandas(log_file, npartitions=1)\\n            log_file = log_file.rename(\\n                columns=dict(zip(dataset_dict[self.log_name], [case_id_col, activity_col, resource_col, timestamp_col])))\\n        else:\\n            raise ValueError(\\\"Unsupported file extension. Supported extensions: .csv, .xes\\\")\\n\\n        # Calculate elapsed time and print resource usage metrics\\n        end_time = time.time()\\n        elapsed_time = end_time - start_time\\n        self._print_metrics(elapsed_time)\\n        return log_file\\n\\n    @profile\\n    def clean_data(self, log_file):\\n        start_time = time.time()\\n        print(\\\"Cleaning data...\\\")\\n\\n        # Convert timestamp column to datetime\\n        log_file[timestamp_col] = pd.to_datetime(log_file[timestamp_col], format=\\\"mixed\\\", infer_datetime_format=True)\\n        log_file = log_file.sort_values(by=[timestamp_col])\\n\\n       # Remove white spaces from column values\\n        log_file = log_file.applymap(lambda x: x.strip() if isinstance(x, str) else x)\\n\\n        # Convert the 'resource' column to string\\n        log_file[resource_col] = log_file[resource_col].astype(str)\\n\\n        # Replace unique resource values with 'res{i}' format\\n        unique_resources = log_file[resource_col].unique()\\n        resource_mapping = {original_value: f'res{i + 1}' for i, original_value in enumerate(unique_resources)}\\n        log_file[resource_col] = log_file[resource_col].replace(resource_mapping)\\n\\n        threshold_percentage = 25\\n        threshold = len(log_file) * (threshold_percentage / 100)\\n        columns_to_drop = log_file.columns[log_file.isna().sum() > threshold].tolist()\\n        log_file = log_file.drop(columns=columns_to_drop)\\n\\n        log_file = log_file.dropna()\\n\\n        activities_to_check = incomplete_dict[log_name]  # [\\\"A_APPROVED\\\", \\\"A_REGISTERED\\\", \\\"A_ACTIVATED\\\", \\\"A_CANCELLED\\\", \\\"A_DECLINED\\\"]       \\n        contains_activity = log_file[activity_col].isin(activities_to_check).groupby(log_file[case_id_col]).max().reset_index()\\n        complete_cases = contains_activity[contains_activity[activity_col] == True][case_id_col]\\n        log_file = log_file[log_file[case_id_col].isin(complete_cases.tolist())]       \\n\\n\\n        # Calculate elapsed time and print resource usage metrics\\n        end_time = time.time()\\n        elapsed_time = end_time - start_time\\n        self._print_metrics(elapsed_time)\\n        return log_file\\n\\n    \\n    @profile\\n    def extract_temporal_features(self, log_file):\\n        start_time = time.time()\\n        print(\\\"Extracting timestamp features...\\\")\\n\\n\\n        # Calculate event_nr\\n        log_file['event_nr'] = log_file.groupby(case_id_col).cumcount() + 1\\n\\n        # Calculate case_length\\n        log_file['case_length'] = log_file.groupby(case_id_col)['event_nr'].transform('max')\\n\\n        # Extract temporal context information\\n        log_file['hour_of_day'] = log_file[timestamp_col].dt.hour\\n        log_file['day_of_week'] = log_file[timestamp_col].dt.dayofweek + 1  # Monday is 1, Sunday is 7\\n        log_file['day_of_month'] = log_file[timestamp_col].dt.day\\n        log_file['month_of_year'] = log_file[timestamp_col].dt.month\\n\\n        # Calculate elapsed time and print resource usage metrics\\n        end_time = time.time()\\n        elapsed_time = end_time - start_time\\n        self._print_metrics(elapsed_time)\\n        \\n        # Get the list of columns to check\\n        columns_to_check = log_file.columns\\n        print(columns_to_check)\\n\\n        # Iterate through the columns and delete those containing 'time' except for 'timestamp'\\n        for col in columns_to_check:\\n            if ('time' in col.lower() or 'date' in col.lower()) and col != timestamp_col:\\n                print(f\\\"column to be deleted: {col}\\\")\\n                del log_file[col]\\n\\n\\n        return log_file\";\n                var nbb_formatted_code = \"import warnings\\n\\nwarnings.filterwarnings(\\\"ignore\\\")\\n\\nimport os\\nimport csv\\n\\nimport psutil\\nimport time\\nfrom memory_profiler import profile\\nfrom pandas import Timestamp\\nimport pm4py\\nimport pandas as pd\\nimport numpy as np\\n\\nfrom concurrent.futures import ProcessPoolExecutor, as_completed\\n\\n\\nclass DataPreprocessorModinDask:\\n    def __init__(self, log_name, input_data_folder, output_data_folder):\\n        self.log_name = log_name\\n        self.input_data_folder = input_data_folder\\n        self.output_data_folder = output_data_folder\\n\\n    def _print_metrics(self, elapsed_time):\\n        # Helper function to print resource usage metrics\\n        cpu_percent = psutil.cpu_percent()\\n        memory_info = psutil.virtual_memory()\\n        # print(f\\\"Time: {elapsed_time} seconds\\\")\\n        # print(f\\\"CPU Usage: {cpu_percent}%\\\")\\n        # print(f\\\"Memory Usage: {memory_info.percent}%\\\")\\n\\n    @profile\\n    def read_log(self):\\n        start_time = time.time()\\n        print(\\\"Reading log...\\\")\\n\\n        # Get the first file in the input folder\\n        files = os.listdir(self.input_data_folder)[0]\\n        log_path = os.path.join(self.input_data_folder, files)\\n\\n        # Read the log based on its extension\\n        if log_path.lower().endswith(\\\".csv\\\"):\\n            with open(log_path, \\\"r\\\") as file:\\n                # Use Sniffer to infer the delimiter\\n                dialect = csv.Sniffer().sniff(file.read(10000))\\n            try:\\n                # Read CSV with Dask, specifying dtype for certain columns\\n                log_file = pd.read_csv(\\n                    log_path,\\n                    sep=dialect.delimiter,\\n                    dtype={\\\"Resource\\\": \\\"object\\\", \\\"article\\\": \\\"object\\\"},\\n                )\\n            except:\\n                log_file = pd.read_csv(log_path, sep=dialect.delimiter)\\n            log_file = log_file.rename(\\n                columns=lambda x: x.strip().lower().replace(\\\" \\\", \\\"_\\\")\\n            )\\n            log_file = log_file.rename(\\n                columns=dict(\\n                    zip(\\n                        dataset_dict[self.log_name],\\n                        [case_id_col, activity_col, resource_col, timestamp_col],\\n                    )\\n                )\\n            )\\n        elif log_path.lower().endswith(\\\".xes\\\") or log_path.lower().endswith(\\\".xes.gz\\\"):\\n            # Read XES log using pm4py and convert to Dask DataFrame\\n            log_file = pm4py.read_xes(log_path)\\n            log_file = dataframe_utils.convert_traces_dataframe(log_file)\\n            log_file = from_pandas(log_file, npartitions=1)\\n            log_file = log_file.rename(\\n                columns=dict(\\n                    zip(\\n                        dataset_dict[self.log_name],\\n                        [case_id_col, activity_col, resource_col, timestamp_col],\\n                    )\\n                )\\n            )\\n        else:\\n            raise ValueError(\\n                \\\"Unsupported file extension. Supported extensions: .csv, .xes\\\"\\n            )\\n\\n        # Calculate elapsed time and print resource usage metrics\\n        end_time = time.time()\\n        elapsed_time = end_time - start_time\\n        self._print_metrics(elapsed_time)\\n        return log_file\\n\\n    @profile\\n    def clean_data(self, log_file):\\n        start_time = time.time()\\n        print(\\\"Cleaning data...\\\")\\n\\n        # Convert timestamp column to datetime\\n        log_file[timestamp_col] = pd.to_datetime(\\n            log_file[timestamp_col], format=\\\"mixed\\\", infer_datetime_format=True\\n        )\\n        log_file = log_file.sort_values(by=[timestamp_col])\\n\\n        # Remove white spaces from column values\\n        log_file = log_file.applymap(lambda x: x.strip() if isinstance(x, str) else x)\\n\\n        # Convert the 'resource' column to string\\n        log_file[resource_col] = log_file[resource_col].astype(str)\\n\\n        # Replace unique resource values with 'res{i}' format\\n        unique_resources = log_file[resource_col].unique()\\n        resource_mapping = {\\n            original_value: f\\\"res{i + 1}\\\"\\n            for i, original_value in enumerate(unique_resources)\\n        }\\n        log_file[resource_col] = log_file[resource_col].replace(resource_mapping)\\n\\n        threshold_percentage = 25\\n        threshold = len(log_file) * (threshold_percentage / 100)\\n        columns_to_drop = log_file.columns[log_file.isna().sum() > threshold].tolist()\\n        log_file = log_file.drop(columns=columns_to_drop)\\n\\n        log_file = log_file.dropna()\\n\\n        activities_to_check = incomplete_dict[\\n            log_name\\n        ]  # [\\\"A_APPROVED\\\", \\\"A_REGISTERED\\\", \\\"A_ACTIVATED\\\", \\\"A_CANCELLED\\\", \\\"A_DECLINED\\\"]\\n        contains_activity = (\\n            log_file[activity_col]\\n            .isin(activities_to_check)\\n            .groupby(log_file[case_id_col])\\n            .max()\\n            .reset_index()\\n        )\\n        complete_cases = contains_activity[contains_activity[activity_col] == True][\\n            case_id_col\\n        ]\\n        log_file = log_file[log_file[case_id_col].isin(complete_cases.tolist())]\\n\\n        # Calculate elapsed time and print resource usage metrics\\n        end_time = time.time()\\n        elapsed_time = end_time - start_time\\n        self._print_metrics(elapsed_time)\\n        return log_file\\n\\n    @profile\\n    def extract_temporal_features(self, log_file):\\n        start_time = time.time()\\n        print(\\\"Extracting timestamp features...\\\")\\n\\n        # Calculate event_nr\\n        log_file[\\\"event_nr\\\"] = log_file.groupby(case_id_col).cumcount() + 1\\n\\n        # Calculate case_length\\n        log_file[\\\"case_length\\\"] = log_file.groupby(case_id_col)[\\\"event_nr\\\"].transform(\\n            \\\"max\\\"\\n        )\\n\\n        # Extract temporal context information\\n        log_file[\\\"hour_of_day\\\"] = log_file[timestamp_col].dt.hour\\n        log_file[\\\"day_of_week\\\"] = (\\n            log_file[timestamp_col].dt.dayofweek + 1\\n        )  # Monday is 1, Sunday is 7\\n        log_file[\\\"day_of_month\\\"] = log_file[timestamp_col].dt.day\\n        log_file[\\\"month_of_year\\\"] = log_file[timestamp_col].dt.month\\n\\n        # Calculate elapsed time and print resource usage metrics\\n        end_time = time.time()\\n        elapsed_time = end_time - start_time\\n        self._print_metrics(elapsed_time)\\n\\n        # Get the list of columns to check\\n        columns_to_check = log_file.columns\\n        print(columns_to_check)\\n\\n        # Iterate through the columns and delete those containing 'time' except for 'timestamp'\\n        for col in columns_to_check:\\n            if (\\n                \\\"time\\\" in col.lower() or \\\"date\\\" in col.lower()\\n            ) and col != timestamp_col:\\n                print(f\\\"column to be deleted: {col}\\\")\\n                del log_file[col]\\n\\n        return log_file\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import csv\n",
    "\n",
    "import psutil\n",
    "import time\n",
    "from memory_profiler import profile\n",
    "from pandas import Timestamp\n",
    "import pm4py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "\n",
    "\n",
    "class DataPreprocessorModinDask:\n",
    "    def __init__(self, log_name, input_data_folder, output_data_folder):\n",
    "        self.log_name = log_name\n",
    "        self.input_data_folder = input_data_folder\n",
    "        self.output_data_folder = output_data_folder\n",
    "\n",
    "    def _print_metrics(self, elapsed_time):\n",
    "        # Helper function to print resource usage metrics\n",
    "        cpu_percent = psutil.cpu_percent()\n",
    "        memory_info = psutil.virtual_memory()\n",
    "        # print(f\"Time: {elapsed_time} seconds\")\n",
    "        # print(f\"CPU Usage: {cpu_percent}%\")\n",
    "        # print(f\"Memory Usage: {memory_info.percent}%\")\n",
    "\n",
    "    @profile\n",
    "    def read_log(self):\n",
    "        start_time = time.time()\n",
    "        print(\"Reading log...\")\n",
    "\n",
    "        # Get the first file in the input folder\n",
    "        files = os.listdir(self.input_data_folder)[0]\n",
    "        log_path = os.path.join(self.input_data_folder, files)\n",
    "\n",
    "        # Read the log based on its extension\n",
    "        if log_path.lower().endswith('.csv'):\n",
    "            with open(log_path, 'r') as file:\n",
    "                # Use Sniffer to infer the delimiter\n",
    "                dialect = csv.Sniffer().sniff(file.read(10000))\n",
    "            try:\n",
    "                # Read CSV with Dask, specifying dtype for certain columns\n",
    "                log_file = pd.read_csv(log_path, sep=dialect.delimiter, dtype={'Resource': 'object', 'article': 'object'})\n",
    "            except:\n",
    "                log_file = pd.read_csv(log_path, sep=dialect.delimiter)\n",
    "            log_file = log_file.rename(columns=lambda x: x.strip().lower().replace(' ', '_'))\n",
    "            log_file = log_file.rename(\n",
    "                columns=dict(zip(dataset_dict[self.log_name], [case_id_col, activity_col, resource_col, timestamp_col])))\n",
    "        elif log_path.lower().endswith('.xes') or log_path.lower().endswith('.xes.gz'):\n",
    "            # Read XES log using pm4py and convert to Dask DataFrame\n",
    "            log_file = pm4py.read_xes(log_path)\n",
    "            log_file = dataframe_utils.convert_traces_dataframe(log_file)\n",
    "            log_file = from_pandas(log_file, npartitions=1)\n",
    "            log_file = log_file.rename(\n",
    "                columns=dict(zip(dataset_dict[self.log_name], [case_id_col, activity_col, resource_col, timestamp_col])))\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported file extension. Supported extensions: .csv, .xes\")\n",
    "\n",
    "        # Calculate elapsed time and print resource usage metrics\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        self._print_metrics(elapsed_time)\n",
    "        return log_file\n",
    "\n",
    "    @profile\n",
    "    def clean_data(self, log_file):\n",
    "        start_time = time.time()\n",
    "        print(\"Cleaning data...\")\n",
    "\n",
    "        # Convert timestamp column to datetime\n",
    "        log_file[timestamp_col] = pd.to_datetime(log_file[timestamp_col], format=\"mixed\", infer_datetime_format=True)\n",
    "        log_file = log_file.sort_values(by=[timestamp_col])\n",
    "\n",
    "       # Remove white spaces from column values\n",
    "        log_file = log_file.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "\n",
    "        # Convert the 'resource' column to string\n",
    "        log_file[resource_col] = log_file[resource_col].astype(str)\n",
    "\n",
    "        # Replace unique resource values with 'res{i}' format\n",
    "        unique_resources = log_file[resource_col].unique()\n",
    "        resource_mapping = {original_value: f'res{i + 1}' for i, original_value in enumerate(unique_resources)}\n",
    "        log_file[resource_col] = log_file[resource_col].replace(resource_mapping)\n",
    "\n",
    "        threshold_percentage = 25\n",
    "        threshold = len(log_file) * (threshold_percentage / 100)\n",
    "        columns_to_drop = log_file.columns[log_file.isna().sum() > threshold].tolist()\n",
    "        log_file = log_file.drop(columns=columns_to_drop)\n",
    "\n",
    "        log_file = log_file.dropna()\n",
    "\n",
    "        activities_to_check = incomplete_dict[log_name]  # [\"A_APPROVED\", \"A_REGISTERED\", \"A_ACTIVATED\", \"A_CANCELLED\", \"A_DECLINED\"]       \n",
    "        contains_activity = log_file[activity_col].isin(activities_to_check).groupby(log_file[case_id_col]).max().reset_index()\n",
    "        complete_cases = contains_activity[contains_activity[activity_col] == True][case_id_col]\n",
    "        log_file = log_file[log_file[case_id_col].isin(complete_cases.tolist())]       \n",
    "\n",
    "\n",
    "        # Calculate elapsed time and print resource usage metrics\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        self._print_metrics(elapsed_time)\n",
    "        return log_file\n",
    "\n",
    "    \n",
    "    @profile\n",
    "    def extract_temporal_features(self, log_file):\n",
    "        start_time = time.time()\n",
    "        print(\"Extracting timestamp features...\")\n",
    "\n",
    "\n",
    "        # Calculate event_nr\n",
    "        log_file['event_nr'] = log_file.groupby(case_id_col).cumcount() + 1\n",
    "\n",
    "        # Calculate case_length\n",
    "        log_file['case_length'] = log_file.groupby(case_id_col)['event_nr'].transform('max')\n",
    "\n",
    "        # Extract temporal context information\n",
    "        log_file['hour_of_day'] = log_file[timestamp_col].dt.hour\n",
    "        log_file['day_of_week'] = log_file[timestamp_col].dt.dayofweek + 1  # Monday is 1, Sunday is 7\n",
    "        log_file['day_of_month'] = log_file[timestamp_col].dt.day\n",
    "        log_file['month_of_year'] = log_file[timestamp_col].dt.month\n",
    "\n",
    "        # Calculate elapsed time and print resource usage metrics\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        self._print_metrics(elapsed_time)\n",
    "        \n",
    "        # Get the list of columns to check\n",
    "        columns_to_check = log_file.columns\n",
    "        print(columns_to_check)\n",
    "\n",
    "        # Iterate through the columns and delete those containing 'time' except for 'timestamp'\n",
    "        for col in columns_to_check:\n",
    "            if ('time' in col.lower() or 'date' in col.lower()) and col != timestamp_col:\n",
    "                print(f\"column to be deleted: {col}\")\n",
    "                del log_file[col]\n",
    "\n",
    "\n",
    "        return log_file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================\n",
      " Log: bpic2012\n",
      "==================\n",
      "\n",
      "ERROR: Could not find file /tmp/ipykernel_336143/3469093111.py\n",
      "Reading log...\n",
      "(164509, 7)\n",
      "ERROR: Could not find file /tmp/ipykernel_336143/3469093111.py\n",
      "Cleaning data...\n",
      "ERROR: Could not find file /tmp/ipykernel_336143/3469093111.py\n",
      "Extracting timestamp features...\n",
      "Index(['case_id', 'timestamp', 'end_time', 'amount_req', 'reg_date',\n",
      "       'activity', 'resource', 'event_nr', 'case_length', 'hour_of_day',\n",
      "       'day_of_week', 'day_of_month', 'month_of_year'],\n",
      "      dtype='object')\n",
      "column to be deleted: end_time\n",
      "column to be deleted: reg_date\n",
      "Saving csv file...\n",
      "(156962, 11)\n",
      "Done!\n",
      "\n",
      "==================\n",
      " Log: bpic2017\n",
      "==================\n",
      "\n",
      "ERROR: Could not find file /tmp/ipykernel_336143/3469093111.py\n",
      "Reading log...\n",
      "(1198366, 26)\n",
      "ERROR: Could not find file /tmp/ipykernel_336143/3469093111.py\n",
      "Cleaning data...\n",
      "ERROR: Could not find file /tmp/ipykernel_336143/3469093111.py\n",
      "Extracting timestamp features...\n",
      "Index(['applicationtype', 'loangoal', 'requestedamount', 'case_id', 'label',\n",
      "       'activity', 'resource', 'action', 'eventorigin', 'lifecycle:transition',\n",
      "       'accepted', 'selected', 'firstwithdrawalamount', 'monthlycost',\n",
      "       'numberofterms', 'offeredamount', 'creditscore', 'timesincelastevent',\n",
      "       'timesincecasestart', 'timesincemidnight', 'event_nr', 'month',\n",
      "       'weekday', 'hour', 'open_cases', 'timestamp', 'case_length',\n",
      "       'hour_of_day', 'day_of_week', 'day_of_month', 'month_of_year'],\n",
      "      dtype='object')\n",
      "column to be deleted: timesincelastevent\n",
      "column to be deleted: timesincecasestart\n",
      "column to be deleted: timesincemidnight\n",
      "Saving csv file...\n",
      "(1198319, 28)\n",
      "Done!\n"
     ]
    },
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 4;\n                var nbb_unformatted_code = \"# Usage example:\\nresults_data = []\\n# Initialize your DataPreprocessorModinDask object and call read_log() and clean_data()\\nfor log_name in logs:\\n    print(\\\"\\\\n==================\\\\n Log: %s\\\\n==================\\\\n\\\" % (log_name,))\\n    input_data_folder = \\\"./../data/%s\\\" % (log_name,)\\n    output_data_folder = \\\"./../prepared_data/%s\\\" % log_name\\n\\n    data_preprocessor = DataPreprocessorModinDask(log_name, input_data_folder, output_data_folder)\\n    log_file = data_preprocessor.read_log()\\n    print(log_file.shape)\\n    cleaned_data = data_preprocessor.clean_data(log_file)\\n    features_data = data_preprocessor.extract_temporal_features(cleaned_data)\\n    features_data.name = \\\"prepared_features_data_%s.csv\\\" % log_name\\n    results_data.append(features_data)\\n\\n    \\n    \\n    print(\\\"Saving csv file...\\\")\\n    results_dir = \\\"./../prepared_data/%s/\\\" % log_name\\n    import os\\n\\n    if not os.path.exists(os.path.join(results_dir)):\\n        os.makedirs(os.path.join(results_dir))\\n\\n    features_data.to_csv(\\n        os.path.join(\\n            results_dir, features_data.name\\n        ),\\n        index=False,\\n        sep=\\\";\\\",\\n    )\\n    print(features_data.shape)\\n    features_data.name = \\\"prepared_features_data_%s.parquet\\\" % log_name\\n    features_data.to_parquet( os.path.join(\\n            results_dir, features_data.name\\n        ))\\n\\n    print(\\\"Done!\\\")\";\n                var nbb_formatted_code = \"# Usage example:\\nresults_data = []\\n# Initialize your DataPreprocessorModinDask object and call read_log() and clean_data()\\nfor log_name in logs:\\n    print(\\\"\\\\n==================\\\\n Log: %s\\\\n==================\\\\n\\\" % (log_name,))\\n    input_data_folder = \\\"./../data/%s\\\" % (log_name,)\\n    output_data_folder = \\\"./../prepared_data/%s\\\" % log_name\\n\\n    data_preprocessor = DataPreprocessorModinDask(\\n        log_name, input_data_folder, output_data_folder\\n    )\\n    log_file = data_preprocessor.read_log()\\n    print(log_file.shape)\\n    cleaned_data = data_preprocessor.clean_data(log_file)\\n    features_data = data_preprocessor.extract_temporal_features(cleaned_data)\\n    features_data.name = \\\"prepared_features_data_%s.csv\\\" % log_name\\n    results_data.append(features_data)\\n\\n    print(\\\"Saving csv file...\\\")\\n    results_dir = \\\"./../prepared_data/%s/\\\" % log_name\\n    import os\\n\\n    if not os.path.exists(os.path.join(results_dir)):\\n        os.makedirs(os.path.join(results_dir))\\n\\n    features_data.to_csv(\\n        os.path.join(results_dir, features_data.name),\\n        index=False,\\n        sep=\\\";\\\",\\n    )\\n    print(features_data.shape)\\n    features_data.name = \\\"prepared_features_data_%s.parquet\\\" % log_name\\n    features_data.to_parquet(os.path.join(results_dir, features_data.name))\\n\\n    print(\\\"Done!\\\")\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Usage example:\n",
    "results_data = []\n",
    "# Initialize your DataPreprocessorModinDask object and call read_log() and clean_data()\n",
    "for log_name in logs:\n",
    "    print(\"\\n==================\\n Log: %s\\n==================\\n\" % (log_name,))\n",
    "    input_data_folder = \"./../data/%s\" % (log_name,)\n",
    "    output_data_folder = \"./../prepared_data/%s\" % log_name\n",
    "\n",
    "    data_preprocessor = DataPreprocessorModinDask(log_name, input_data_folder, output_data_folder)\n",
    "    log_file = data_preprocessor.read_log()\n",
    "    print(log_file.shape)\n",
    "    cleaned_data = data_preprocessor.clean_data(log_file)\n",
    "    features_data = data_preprocessor.extract_temporal_features(cleaned_data)\n",
    "    features_data.name = \"prepared_features_data_%s.csv\" % log_name\n",
    "    results_data.append(features_data)\n",
    "\n",
    "    \n",
    "    \n",
    "    print(\"Saving csv file...\")\n",
    "    results_dir = \"./../prepared_data/%s/\" % log_name\n",
    "    import os\n",
    "\n",
    "    if not os.path.exists(os.path.join(results_dir)):\n",
    "        os.makedirs(os.path.join(results_dir))\n",
    "\n",
    "    features_data.to_csv(\n",
    "        os.path.join(\n",
    "            results_dir, features_data.name\n",
    "        ),\n",
    "        index=False,\n",
    "        sep=\";\",\n",
    "    )\n",
    "    print(features_data.shape)\n",
    "    features_data.name = \"prepared_features_data_%s.parquet\" % log_name\n",
    "    features_data.to_parquet( os.path.join(\n",
    "            results_dir, features_data.name\n",
    "        ))\n",
    "\n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Possible Interventions For BPIC2017 from the winner student report:\n",
    "\n",
    "1. **Sending Another Loan Offer:**\n",
    "   - *Intervention:* Send offers to clients as soon as possible. For all case endpoints, this has been shown to have the greatest effect on cancellation rates. Sending offers to clients within 4 days may decrease cancellation rates by 5% up to 10%.\n",
    "   - *Treatment 1:* Cases that receive only one offer are in the control group, while cases that receive more than one offer are in the treatment group.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 5;\n                var nbb_unformatted_code = \"# import pandas as pd\\n# from concurrent.futures import ProcessPoolExecutor, as_completed\\n\\n\\n# results_data2 = []\\n# for log_name in logs:\\n#     print(\\\"\\\\n==================\\\\n Log: %s\\\\n==================\\\\n\\\" % (log_name,))\\n#     positive_activities = positive_activities_dict[log_name]\\n\\n#     # Initialize a global state to keep track of positive activities across cases\\n#     global_positive_cases = set()\\n\\n#     # Define a function to label each group\\n#     def label_group(chunk):\\n#         # Check if any positive activity exists in the case\\n#         chunk['is_positive'] = chunk.groupby('case_id')[activity_col].transform(lambda x: any(activity in positive_activities for activity in x))\\n        \\n#         # Update the global positive cases state\\n#         positive_cases_from_chunk = set(chunk.loc[chunk['is_positive'], 'case_id'])\\n#         global_positive_cases.update(positive_cases_from_chunk)\\n        \\n#         # Assign label based on the result\\n#         chunk['label'] = chunk.apply(lambda row: pos_label if row['is_positive'] else neg_label, axis=1)\\n        \\n#         return chunk.drop('is_positive', axis=1)\\n\\n#     # Set the chunk size based on your system's memory constraints\\n#     chunk_size = chunk_s\\n\\n#     # Split the DataFrame into chunks\\n#     features_data = results_data[logs.index(log_name)]\\n\\n#     num_chunks = len(features_data) // chunk_size\\n#     chunks = [features_data.iloc[i * chunk_size:(i + 1) * chunk_size] for i in range(num_chunks + 1)]\\n\\n#     # Initialize a ProcessPoolExecutor for parallel processing\\n#     with ProcessPoolExecutor() as executor:\\n#         futures = [executor.submit(label_group, chunk) for chunk in chunks]\\n\\n#         # Collect the results as they become available\\n#         results = []\\n#         for future in as_completed(futures):\\n#             results.append(future.result())\\n\\n#     # Concatenate the results of all chunks into a final DataFrame\\n#     labeled_data = pd.concat(results, ignore_index=True)\\n#     results_data2.append(labeled_data)\\n    \\n\\n#     labeled_data.name = \\\"labeled_data_%s.csv\\\" % log_name\\n\\n#     print(\\\"Saving csv file...\\\")\\n#     results_dir = \\\"./../prepared_data/%s/\\\" % log_name\\n#     import os\\n\\n#     if not os.path.exists(os.path.join(results_dir)):\\n#         os.makedirs(os.path.join(results_dir))\\n\\n#     labeled_data.to_csv(\\n#         os.path.join(\\n#             results_dir, labeled_data.name\\n#         ),\\n#         index=False,\\n#         sep=\\\";\\\",\\n#     )\\n\\n#     print(\\\"Done!\\\\n\\\")\";\n                var nbb_formatted_code = \"# import pandas as pd\\n# from concurrent.futures import ProcessPoolExecutor, as_completed\\n\\n\\n# results_data2 = []\\n# for log_name in logs:\\n#     print(\\\"\\\\n==================\\\\n Log: %s\\\\n==================\\\\n\\\" % (log_name,))\\n#     positive_activities = positive_activities_dict[log_name]\\n\\n#     # Initialize a global state to keep track of positive activities across cases\\n#     global_positive_cases = set()\\n\\n#     # Define a function to label each group\\n#     def label_group(chunk):\\n#         # Check if any positive activity exists in the case\\n#         chunk['is_positive'] = chunk.groupby('case_id')[activity_col].transform(lambda x: any(activity in positive_activities for activity in x))\\n\\n#         # Update the global positive cases state\\n#         positive_cases_from_chunk = set(chunk.loc[chunk['is_positive'], 'case_id'])\\n#         global_positive_cases.update(positive_cases_from_chunk)\\n\\n#         # Assign label based on the result\\n#         chunk['label'] = chunk.apply(lambda row: pos_label if row['is_positive'] else neg_label, axis=1)\\n\\n#         return chunk.drop('is_positive', axis=1)\\n\\n#     # Set the chunk size based on your system's memory constraints\\n#     chunk_size = chunk_s\\n\\n#     # Split the DataFrame into chunks\\n#     features_data = results_data[logs.index(log_name)]\\n\\n#     num_chunks = len(features_data) // chunk_size\\n#     chunks = [features_data.iloc[i * chunk_size:(i + 1) * chunk_size] for i in range(num_chunks + 1)]\\n\\n#     # Initialize a ProcessPoolExecutor for parallel processing\\n#     with ProcessPoolExecutor() as executor:\\n#         futures = [executor.submit(label_group, chunk) for chunk in chunks]\\n\\n#         # Collect the results as they become available\\n#         results = []\\n#         for future in as_completed(futures):\\n#             results.append(future.result())\\n\\n#     # Concatenate the results of all chunks into a final DataFrame\\n#     labeled_data = pd.concat(results, ignore_index=True)\\n#     results_data2.append(labeled_data)\\n\\n\\n#     labeled_data.name = \\\"labeled_data_%s.csv\\\" % log_name\\n\\n#     print(\\\"Saving csv file...\\\")\\n#     results_dir = \\\"./../prepared_data/%s/\\\" % log_name\\n#     import os\\n\\n#     if not os.path.exists(os.path.join(results_dir)):\\n#         os.makedirs(os.path.join(results_dir))\\n\\n#     labeled_data.to_csv(\\n#         os.path.join(\\n#             results_dir, labeled_data.name\\n#         ),\\n#         index=False,\\n#         sep=\\\";\\\",\\n#     )\\n\\n#     print(\\\"Done!\\\\n\\\")\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "\n",
    "\n",
    "# results_data2 = []\n",
    "# for log_name in logs:\n",
    "#     print(\"\\n==================\\n Log: %s\\n==================\\n\" % (log_name,))\n",
    "#     positive_activities = positive_activities_dict[log_name]\n",
    "\n",
    "#     # Initialize a global state to keep track of positive activities across cases\n",
    "#     global_positive_cases = set()\n",
    "\n",
    "#     # Define a function to label each group\n",
    "#     def label_group(chunk):\n",
    "#         # Check if any positive activity exists in the case\n",
    "#         chunk['is_positive'] = chunk.groupby('case_id')[activity_col].transform(lambda x: any(activity in positive_activities for activity in x))\n",
    "        \n",
    "#         # Update the global positive cases state\n",
    "#         positive_cases_from_chunk = set(chunk.loc[chunk['is_positive'], 'case_id'])\n",
    "#         global_positive_cases.update(positive_cases_from_chunk)\n",
    "        \n",
    "#         # Assign label based on the result\n",
    "#         chunk['label'] = chunk.apply(lambda row: pos_label if row['is_positive'] else neg_label, axis=1)\n",
    "        \n",
    "#         return chunk.drop('is_positive', axis=1)\n",
    "\n",
    "#     # Set the chunk size based on your system's memory constraints\n",
    "#     chunk_size = chunk_s\n",
    "\n",
    "#     # Split the DataFrame into chunks\n",
    "#     features_data = results_data[logs.index(log_name)]\n",
    "\n",
    "#     num_chunks = len(features_data) // chunk_size\n",
    "#     chunks = [features_data.iloc[i * chunk_size:(i + 1) * chunk_size] for i in range(num_chunks + 1)]\n",
    "\n",
    "#     # Initialize a ProcessPoolExecutor for parallel processing\n",
    "#     with ProcessPoolExecutor() as executor:\n",
    "#         futures = [executor.submit(label_group, chunk) for chunk in chunks]\n",
    "\n",
    "#         # Collect the results as they become available\n",
    "#         results = []\n",
    "#         for future in as_completed(futures):\n",
    "#             results.append(future.result())\n",
    "\n",
    "#     # Concatenate the results of all chunks into a final DataFrame\n",
    "#     labeled_data = pd.concat(results, ignore_index=True)\n",
    "#     results_data2.append(labeled_data)\n",
    "    \n",
    "\n",
    "#     labeled_data.name = \"labeled_data_%s.csv\" % log_name\n",
    "\n",
    "#     print(\"Saving csv file...\")\n",
    "#     results_dir = \"./../prepared_data/%s/\" % log_name\n",
    "#     import os\n",
    "\n",
    "#     if not os.path.exists(os.path.join(results_dir)):\n",
    "#         os.makedirs(os.path.join(results_dir))\n",
    "\n",
    "#     labeled_data.to_csv(\n",
    "#         os.path.join(\n",
    "#             results_dir, labeled_data.name\n",
    "#         ),\n",
    "#         index=False,\n",
    "#         sep=\";\",\n",
    "#     )\n",
    "\n",
    "#     print(\"Done!\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================\n",
      " Log: bpic2012\n",
      "==================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Submitting Tasks: 100%|██████████| 12688/12688 [01:04<00:00, 195.78it/s]\n",
      "Collecting Results: 100%|██████████| 12688/12688 [00:00<00:00, 65651.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done! - Final DataFrame:\n",
      "\n",
      "Saving csv file...\n",
      "(156962, 14)\n",
      "Done!\n",
      "\n",
      "\n",
      "==================\n",
      " Log: bpic2017\n",
      "==================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Submitting Tasks: 100%|██████████| 31411/31411 [03:08<00:00, 166.46it/s]\n",
      "Collecting Results: 100%|██████████| 31411/31411 [00:00<00:00, 79203.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done! - Final DataFrame:\n",
      "\n",
      "Saving csv file...\n",
      "(1198319, 33)\n",
      "Done!\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 6;\n                var nbb_unformatted_code = \"import pandas as pd\\nfrom concurrent.futures import ProcessPoolExecutor, as_completed\\nfrom tqdm import tqdm\\n\\n\\nresults_data3 = []\\n\\nrelevant_activities_dict = {\\n    \\\"bpic2017\\\": ['O_Sent (mail and online)', 'O_Sent (online only)'],\\n    \\\"bpic2012\\\": ['O_SENT'],\\n    \\\"trafficFines\\\": ['Add penalty']\\n    }\\n\\n# Function for Treatment 1: Increase the number of offers\\ndef apply_treatment1(group, log_name):\\n\\n    treatment2_col = \\\"numberofterms\\\"\\n    treatment3_col = \\\"firstwithdrawalamount\\\"\\n    treatment4_col = \\\"monthlycost\\\"\\n\\n\\n    \\n    if log_name == \\\"bpic2017\\\":        \\n        # Count how much offers sent in mail and online versus online only within each group\\n        group['Mail_and_Online_Count'] = (group[activity_col] == 'O_Sent (mail and online)').sum()\\n        group['Online_Only_Count'] = (group[activity_col] == 'O_Sent (online only)').sum()\\n        group['Total_Offers'] = group['Mail_and_Online_Count'] + group['Online_Only_Count']\\n\\n        # Determine the treatment or control based on the counts within each group\\n        group['Treatment1'] = 'Control'\\n        if group['Mail_and_Online_Count'].sum() > 1 or group['Online_Only_Count'].sum() > 1:\\n            group['Treatment1'] = 'Treatment'\\n\\n        # Reset the index for each group\\n        group.reset_index(drop=True, inplace=True)\\n\\n\\n\\n        return group\\n\\n    elif log_name == \\\"bpic2012\\\":\\n        group['Total_Offers'] = (group[activity_col] == 'O_SENT').sum()\\n        group['Treatment1'] = 'Control'\\n        if group['Total_Offers'].sum() > 1:\\n            group['Treatment1'] = 'Treatment'\\n        \\n        # Reset the index for each group\\n        group.reset_index(drop=True, inplace=True)\\n        return group\\n\\n\\n\\n\\n\\n\\n\\n\\ntreatments_functions = [apply_treatment1,] # apply_treatment2, apply_treatment3, apply_treatment4]\\n\\n# Set the maximum number of cases per chunk\\nmax_cases_per_chunk = 100\\n\\n\\n# List to store futures\\nfutures = []\\n\\n# Set the maximum number of worker processes\\nmax_workers = 7  # You can adjust this number based on your system's capabilities\\n\\nimport pandas as pd\\n\\ndef add_time_to_last_event_column(df, case_id_col, timestamp_col, activity_col, activities_to_track):\\n    \\\"\\\"\\\"\\n    Add a new column representing the time (in days) to the last occurrence of specified activities for each case.\\n\\n    Parameters:\\n    - df (pd.DataFrame): The DataFrame containing the event log.\\n    - case_id_col (str): The column name representing case IDs.\\n    - timestamp_col (str): The column name representing timestamps.\\n    - activity_col (str): The column name representing activity names.\\n    - activities_to_track (list): List of activity names to track the time to the last occurrence.\\n\\n    Returns:\\n    - pd.DataFrame: The DataFrame with the new column added.\\n    \\\"\\\"\\\"\\n    # Ensure timestamps are in datetime format\\n    df[timestamp_col] = pd.to_datetime(df[timestamp_col])\\n\\n    # Filter for specified activities\\n    filtered_df = df[df[activity_col].isin(activities_to_track)]\\n\\n    # Find the last occurrence for each case\\n    last_occurrences = filtered_df.groupby(case_id_col)[timestamp_col].max()\\n\\n    # Merge the last occurrences back to the original DataFrame\\n    df = pd.merge(df, last_occurrences, how='left', on=case_id_col, suffixes=('', '_last'))\\n\\n    # Calculate the time difference in days\\n    df['time_to_last_event_days'] = (df['timestamp_last'] - df[timestamp_col]).dt.total_seconds() / (60 * 60 * 24)\\n\\n    # Fill NaN values (cases without the specified activities) with a default value, e.g., -1\\n    df['time_to_last_event_days'].fillna(-1, inplace=True)\\n\\n    # Drop the auxiliary columns used for the calculation\\n    df.drop(['timestamp_last'], axis=1, inplace=True)\\n    df.reset_index(drop=True, inplace=True)\\n\\n    return df\\n\\n\\ndef determine_overall_treatment(row):\\n    treatments = ['Treatment1', 'Treatment2', 'Treatment3', 'Treatment4']\\n    for treatment in treatments:\\n        if row[treatment] == 'Treatment':\\n            return treatment\\n    return 'Controle'\\n\\nfor log_name in logs:\\n    print(\\\"\\\\n==================\\\\n Log: %s\\\\n==================\\\\n\\\" % (log_name,))    \\n    relevant_activities = relevant_activities_dict[log_name]\\n    labeled_data = results_data[logs.index(log_name)]\\n  \\n    futures = []\\n    results = []\\n        \\n    # Initialize a ProcessPoolExecutor for parallel processing\\n    with ProcessPoolExecutor(max_workers=max_workers) as executor:\\n        for treatment_function in treatments_functions:\\n            # Split the DataFrame into chunks based on case_id_col\\n            grouped = None\\n            grouped = labeled_data.groupby(case_id_col, as_index=False)\\n            # Iterate over groups and submit tasks to the executor\\n            for name, group in tqdm(grouped, desc=\\\"Submitting Tasks\\\"):\\n                # Reset the index for each group\\n                group.reset_index(drop=True, inplace=True)\\n\\n\\n                # Submit the task to the executor for each group\\n                future = executor.submit(treatment_function, group.copy(), log_name,)\\n                futures.append(future)\\n\\n    \\n    for future in tqdm(as_completed(futures), total=len(futures), desc=\\\"Collecting Results\\\"):\\n        try:\\n            treated_chunk = future.result()\\n            results.append(treated_chunk)\\n        except Exception as e:\\n            print(f\\\"Error collecting result for {future}: {e}\\\")\\n\\n    try:\\n        final_result = pd.concat(results, ignore_index=True)\\n        final_result.reset_index(drop=True, inplace=True)  # Resetting index to avoid duplicate indices\\n        # print(\\\"Successfully concatenated results into final DataFrame.\\\")\\n    except Exception as e:\\n        print(f\\\"Error concatenating results into final DataFrame: {e}\\\")\\n\\n    \\n    # add time to last event column\\n    activities_to_track = incomplete_dict[log_name]\\n    #print(\\\"Activities to track:\\\", activities_to_track)\\n    final_result = add_time_to_last_event_column(final_result, case_id_col, timestamp_col, activity_col, activities_to_track)\\n\\n    # Display the final result\\n    print(\\\"\\\\nDone! - Final DataFrame:\\\\n\\\")\\n    final_result.name = \\\"date_with_treatments_%s.csv\\\" % log_name\\n    #print(\\\"shape after treatment:\\\", final_result.shape[0], \\\"rows\\\", final_result.shape[1], \\\"columns\\\")\\n\\n    print(\\\"Saving csv file...\\\")\\n    results_dir = \\\"./../prepared_data/%s/\\\" % log_name\\n    import os\\n\\n    results_data3.append(final_result)\\n\\n    if not os.path.exists(os.path.join(results_dir)):\\n        os.makedirs(os.path.join(results_dir))\\n\\n    final_result.to_csv(\\n        os.path.join(\\n            results_dir, final_result.name\\n        ),\\n        index=False,\\n        sep=\\\";\\\",\\n    )\\n    final_result.name = \\\"date_with_treatments_%s.parquet\\\" % log_name\\n    print(final_result.shape)\\n    \\n    final_result.to_parquet( os.path.join(\\n            results_dir, final_result.name\\n        ))\\n\\n    print(\\\"Done!\\\\n\\\")\\n    final_result = None\";\n                var nbb_formatted_code = \"import pandas as pd\\nfrom concurrent.futures import ProcessPoolExecutor, as_completed\\nfrom tqdm import tqdm\\n\\n\\nresults_data3 = []\\n\\nrelevant_activities_dict = {\\n    \\\"bpic2017\\\": [\\\"O_Sent (mail and online)\\\", \\\"O_Sent (online only)\\\"],\\n    \\\"bpic2012\\\": [\\\"O_SENT\\\"],\\n    \\\"trafficFines\\\": [\\\"Add penalty\\\"],\\n}\\n\\n\\n# Function for Treatment 1: Increase the number of offers\\ndef apply_treatment1(group, log_name):\\n    treatment2_col = \\\"numberofterms\\\"\\n    treatment3_col = \\\"firstwithdrawalamount\\\"\\n    treatment4_col = \\\"monthlycost\\\"\\n\\n    if log_name == \\\"bpic2017\\\":\\n        # Count how much offers sent in mail and online versus online only within each group\\n        group[\\\"Mail_and_Online_Count\\\"] = (\\n            group[activity_col] == \\\"O_Sent (mail and online)\\\"\\n        ).sum()\\n        group[\\\"Online_Only_Count\\\"] = (\\n            group[activity_col] == \\\"O_Sent (online only)\\\"\\n        ).sum()\\n        group[\\\"Total_Offers\\\"] = (\\n            group[\\\"Mail_and_Online_Count\\\"] + group[\\\"Online_Only_Count\\\"]\\n        )\\n\\n        # Determine the treatment or control based on the counts within each group\\n        group[\\\"Treatment1\\\"] = \\\"Control\\\"\\n        if (\\n            group[\\\"Mail_and_Online_Count\\\"].sum() > 1\\n            or group[\\\"Online_Only_Count\\\"].sum() > 1\\n        ):\\n            group[\\\"Treatment1\\\"] = \\\"Treatment\\\"\\n\\n        # Reset the index for each group\\n        group.reset_index(drop=True, inplace=True)\\n\\n        return group\\n\\n    elif log_name == \\\"bpic2012\\\":\\n        group[\\\"Total_Offers\\\"] = (group[activity_col] == \\\"O_SENT\\\").sum()\\n        group[\\\"Treatment1\\\"] = \\\"Control\\\"\\n        if group[\\\"Total_Offers\\\"].sum() > 1:\\n            group[\\\"Treatment1\\\"] = \\\"Treatment\\\"\\n\\n        # Reset the index for each group\\n        group.reset_index(drop=True, inplace=True)\\n        return group\\n\\n\\ntreatments_functions = [\\n    apply_treatment1,\\n]  # apply_treatment2, apply_treatment3, apply_treatment4]\\n\\n# Set the maximum number of cases per chunk\\nmax_cases_per_chunk = 100\\n\\n\\n# List to store futures\\nfutures = []\\n\\n# Set the maximum number of worker processes\\nmax_workers = 7  # You can adjust this number based on your system's capabilities\\n\\nimport pandas as pd\\n\\n\\ndef add_time_to_last_event_column(\\n    df, case_id_col, timestamp_col, activity_col, activities_to_track\\n):\\n    \\\"\\\"\\\"\\n    Add a new column representing the time (in days) to the last occurrence of specified activities for each case.\\n\\n    Parameters:\\n    - df (pd.DataFrame): The DataFrame containing the event log.\\n    - case_id_col (str): The column name representing case IDs.\\n    - timestamp_col (str): The column name representing timestamps.\\n    - activity_col (str): The column name representing activity names.\\n    - activities_to_track (list): List of activity names to track the time to the last occurrence.\\n\\n    Returns:\\n    - pd.DataFrame: The DataFrame with the new column added.\\n    \\\"\\\"\\\"\\n    # Ensure timestamps are in datetime format\\n    df[timestamp_col] = pd.to_datetime(df[timestamp_col])\\n\\n    # Filter for specified activities\\n    filtered_df = df[df[activity_col].isin(activities_to_track)]\\n\\n    # Find the last occurrence for each case\\n    last_occurrences = filtered_df.groupby(case_id_col)[timestamp_col].max()\\n\\n    # Merge the last occurrences back to the original DataFrame\\n    df = pd.merge(\\n        df, last_occurrences, how=\\\"left\\\", on=case_id_col, suffixes=(\\\"\\\", \\\"_last\\\")\\n    )\\n\\n    # Calculate the time difference in days\\n    df[\\\"time_to_last_event_days\\\"] = (\\n        df[\\\"timestamp_last\\\"] - df[timestamp_col]\\n    ).dt.total_seconds() / (60 * 60 * 24)\\n\\n    # Fill NaN values (cases without the specified activities) with a default value, e.g., -1\\n    df[\\\"time_to_last_event_days\\\"].fillna(-1, inplace=True)\\n\\n    # Drop the auxiliary columns used for the calculation\\n    df.drop([\\\"timestamp_last\\\"], axis=1, inplace=True)\\n    df.reset_index(drop=True, inplace=True)\\n\\n    return df\\n\\n\\ndef determine_overall_treatment(row):\\n    treatments = [\\\"Treatment1\\\", \\\"Treatment2\\\", \\\"Treatment3\\\", \\\"Treatment4\\\"]\\n    for treatment in treatments:\\n        if row[treatment] == \\\"Treatment\\\":\\n            return treatment\\n    return \\\"Controle\\\"\\n\\n\\nfor log_name in logs:\\n    print(\\\"\\\\n==================\\\\n Log: %s\\\\n==================\\\\n\\\" % (log_name,))\\n    relevant_activities = relevant_activities_dict[log_name]\\n    labeled_data = results_data[logs.index(log_name)]\\n\\n    futures = []\\n    results = []\\n\\n    # Initialize a ProcessPoolExecutor for parallel processing\\n    with ProcessPoolExecutor(max_workers=max_workers) as executor:\\n        for treatment_function in treatments_functions:\\n            # Split the DataFrame into chunks based on case_id_col\\n            grouped = None\\n            grouped = labeled_data.groupby(case_id_col, as_index=False)\\n            # Iterate over groups and submit tasks to the executor\\n            for name, group in tqdm(grouped, desc=\\\"Submitting Tasks\\\"):\\n                # Reset the index for each group\\n                group.reset_index(drop=True, inplace=True)\\n\\n                # Submit the task to the executor for each group\\n                future = executor.submit(\\n                    treatment_function,\\n                    group.copy(),\\n                    log_name,\\n                )\\n                futures.append(future)\\n\\n    for future in tqdm(\\n        as_completed(futures), total=len(futures), desc=\\\"Collecting Results\\\"\\n    ):\\n        try:\\n            treated_chunk = future.result()\\n            results.append(treated_chunk)\\n        except Exception as e:\\n            print(f\\\"Error collecting result for {future}: {e}\\\")\\n\\n    try:\\n        final_result = pd.concat(results, ignore_index=True)\\n        final_result.reset_index(\\n            drop=True, inplace=True\\n        )  # Resetting index to avoid duplicate indices\\n        # print(\\\"Successfully concatenated results into final DataFrame.\\\")\\n    except Exception as e:\\n        print(f\\\"Error concatenating results into final DataFrame: {e}\\\")\\n\\n    # add time to last event column\\n    activities_to_track = incomplete_dict[log_name]\\n    # print(\\\"Activities to track:\\\", activities_to_track)\\n    final_result = add_time_to_last_event_column(\\n        final_result, case_id_col, timestamp_col, activity_col, activities_to_track\\n    )\\n\\n    # Display the final result\\n    print(\\\"\\\\nDone! - Final DataFrame:\\\\n\\\")\\n    final_result.name = \\\"date_with_treatments_%s.csv\\\" % log_name\\n    # print(\\\"shape after treatment:\\\", final_result.shape[0], \\\"rows\\\", final_result.shape[1], \\\"columns\\\")\\n\\n    print(\\\"Saving csv file...\\\")\\n    results_dir = \\\"./../prepared_data/%s/\\\" % log_name\\n    import os\\n\\n    results_data3.append(final_result)\\n\\n    if not os.path.exists(os.path.join(results_dir)):\\n        os.makedirs(os.path.join(results_dir))\\n\\n    final_result.to_csv(\\n        os.path.join(results_dir, final_result.name),\\n        index=False,\\n        sep=\\\";\\\",\\n    )\\n    final_result.name = \\\"date_with_treatments_%s.parquet\\\" % log_name\\n    print(final_result.shape)\\n\\n    final_result.to_parquet(os.path.join(results_dir, final_result.name))\\n\\n    print(\\\"Done!\\\\n\\\")\\n    final_result = None\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "results_data3 = []\n",
    "\n",
    "relevant_activities_dict = {\n",
    "    \"bpic2017\": ['O_Sent (mail and online)', 'O_Sent (online only)'],\n",
    "    \"bpic2012\": ['O_SENT'],\n",
    "    \"trafficFines\": ['Add penalty']\n",
    "    }\n",
    "\n",
    "# Function for Treatment 1: Increase the number of offers\n",
    "def apply_treatment1(group, log_name):\n",
    "\n",
    "    treatment2_col = \"numberofterms\"\n",
    "    treatment3_col = \"firstwithdrawalamount\"\n",
    "    treatment4_col = \"monthlycost\"\n",
    "\n",
    "\n",
    "    \n",
    "    if log_name == \"bpic2017\":        \n",
    "        # Count how much offers sent in mail and online versus online only within each group\n",
    "        group['Mail_and_Online_Count'] = (group[activity_col] == 'O_Sent (mail and online)').sum()\n",
    "        group['Online_Only_Count'] = (group[activity_col] == 'O_Sent (online only)').sum()\n",
    "        group['Total_Offers'] = group['Mail_and_Online_Count'] + group['Online_Only_Count']\n",
    "\n",
    "        # Determine the treatment or control based on the counts within each group\n",
    "        group['Treatment1'] = 'Control'\n",
    "        if group['Mail_and_Online_Count'].sum() > 1 or group['Online_Only_Count'].sum() > 1:\n",
    "            group['Treatment1'] = 'Treatment'\n",
    "\n",
    "        # Reset the index for each group\n",
    "        group.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "        return group\n",
    "\n",
    "    elif log_name == \"bpic2012\":\n",
    "        group['Total_Offers'] = (group[activity_col] == 'O_SENT').sum()\n",
    "        group['Treatment1'] = 'Control'\n",
    "        if group['Total_Offers'].sum() > 1:\n",
    "            group['Treatment1'] = 'Treatment'\n",
    "        \n",
    "        # Reset the index for each group\n",
    "        group.reset_index(drop=True, inplace=True)\n",
    "        return group\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "treatments_functions = [apply_treatment1,] # apply_treatment2, apply_treatment3, apply_treatment4]\n",
    "\n",
    "# Set the maximum number of cases per chunk\n",
    "max_cases_per_chunk = 100\n",
    "\n",
    "\n",
    "# List to store futures\n",
    "futures = []\n",
    "\n",
    "# Set the maximum number of worker processes\n",
    "max_workers = 7  # You can adjust this number based on your system's capabilities\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def add_time_to_last_event_column(df, case_id_col, timestamp_col, activity_col, activities_to_track):\n",
    "    \"\"\"\n",
    "    Add a new column representing the time (in days) to the last occurrence of specified activities for each case.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The DataFrame containing the event log.\n",
    "    - case_id_col (str): The column name representing case IDs.\n",
    "    - timestamp_col (str): The column name representing timestamps.\n",
    "    - activity_col (str): The column name representing activity names.\n",
    "    - activities_to_track (list): List of activity names to track the time to the last occurrence.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: The DataFrame with the new column added.\n",
    "    \"\"\"\n",
    "    # Ensure timestamps are in datetime format\n",
    "    df[timestamp_col] = pd.to_datetime(df[timestamp_col])\n",
    "\n",
    "    # Filter for specified activities\n",
    "    filtered_df = df[df[activity_col].isin(activities_to_track)]\n",
    "\n",
    "    # Find the last occurrence for each case\n",
    "    last_occurrences = filtered_df.groupby(case_id_col)[timestamp_col].max()\n",
    "\n",
    "    # Merge the last occurrences back to the original DataFrame\n",
    "    df = pd.merge(df, last_occurrences, how='left', on=case_id_col, suffixes=('', '_last'))\n",
    "\n",
    "    # Calculate the time difference in days\n",
    "    df['time_to_last_event_days'] = (df['timestamp_last'] - df[timestamp_col]).dt.total_seconds() / (60 * 60 * 24)\n",
    "\n",
    "    # Fill NaN values (cases without the specified activities) with a default value, e.g., -1\n",
    "    df['time_to_last_event_days'].fillna(-1, inplace=True)\n",
    "\n",
    "    # Drop the auxiliary columns used for the calculation\n",
    "    df.drop(['timestamp_last'], axis=1, inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def determine_overall_treatment(row):\n",
    "    treatments = ['Treatment1', 'Treatment2', 'Treatment3', 'Treatment4']\n",
    "    for treatment in treatments:\n",
    "        if row[treatment] == 'Treatment':\n",
    "            return treatment\n",
    "    return 'Controle'\n",
    "\n",
    "for log_name in logs:\n",
    "    print(\"\\n==================\\n Log: %s\\n==================\\n\" % (log_name,))    \n",
    "    relevant_activities = relevant_activities_dict[log_name]\n",
    "    labeled_data = results_data[logs.index(log_name)]\n",
    "  \n",
    "    futures = []\n",
    "    results = []\n",
    "        \n",
    "    # Initialize a ProcessPoolExecutor for parallel processing\n",
    "    with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "        for treatment_function in treatments_functions:\n",
    "            # Split the DataFrame into chunks based on case_id_col\n",
    "            grouped = None\n",
    "            grouped = labeled_data.groupby(case_id_col, as_index=False)\n",
    "            # Iterate over groups and submit tasks to the executor\n",
    "            for name, group in tqdm(grouped, desc=\"Submitting Tasks\"):\n",
    "                # Reset the index for each group\n",
    "                group.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "                # Submit the task to the executor for each group\n",
    "                future = executor.submit(treatment_function, group.copy(), log_name,)\n",
    "                futures.append(future)\n",
    "\n",
    "    \n",
    "    for future in tqdm(as_completed(futures), total=len(futures), desc=\"Collecting Results\"):\n",
    "        try:\n",
    "            treated_chunk = future.result()\n",
    "            results.append(treated_chunk)\n",
    "        except Exception as e:\n",
    "            print(f\"Error collecting result for {future}: {e}\")\n",
    "\n",
    "    try:\n",
    "        final_result = pd.concat(results, ignore_index=True)\n",
    "        final_result.reset_index(drop=True, inplace=True)  # Resetting index to avoid duplicate indices\n",
    "        # print(\"Successfully concatenated results into final DataFrame.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error concatenating results into final DataFrame: {e}\")\n",
    "\n",
    "    \n",
    "    # add time to last event column\n",
    "    activities_to_track = incomplete_dict[log_name]\n",
    "    #print(\"Activities to track:\", activities_to_track)\n",
    "    final_result = add_time_to_last_event_column(final_result, case_id_col, timestamp_col, activity_col, activities_to_track)\n",
    "\n",
    "    # Display the final result\n",
    "    print(\"\\nDone! - Final DataFrame:\\n\")\n",
    "    final_result.name = \"date_with_treatments_%s.csv\" % log_name\n",
    "    #print(\"shape after treatment:\", final_result.shape[0], \"rows\", final_result.shape[1], \"columns\")\n",
    "\n",
    "    print(\"Saving csv file...\")\n",
    "    results_dir = \"./../prepared_data/%s/\" % log_name\n",
    "    import os\n",
    "\n",
    "    results_data3.append(final_result)\n",
    "\n",
    "    if not os.path.exists(os.path.join(results_dir)):\n",
    "        os.makedirs(os.path.join(results_dir))\n",
    "\n",
    "    final_result.to_csv(\n",
    "        os.path.join(\n",
    "            results_dir, final_result.name\n",
    "        ),\n",
    "        index=False,\n",
    "        sep=\";\",\n",
    "    )\n",
    "    final_result.name = \"date_with_treatments_%s.parquet\" % log_name\n",
    "    print(final_result.shape)\n",
    "    \n",
    "    final_result.to_parquet( os.path.join(\n",
    "            results_dir, final_result.name\n",
    "        ))\n",
    "\n",
    "    print(\"Done!\\n\")\n",
    "    final_result = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 7;\n                var nbb_unformatted_code = \"# import pandas as pd\\n# import numpy as np\\n# import os\\n\\n\\n\\n# # Define the label column\\n# label_col = 'label'\\n\\n\\n# def split_data(data, train_ratio, val_ratio, split_type=\\\"temporal\\\", seed=22):\\n#     # Split data into train, val, and test sets based on the specified ratios\\n#     grouped = data.groupby(case_id_col)\\n#     start_timestamps = grouped[timestamp_col].min().reset_index()\\n\\n#     # Sort start_timestamps based on the split_type\\n#     if split_type == \\\"temporal\\\":\\n#         start_timestamps = start_timestamps.sort_values(timestamp_col, ascending=True, kind=\\\"mergesort\\\")\\n#     elif split_type == \\\"random\\\":\\n#         np.random.seed(seed)\\n#         start_timestamps = start_timestamps.reindex(np.random.permutation(start_timestamps.index))\\n\\n#     train_size = int(train_ratio * len(start_timestamps))\\n#     val_size = int(val_ratio * len(start_timestamps))\\n#     test_size = len(start_timestamps) - train_size - val_size\\n\\n#     train_ids = list(start_timestamps[case_id_col])[:train_size]\\n#     val_ids = list(start_timestamps[case_id_col])[train_size:train_size + val_size]\\n#     test_ids = list(start_timestamps[case_id_col])[train_size + val_size:]\\n\\n#     train = data[data[case_id_col].isin(train_ids)].sort_values(sorting_cols, ascending=True, kind='mergesort').reset_index(drop=True)\\n#     val = data[data[case_id_col].isin(val_ids)].sort_values(sorting_cols, ascending=True, kind='mergesort').reset_index(drop=True)\\n#     test = data[data[case_id_col].isin(test_ids)].sort_values(sorting_cols, ascending=True, kind='mergesort').reset_index(drop=True)\\n\\n#     return train, val, test\\n\\n# def encode_data(data, treatment_cols, label_col, data_type):\\n#     y_numeric = np.where(data[label_col] == 'deviant', 0, 1)\\n\\n#     T_numeric = [np.where(data[treatment] == \\\"Treatment\\\", 1, 0) for treatment in treatment_cols]\\n\\n#     numeric_cols = data.select_dtypes(include='number').columns\\n#     # Exclude label_col and treatment_cols from categorical_cols\\n#     exclude_cols = [label_col] + treatment_cols\\n#     categorical_cols = data.select_dtypes(exclude='number').columns.difference(exclude_cols)\\n\\n\\n#     # Convert numeric columns to float\\n#     data[numeric_cols] = data[numeric_cols].astype(float)\\n\\n#     # Convert categorical columns to categorical type\\n#     data[categorical_cols] = data[categorical_cols].astype('category')\\n\\n#     # Apply cat.codes only to categorical columns\\n#     data[categorical_cols] = data[categorical_cols].apply(lambda x: x.cat.codes)\\n\\n#     # One-hot encode categorical columns\\n#     data_encoded = pd.concat([\\n#         pd.DataFrame(data[numeric_cols]),  # Keep numeric columns\\n#         pd.DataFrame(y_numeric, columns=['Outcome']),\\n#         pd.DataFrame(np.array(T_numeric).T, columns=treatment_cols),\\n#         pd.DataFrame(data[categorical_cols], columns=categorical_cols),\\n#     ], axis=1)\\n\\n\\n#     return data_encoded\\n\\n# def save_data(data, results_dir, dataset_name, data_type):\\n#     data.to_csv(os.path.join(results_dir, f\\\"{data_type}_{dataset_name}.csv\\\"), sep=\\\";\\\", index=False)\\n\\n# def save_data_encoded(data_encoded, results_dir, dataset_name, data_type):\\n#     data_encoded.to_csv(os.path.join(results_dir, f\\\"{data_type}_encoded_{dataset_name}.csv\\\"), sep=\\\";\\\", index=False)\\n\\n\\n# sorting_cols = [timestamp_col, activity_col]\\n\\n\\n# # Specify the desired ratios\\n# train_ratio = 0.5\\n# val_ratio = 0.3\\n# test_ratio = 0.2\\n\\n\\n\\n# for log_name in logs:\\n#     print(\\\"\\\\n==================\\\\n Log: %s\\\\n==================\\\\n\\\" % (log_name,))\\n#     treatments_data = results_data3[logs.index(log_name)]\\n\\n#     data = treatments_data.copy()\\n#     all_columns = data.columns\\n\\n#     # Split the data into train, val, and test\\n#     train, val, test = split_data(data, train_ratio, val_ratio, split_type=\\\"temporal\\\", seed=22)\\n\\n   \\n#     treatment_cols = ['Treatment1',]\\n\\n#     #dataset_name = log_name\\n#     results_dir = \\\"./../prepared_data/%s/\\\" % log_name\\n\\n\\n#     save_data(train, results_dir, log_name, \\\"train\\\")\\n#     save_data(test, results_dir, log_name, \\\"test\\\")\\n#     save_data(val, results_dir, log_name, \\\"valid\\\")\\n\\n\\n#     train_data_encoded = encode_data(train, treatment_cols, label_col, \\\"train\\\")\\n#     test_data_encoded = encode_data(test, treatment_cols, label_col, \\\"test\\\")\\n#     val_data_encoded = encode_data(val, treatment_cols, label_col, \\\"valid\\\")\\n\\n    \\n    \\n#     print(\\\"Saving the train, val, and test sets to separate files...\\\")\\n#     save_data_encoded(train_data_encoded, results_dir, log_name, \\\"train\\\")\\n#     save_data_encoded(test_data_encoded, results_dir, log_name, \\\"test\\\")\\n#     save_data_encoded(val_data_encoded, results_dir, log_name, \\\"valid\\\")\";\n                var nbb_formatted_code = \"# import pandas as pd\\n# import numpy as np\\n# import os\\n\\n\\n# # Define the label column\\n# label_col = 'label'\\n\\n\\n# def split_data(data, train_ratio, val_ratio, split_type=\\\"temporal\\\", seed=22):\\n#     # Split data into train, val, and test sets based on the specified ratios\\n#     grouped = data.groupby(case_id_col)\\n#     start_timestamps = grouped[timestamp_col].min().reset_index()\\n\\n#     # Sort start_timestamps based on the split_type\\n#     if split_type == \\\"temporal\\\":\\n#         start_timestamps = start_timestamps.sort_values(timestamp_col, ascending=True, kind=\\\"mergesort\\\")\\n#     elif split_type == \\\"random\\\":\\n#         np.random.seed(seed)\\n#         start_timestamps = start_timestamps.reindex(np.random.permutation(start_timestamps.index))\\n\\n#     train_size = int(train_ratio * len(start_timestamps))\\n#     val_size = int(val_ratio * len(start_timestamps))\\n#     test_size = len(start_timestamps) - train_size - val_size\\n\\n#     train_ids = list(start_timestamps[case_id_col])[:train_size]\\n#     val_ids = list(start_timestamps[case_id_col])[train_size:train_size + val_size]\\n#     test_ids = list(start_timestamps[case_id_col])[train_size + val_size:]\\n\\n#     train = data[data[case_id_col].isin(train_ids)].sort_values(sorting_cols, ascending=True, kind='mergesort').reset_index(drop=True)\\n#     val = data[data[case_id_col].isin(val_ids)].sort_values(sorting_cols, ascending=True, kind='mergesort').reset_index(drop=True)\\n#     test = data[data[case_id_col].isin(test_ids)].sort_values(sorting_cols, ascending=True, kind='mergesort').reset_index(drop=True)\\n\\n#     return train, val, test\\n\\n# def encode_data(data, treatment_cols, label_col, data_type):\\n#     y_numeric = np.where(data[label_col] == 'deviant', 0, 1)\\n\\n#     T_numeric = [np.where(data[treatment] == \\\"Treatment\\\", 1, 0) for treatment in treatment_cols]\\n\\n#     numeric_cols = data.select_dtypes(include='number').columns\\n#     # Exclude label_col and treatment_cols from categorical_cols\\n#     exclude_cols = [label_col] + treatment_cols\\n#     categorical_cols = data.select_dtypes(exclude='number').columns.difference(exclude_cols)\\n\\n\\n#     # Convert numeric columns to float\\n#     data[numeric_cols] = data[numeric_cols].astype(float)\\n\\n#     # Convert categorical columns to categorical type\\n#     data[categorical_cols] = data[categorical_cols].astype('category')\\n\\n#     # Apply cat.codes only to categorical columns\\n#     data[categorical_cols] = data[categorical_cols].apply(lambda x: x.cat.codes)\\n\\n#     # One-hot encode categorical columns\\n#     data_encoded = pd.concat([\\n#         pd.DataFrame(data[numeric_cols]),  # Keep numeric columns\\n#         pd.DataFrame(y_numeric, columns=['Outcome']),\\n#         pd.DataFrame(np.array(T_numeric).T, columns=treatment_cols),\\n#         pd.DataFrame(data[categorical_cols], columns=categorical_cols),\\n#     ], axis=1)\\n\\n\\n#     return data_encoded\\n\\n# def save_data(data, results_dir, dataset_name, data_type):\\n#     data.to_csv(os.path.join(results_dir, f\\\"{data_type}_{dataset_name}.csv\\\"), sep=\\\";\\\", index=False)\\n\\n# def save_data_encoded(data_encoded, results_dir, dataset_name, data_type):\\n#     data_encoded.to_csv(os.path.join(results_dir, f\\\"{data_type}_encoded_{dataset_name}.csv\\\"), sep=\\\";\\\", index=False)\\n\\n\\n# sorting_cols = [timestamp_col, activity_col]\\n\\n\\n# # Specify the desired ratios\\n# train_ratio = 0.5\\n# val_ratio = 0.3\\n# test_ratio = 0.2\\n\\n\\n# for log_name in logs:\\n#     print(\\\"\\\\n==================\\\\n Log: %s\\\\n==================\\\\n\\\" % (log_name,))\\n#     treatments_data = results_data3[logs.index(log_name)]\\n\\n#     data = treatments_data.copy()\\n#     all_columns = data.columns\\n\\n#     # Split the data into train, val, and test\\n#     train, val, test = split_data(data, train_ratio, val_ratio, split_type=\\\"temporal\\\", seed=22)\\n\\n\\n#     treatment_cols = ['Treatment1',]\\n\\n#     #dataset_name = log_name\\n#     results_dir = \\\"./../prepared_data/%s/\\\" % log_name\\n\\n\\n#     save_data(train, results_dir, log_name, \\\"train\\\")\\n#     save_data(test, results_dir, log_name, \\\"test\\\")\\n#     save_data(val, results_dir, log_name, \\\"valid\\\")\\n\\n\\n#     train_data_encoded = encode_data(train, treatment_cols, label_col, \\\"train\\\")\\n#     test_data_encoded = encode_data(test, treatment_cols, label_col, \\\"test\\\")\\n#     val_data_encoded = encode_data(val, treatment_cols, label_col, \\\"valid\\\")\\n\\n\\n#     print(\\\"Saving the train, val, and test sets to separate files...\\\")\\n#     save_data_encoded(train_data_encoded, results_dir, log_name, \\\"train\\\")\\n#     save_data_encoded(test_data_encoded, results_dir, log_name, \\\"test\\\")\\n#     save_data_encoded(val_data_encoded, results_dir, log_name, \\\"valid\\\")\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import os\n",
    "\n",
    "\n",
    "\n",
    "# # Define the label column\n",
    "# label_col = 'label'\n",
    "\n",
    "\n",
    "# def split_data(data, train_ratio, val_ratio, split_type=\"temporal\", seed=22):\n",
    "#     # Split data into train, val, and test sets based on the specified ratios\n",
    "#     grouped = data.groupby(case_id_col)\n",
    "#     start_timestamps = grouped[timestamp_col].min().reset_index()\n",
    "\n",
    "#     # Sort start_timestamps based on the split_type\n",
    "#     if split_type == \"temporal\":\n",
    "#         start_timestamps = start_timestamps.sort_values(timestamp_col, ascending=True, kind=\"mergesort\")\n",
    "#     elif split_type == \"random\":\n",
    "#         np.random.seed(seed)\n",
    "#         start_timestamps = start_timestamps.reindex(np.random.permutation(start_timestamps.index))\n",
    "\n",
    "#     train_size = int(train_ratio * len(start_timestamps))\n",
    "#     val_size = int(val_ratio * len(start_timestamps))\n",
    "#     test_size = len(start_timestamps) - train_size - val_size\n",
    "\n",
    "#     train_ids = list(start_timestamps[case_id_col])[:train_size]\n",
    "#     val_ids = list(start_timestamps[case_id_col])[train_size:train_size + val_size]\n",
    "#     test_ids = list(start_timestamps[case_id_col])[train_size + val_size:]\n",
    "\n",
    "#     train = data[data[case_id_col].isin(train_ids)].sort_values(sorting_cols, ascending=True, kind='mergesort').reset_index(drop=True)\n",
    "#     val = data[data[case_id_col].isin(val_ids)].sort_values(sorting_cols, ascending=True, kind='mergesort').reset_index(drop=True)\n",
    "#     test = data[data[case_id_col].isin(test_ids)].sort_values(sorting_cols, ascending=True, kind='mergesort').reset_index(drop=True)\n",
    "\n",
    "#     return train, val, test\n",
    "\n",
    "# def encode_data(data, treatment_cols, label_col, data_type):\n",
    "#     y_numeric = np.where(data[label_col] == 'deviant', 0, 1)\n",
    "\n",
    "#     T_numeric = [np.where(data[treatment] == \"Treatment\", 1, 0) for treatment in treatment_cols]\n",
    "\n",
    "#     numeric_cols = data.select_dtypes(include='number').columns\n",
    "#     # Exclude label_col and treatment_cols from categorical_cols\n",
    "#     exclude_cols = [label_col] + treatment_cols\n",
    "#     categorical_cols = data.select_dtypes(exclude='number').columns.difference(exclude_cols)\n",
    "\n",
    "\n",
    "#     # Convert numeric columns to float\n",
    "#     data[numeric_cols] = data[numeric_cols].astype(float)\n",
    "\n",
    "#     # Convert categorical columns to categorical type\n",
    "#     data[categorical_cols] = data[categorical_cols].astype('category')\n",
    "\n",
    "#     # Apply cat.codes only to categorical columns\n",
    "#     data[categorical_cols] = data[categorical_cols].apply(lambda x: x.cat.codes)\n",
    "\n",
    "#     # One-hot encode categorical columns\n",
    "#     data_encoded = pd.concat([\n",
    "#         pd.DataFrame(data[numeric_cols]),  # Keep numeric columns\n",
    "#         pd.DataFrame(y_numeric, columns=['Outcome']),\n",
    "#         pd.DataFrame(np.array(T_numeric).T, columns=treatment_cols),\n",
    "#         pd.DataFrame(data[categorical_cols], columns=categorical_cols),\n",
    "#     ], axis=1)\n",
    "\n",
    "\n",
    "#     return data_encoded\n",
    "\n",
    "# def save_data(data, results_dir, dataset_name, data_type):\n",
    "#     data.to_csv(os.path.join(results_dir, f\"{data_type}_{dataset_name}.csv\"), sep=\";\", index=False)\n",
    "\n",
    "# def save_data_encoded(data_encoded, results_dir, dataset_name, data_type):\n",
    "#     data_encoded.to_csv(os.path.join(results_dir, f\"{data_type}_encoded_{dataset_name}.csv\"), sep=\";\", index=False)\n",
    "\n",
    "\n",
    "# sorting_cols = [timestamp_col, activity_col]\n",
    "\n",
    "\n",
    "# # Specify the desired ratios\n",
    "# train_ratio = 0.5\n",
    "# val_ratio = 0.3\n",
    "# test_ratio = 0.2\n",
    "\n",
    "\n",
    "\n",
    "# for log_name in logs:\n",
    "#     print(\"\\n==================\\n Log: %s\\n==================\\n\" % (log_name,))\n",
    "#     treatments_data = results_data3[logs.index(log_name)]\n",
    "\n",
    "#     data = treatments_data.copy()\n",
    "#     all_columns = data.columns\n",
    "\n",
    "#     # Split the data into train, val, and test\n",
    "#     train, val, test = split_data(data, train_ratio, val_ratio, split_type=\"temporal\", seed=22)\n",
    "\n",
    "   \n",
    "#     treatment_cols = ['Treatment1',]\n",
    "\n",
    "#     #dataset_name = log_name\n",
    "#     results_dir = \"./../prepared_data/%s/\" % log_name\n",
    "\n",
    "\n",
    "#     save_data(train, results_dir, log_name, \"train\")\n",
    "#     save_data(test, results_dir, log_name, \"test\")\n",
    "#     save_data(val, results_dir, log_name, \"valid\")\n",
    "\n",
    "\n",
    "#     train_data_encoded = encode_data(train, treatment_cols, label_col, \"train\")\n",
    "#     test_data_encoded = encode_data(test, treatment_cols, label_col, \"test\")\n",
    "#     val_data_encoded = encode_data(val, treatment_cols, label_col, \"valid\")\n",
    "\n",
    "    \n",
    "    \n",
    "#     print(\"Saving the train, val, and test sets to separate files...\")\n",
    "#     save_data_encoded(train_data_encoded, results_dir, log_name, \"train\")\n",
    "#     save_data_encoded(test_data_encoded, results_dir, log_name, \"test\")\n",
    "#     save_data_encoded(val_data_encoded, results_dir, log_name, \"valid\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 8;\n                var nbb_unformatted_code = \"# import pandas as pd\\n# train_encoded_17 = pd.read_csv(\\\"./../prepared_data/bpic2017/train_encoded_bpic2017.csv\\\", sep=\\\";\\\")\\n# test_encoded_17 = pd.read_csv(\\\"./../prepared_data/bpic2017/test_encoded_bpic2017.csv\\\", sep=\\\";\\\")\\n# valid_encoded_17 = pd.read_csv(\\\"./../prepared_data/bpic2017/valid_encoded_bpic2017.csv\\\", sep=\\\";\\\")\\n# bpic2017_encoded = pd.concat([train_encoded_17, test_encoded_17, valid_encoded_17], axis=0)\\n# # save the encoded data to separate files\\n\\n# bpic2017_encoded.to_csv(\\\"./../prepared_data/bpic2017/bpic2017_encoded.csv\\\", sep=\\\";\\\", index=False)\";\n                var nbb_formatted_code = \"# import pandas as pd\\n# train_encoded_17 = pd.read_csv(\\\"./../prepared_data/bpic2017/train_encoded_bpic2017.csv\\\", sep=\\\";\\\")\\n# test_encoded_17 = pd.read_csv(\\\"./../prepared_data/bpic2017/test_encoded_bpic2017.csv\\\", sep=\\\";\\\")\\n# valid_encoded_17 = pd.read_csv(\\\"./../prepared_data/bpic2017/valid_encoded_bpic2017.csv\\\", sep=\\\";\\\")\\n# bpic2017_encoded = pd.concat([train_encoded_17, test_encoded_17, valid_encoded_17], axis=0)\\n# # save the encoded data to separate files\\n\\n# bpic2017_encoded.to_csv(\\\"./../prepared_data/bpic2017/bpic2017_encoded.csv\\\", sep=\\\";\\\", index=False)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# train_encoded_17 = pd.read_csv(\"./../prepared_data/bpic2017/train_encoded_bpic2017.csv\", sep=\";\")\n",
    "# test_encoded_17 = pd.read_csv(\"./../prepared_data/bpic2017/test_encoded_bpic2017.csv\", sep=\";\")\n",
    "# valid_encoded_17 = pd.read_csv(\"./../prepared_data/bpic2017/valid_encoded_bpic2017.csv\", sep=\";\")\n",
    "# bpic2017_encoded = pd.concat([train_encoded_17, test_encoded_17, valid_encoded_17], axis=0)\n",
    "# # save the encoded data to separate files\n",
    "\n",
    "# bpic2017_encoded.to_csv(\"./../prepared_data/bpic2017/bpic2017_encoded.csv\", sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 9;\n                var nbb_unformatted_code = \"# import pandas as pd\\n# train_encoded_12 = pd.read_csv(\\\"./../prepared_data/bpic2012/train_encoded_bpic2012.csv\\\", sep=\\\";\\\")\\n# test_encoded_12 = pd.read_csv(\\\"./../prepared_data/bpic2012/test_encoded_bpic2012.csv\\\", sep=\\\";\\\")\\n# valid_encoded_12 = pd.read_csv(\\\"./../prepared_data/bpic2012/valid_encoded_bpic2012.csv\\\", sep=\\\";\\\")\\n\\n# bpic2012_encoded = pd.concat([train_encoded_12, valid_encoded_12, test_encoded_12], axis=0)\\n# # save the encoded data to separate files\\n\\n# bpic2012_encoded.to_csv(\\\"./../prepared_data/bpic2012/bpic2012_encoded.csv\\\", sep=\\\";\\\", index=False)\";\n                var nbb_formatted_code = \"# import pandas as pd\\n# train_encoded_12 = pd.read_csv(\\\"./../prepared_data/bpic2012/train_encoded_bpic2012.csv\\\", sep=\\\";\\\")\\n# test_encoded_12 = pd.read_csv(\\\"./../prepared_data/bpic2012/test_encoded_bpic2012.csv\\\", sep=\\\";\\\")\\n# valid_encoded_12 = pd.read_csv(\\\"./../prepared_data/bpic2012/valid_encoded_bpic2012.csv\\\", sep=\\\";\\\")\\n\\n# bpic2012_encoded = pd.concat([train_encoded_12, valid_encoded_12, test_encoded_12], axis=0)\\n# # save the encoded data to separate files\\n\\n# bpic2012_encoded.to_csv(\\\"./../prepared_data/bpic2012/bpic2012_encoded.csv\\\", sep=\\\";\\\", index=False)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# train_encoded_12 = pd.read_csv(\"./../prepared_data/bpic2012/train_encoded_bpic2012.csv\", sep=\";\")\n",
    "# test_encoded_12 = pd.read_csv(\"./../prepared_data/bpic2012/test_encoded_bpic2012.csv\", sep=\";\")\n",
    "# valid_encoded_12 = pd.read_csv(\"./../prepared_data/bpic2012/valid_encoded_bpic2012.csv\", sep=\";\")\n",
    "\n",
    "# bpic2012_encoded = pd.concat([train_encoded_12, valid_encoded_12, test_encoded_12], axis=0)\n",
    "# # save the encoded data to separate files\n",
    "\n",
    "# bpic2012_encoded.to_csv(\"./../prepared_data/bpic2012/bpic2012_encoded.csv\", sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prpm_py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
